---
title: 暑假机器学习总结
date: 2017-09-13 20:49:32
tags: [ML, DIP]
category: [CS]
---

# 机器学习学习总结

经过两个月的学习，从对机器学习一点点懵懂认知，到现在对机器学习的基础知识跟体系有一定的认知。如今学习暂告一段落，总结如今学习过的重点知识，可以起到很好的复习作业，也是对两个月以来的交代。以下，我将按照每周学习进度来总结回顾机器学习的知识。

<!-- more -->

## 第一周

我们先学习了`python`编程基础，之后的学习是基于python各种库来实验的。我之前已经比较熟悉python，所以很快就完成这段学习；之后接触`numpy`，这是C语言为python编写的底层矩阵库，我之前也接触过，但比较浅，不过numpy封装的很好，用起来门槛也很低，很快就上手了；numpy之后就是`pandas`,它是数据分析常用的库，基于numpy，非常全面，我之前也用过，但基本需要重新学习，pandas比较难用，尤其是IO部分，有细粒度的操作，文档看起来也比较麻烦，没有示例，所以学习的过程中，是遇到问题再去查找方法，后面的学习pandas其实用到的也比较少。

以上内容大概花了两天，算是对机器学习的预备知识的准备。当然期间也学习了简单的使用anaconda，jupyter等工具，不再一一总结了。

之后开始学习基本的图像知识：

- [颜色直方图](http://baike.baidu.com/item/%E9%A2%9C%E8%89%B2%E7%9B%B4%E6%96%B9%E5%9B%BE),它所描述的是不同色彩在整幅图像中所占的比例，而并不关心每种色彩所处的空间位置。之后有几次作业中要编写颜色直方图的处理，一开始还是挺棘手的。不过跟后来的图像特征提取就是小巫见大巫了
- [HOG特征](http://blog.csdn.net/zouxy09/article/details/7929348)，主要思想：在一副图像中，局部目标的表象和形状（appearance and shape）能够被梯度或边缘的方向密度分布很好地描述。（本质：梯度的统计信息，而梯度主要存在于边缘的地方）。这也是之后的需要实现的任务，学习过程中，只有这个博客资料可以参考，其他资料要么可能是全英文晦涩难懂，要么还不如这个。这个博客写的太精炼了，初学的时候，实现起来非常困难，以至于有些同学对`根据这个博客写出算法`的可能性产生怀疑。我这周的作业也卡在这里了，学习机器学习的时候反而是因为图像处理知识不过关。后来经过讲解对这些特征有更新的认识了，不过可能实现起来对于现在我的依然有些困难。
- [LBP特征](http://blog.csdn.net/zouxy09/article/details/7929531)、[Haar特征](http://blog.csdn.net/zouxy09/article/details/7929570)也是这周的基础知识，我稍微学习了下LBP，发现比HOG要好懂，Haar并没有怎么看，最后这2个内容没有出现在作业里，我对这些也只有粗浅的认识。`LBP`（Local Binary Pattern，局部二值模式）是一种用来描述图像局部纹理特征的算子；它具有旋转不变性和灰度不变性等显著的优点。而`Haar`特征值反映了图像的灰度变化情况。

图像的部分内容学习后，开始了最基础的机器学习的内容：

- [Regression](http://www.bilibili.com/video/av10590361/)，先从最简单的`线性回归`开始，线性回归可能是踏入机器学习世界的第一步吧，它教你如何做最简单的预测和机器学习比较本质的思维。前几周学习的知识大多是看李宏毅的视频，前几周感觉还是不错的。从线性回归开始，学到了基本的`梯度下降`思想和度量性能的`代价函数`。

- [Error](http://www.bilibili.com/video/av10590361/#page=2)，第二节就是深入理解各种模型评估的知识，讲授了`误差`,`偏差`,`方差`的区别与联系。

- [Gradient Descent](http://www.bilibili.com/video/av10590361/#page=3)，最后是深入学习梯度下降，学习推导基本的梯度下降，然后提出`随机梯度下降(SGD)`，从大量数据中随机选择一定量数据来训练，提高学习效率。除此之外，讲解了梯度下降的问题：学习率的选择。然后基于此讲解了一个算法`Adagrad`来控制学习率。

- 最后还有一些比较杂的知识，了解了[K折交叉验证](https://github.com/basicv8vc/Python-Machine-Learning-zh/blob/master/%E7%AC%AC%E5%85%AD%E7%AB%A0/ch6_section2.md)的思想与作用，把数据集分割为训练集，验证集，测试集的思想与作用。各种[距离度量](http://blog.csdn.net/shiwei408/article/details/7602324)，可以衡量样本近似度。最后完成两个作业

图像知识可能在第一周学习，有点不知其有何用的感觉，就算想在思想上重视它，但没有实际用起来，还是难以深刻理解它的重要性吧。谈点个人感受，我其实挺不擅长也不太喜欢处理图像的，大一自己有简单接触过图像处理（跟现在的学的不太一样，而是常规的图像处理，不跟特征，知识等内容关联），虽然不比这些难，但也很吃力。

## 第二周

第二周主要学习的是`分类`的基础算法，分别学习`K近邻`,`决策树`,`逻辑斯蒂回归`：

- [KNN](http://cs231n.github.io/classification/)的思想就是把某个样本跟其他所有样本进行距离度量并总和，该样本离哪个类别'最近'，就标记为该类别。KNN是`懒惰学习`的典型算法，即到需要分类的时候才使用上训练集。在学习KNN的时候还了解到矢量化编程的重要性，减少不必要的python for 循环可以利于底层numpy优化为并行代码，在我这次实验里速度提升了近百倍。

- [Logistic Regression](https://www.bilibili.com/video/av10590361/#page=5) 该算法跟线性回归（跟感知器也类似）基本类似，不过它加入了`sigmoid`函数来进行分类而不是回归。后来学习`深度学习`才知道这里有`神经网络`的最基本的思想，或者说可能是最简单的神经网络了。

- [Decision Tree](https://www.bilibili.com/video/av12469267/#page=34)决策树可能是到这周为止最难的算法了，写起来会特别长。思想其实很简单，就是树的思想 + 人类决策过程。常用的决策树有`ID3`,`C4.5`,`C5.0`,`CART`等，不过我只编写了最简单的ID3，其他决策树进行了简单的了解。学习过程中认识到如果生成完整的决策树，那会变得非常耗时，后面学习到剪枝知识（我在作业里面编写`预剪枝`了，不过效果很差）。学习决策树里面知道了一些信息论的知识，如`信息熵`,`信息增益`,`纯度`等知识，决策的依据便是依据这些数值来找到最佳决策特征。

最后根据学到的知识完成一个井字棋胜负预测，不过我的模型很一般。这周的知识量不是很大，更多侧重机器学习基础编程，但是感觉学到东西很多的一周。

## 第三周

这周学习的机器学习非常强大和实用，是现在也很常用的模型：

- [SVM](https://www.bilibili.com/video/av9770190/index_27.html#page=20)非常理论，学到这里，我感觉我的高数白学了。其实现在我也不是很了解SVM，只对基本概念有了解。基本思想应该是把低维空间的非线性问题映射到高维空间线性问题来解决。然后里面概念非常多：支持向量的概念，距离度量，核函数，核方法，对偶问题，KKT等。

- [集成学习](https://www.bilibili.com/video/av10590361/index_22.html#page=22)非常实用且广用。许多一般模型集成后都可以大幅度提升性能。这周接触了许多集成学习算法：
    - `bagging`，非常简单的集成学习算法，我后来实现了bagging决策树，性能提升了许多。基本思想是`自助采样`一些样本后，分别训练n个模型，然后进行投票决策。这样可以大大减少过拟合而提升性能。
    
    - `Random Forest`，bagging算法的变体，基于决策树实现的。它与bagging的区别在于特化了决策树，在节点决策时，加入`属性扰动`（即只从一部分特征里选择最优特征），而bagging只有`样本扰动`（随机采集样本）。它的性能一般来说比bagging要好，我猜大概是加入了新的扰动后，更能避免决策树容易过拟合的缺点吧。
    
    - 其他如 `boosting`的`adboost`和`xgboost`，进行了简单了解，xgboost在kaggle里面很热门，因为性能特别好。不过这几个算法难度更大，我了解的也比较少。adboost的基本思想是让之前训练错误的部分对应的权重变大，让模型认识到这点。
     
- 最后是一点点图像特征的知识[bag of words model](https://gurus.pyimagesearch.com/the-bag-of-visual-words-model/)，对此进行一些了解

## 第四周

这周学习无监督学习，主要是聚类跟降维，不过主要是侧重分析并运用这些技术：

- `聚类`可以按结构特性分为原型聚类，层次聚类，密度聚类。[K-means](https://www.bilibili.com/video/av9912938/#page=78)是最基础的聚类算法，主要思想是通过多次迭代来把刻画原型，来使误差最小。K-means之后是了解了`高斯混合聚类`，也是原型聚类。

- `降维`这部分主要接触了[PCA](https://www.bilibili.com/video/av10590361/index_22.html#page=13)，不过这部分理论特别麻烦，我只搞懂了基本思想，并学会基本运用。总之，它变换了基底，并把比重最小的维度去掉来降维，这样能最大程度的保存原始样本的信息。

这周机器学习的知识就进入进阶难度了，说实话，我掌握的不太会好，不过算是对机器学习的体系更加了解了。除了以上还有一些[其他知识](http://ufldl.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B)，不过比较零碎，我就不一一总结了。

## 第五六七八周

这里把深度学习一起总结了吧，这几周感觉学习的不太好。

第六周开始，陆陆续续有许多高大上的报告可以听讲了，了解了许多前沿的应用，思想，算法。从第五周开始也步入了[深度学习](https://www.bilibili.com/video/av10590361/index_13.html#page=6)的大门。

这几周主要学习`全连接网络`跟`卷积神经网络`跟`循环神经网络`，理解了很久[反向传播](https://www.bilibili.com/video/av10590361/index_13.html#page=7)，[卷积运算](https://www.bilibili.com/video/av10590361/index_13.html#page=10)。其实参考过很多资料，这里就不一一列出了。目前对`反向传播`也有稍微清晰的了解了，对于卷积，了解了其思想，但对其运算还是只有抽象的认识，反向传播可以说是目前神经网络的基础。`全连接网络`在理论上是近似的`图灵机`，不过实用性很差，一般是配合其他网络而使用。而卷积神经网络是针对图像而发明的，在图像处理跟机器视觉应用广泛。而循环神经网络更适用于序列数据，如文本，这在自然语言处理很常用，而由于一般的`RNN`有局限性，所以有一个`LSTM`的变体，这个模型我不是很清楚，但它的表现更好（最近好像又出现一个比较厉害的新变体`SRU`）。

由于神经网络的代码非常难写，写出来也基本不可以重用，我们学习了`keras`这个基于`tensorflow`的深度学习框架来实现一些经典的模型。后来应用keras解决了一些基本的图像分类问题。

第七八周陆陆续续的讲座也应该提一提，感觉大大开阔了我的视野。原来我以为深度学习基本都是在对图像上进行工作，好像这样也没什么意思。不过后来发现图像几乎是实现人工智能最基本的办法。而老师们的方向五花八门，问题的复杂度也远超过我的脑容量，深度学习反而变成了实现人工智能的基本工具而已，更多在于对于问题的深入研究和对特征的深入探究。

这几周的学习积极性变得比较差劲了，不过收获还是很多。至少认识到机器学习跟深度学习的基本思想，我觉得未来的程序员都或多或少需要了解这些知识，因为他们可能会用到相应的算法来部署一些人工智能应用到各种设备、各种网站、各种系统中去，而懂得这些知识的人显然能在工作中更胜一筹。

## 总结

以上大概就是我的总结了，很粗略，也没有讲到具体的数学知识，更多是讲到自己的小小收获跟感受。我不清楚以后是否会从事机器学习相关的工作，但我以后肯定会抱着好奇心继续完善我对这些知识认识，了解里面的新思想，跟上人工智能的潮流，做一个终身学习的人吧。
