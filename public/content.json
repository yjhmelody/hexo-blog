{"meta":{"title":"yjhmelody's blog","subtitle":"about my life","description":"想记录很多东西","author":"yjhmelody","url":"https://yjhmelody.github.io"},"pages":[{"title":"categories","date":"2017-08-08T02:57:40.000Z","updated":"2017-08-08T03:27:20.355Z","comments":true,"path":"categories/index.html","permalink":"https://yjhmelody.github.io/categories/index.html","excerpt":"","text":""},{"title":"","date":"2017-08-07T06:58:08.000Z","updated":"2017-08-08T03:13:49.514Z","comments":true,"path":"about/index.html","permalink":"https://yjhmelody.github.io/about/index.html","excerpt":"Hiker An attractive, exquisite theme for Hexo. named “Hiker”, short for “HikerNews”. ☞ Live Preview | ✎ Hiker 中文版使用文档","text":"Hiker An attractive, exquisite theme for Hexo. named “Hiker”, short for “HikerNews”. ☞ Live Preview | ✎ Hiker 中文版使用文档 Installation Get it from GitHub 1$ git clone https://github.com/iTimeTraveler/hexo-theme-hiker.git themes/hiker Enable Modify theme setting in _config.yml to hiker. 1234# Extensions## Plugins: http://hexo.io/plugins/## Themes: http://hexo.io/themes/theme: hiker Update 12$ cd themes/Hiker$ git pull ConfigurationTheme configuration example1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192# ---------------------------------------------------------------# Site Information Settings# ---------------------------------------------------------------# Header Menumenu: Home: / Archives: archives Categories: categories Tags: tags About: aboutrss: /atom.xmlsince: 2013# Set default keywords (Use a comma to separate)keywords: \"\"# Put your favicon.ico into `hexo-site/themes/hiker/source/` directory.avatar: css/images/mylogo.jpg# Homepage# eg. home_background_image: css/images/home-bg.jpghome_background_image: css/images/home-bg.jpghome_logo_image: enable: false border: false url: css/images/homelogo.jpg# AboutPage backgroundabout_big_image: css/images/pose.jpg# Contentexcerpt_link: Read Morefancybox: true# Sidebarsidebar: rightwidgets:- category- tag- tagcloud- archive- recent_posts# comment ShortName, you can choose only ONE to display.duoshuo_shortname: iTimeTravelerdisqus_shortname:# Code Highlight theme# Available value:# default | normal | night | night eighties | night blue | night bright# https://github.com/chriskempson/tomorrow-themehighlight_theme: default# Article theme color# Available value:# random | orange | blue | red | green | blacktheme_color: random# display widgets at the bottom of index pages (pagination == 2)index_widgets:# - category# - tagcloud# - archive# widget behaviorarchive_type: 'monthly'show_count: true# Google Webmaster tools verification setting# See: https://www.google.com/webmasters/google_site_verification:baidu_site_verification:qihu_site_verification:# Miscellaneousgoogle_analytics:gauges_analytics:twitter:google_plus:fb_admins:fb_app_id: menu - Navigation menu rss - RSS link excerpt_link - “Read More” link at the bottom of excerpted articles. false to hide the link. fancybox - Enable Fancybox sidebar - Sidebar style. You can choose left, right, bottom or false. widgets - Widgets displaying in sidebar google_analytics - Google Analytics ID favicon - Favicon path twitter - Twiiter ID google_plus - Google+ ID FeaturesHomepage backgroundYou could place the image file in YOUR_HEXO_SITE\\themes\\hiker\\source\\css\\images directory. and modify home_background_image in hiker/_config.yml. 123# Homepage# eg. home_background_image: css/images/home-bg.jpghome_background_image: css/images/home-bg.jpg If you DON’T want any image as your homepage background, just set home_background_image empty in hiker/_config.yml, then you have an default homepage with random decorative pattern. Code Highlight ThemeHiker use Tomorrow Theme for your code block. We have six options in total: default, normal, night, night blue, night bright, night eighties Above preview picture is default theme. the image below show other five Highlight themes. Modify highlight_theme in hiker/_config.yml. 12345# Code Highlight theme# Available value:# default | normal | night | night eighties | night blue | night bright# https://github.com/chriskempson/tomorrow-themehighlight_theme: default Blog Theme ColorHiker provide five color themes for your blog. orange blue red green black You can modify theme_color in hiker/_config.yml. 1234# Article theme color# Available value:# random | orange | blue | red | green | blacktheme_color: random Night modeJust for article reading. In article page, you can click the logo image of header to switch to Night mode. SearchHiker use Insight Search to help you search anything inside your site without any third-party plugin. 12345# Searchsearch: insight: true # you need to install `hexo-generator-json-content` before using Insight Search swiftype: # enter swiftype install key here baidu: false # you need to disable other search engines to use Baidu search, options: true, false Attention: You need to install hexo-generator-json-content before using Insight Search. 1$ npm install -S hexo-generator-json-content FancyboxHiker uses Fancybox to showcase your photos. You can use Markdown syntax or fancybox tag plugin to add your photos. 123![img caption](img url)&#123;% fancybox img_url [img_thumbnail] [img_caption] %&#125; SidebarYou can put your sidebar in left side, right side or bottom of your site by editing sidebar setting.Hiker provides 5 built-in widgets: category tag tagcloud archives recent_posts All of them are enabled by default. You can edit them in widget setting. Comment supportHiker has native support for DuoShuo &amp; Disqus comment systems. Modify the following snippets to hiker hiker/_config.yml: 123# comment ShortName, you can choose only ONE to display.duoshuo_shortname: iTimeTravelerdisqus_shortname: Browser support ContributingAll kinds of contributions (enhancements, new features, documentation &amp; code improvements, issues &amp; bugs reporting) are welcome. Looking forward to your pull request."},{"title":"Archives","date":"2017-08-07T07:00:24.000Z","updated":"2017-08-08T03:13:40.729Z","comments":true,"path":"archives/index.html","permalink":"https://yjhmelody.github.io/archives/index.html","excerpt":"","text":""},{"title":"Tags","date":"2017-08-08T04:12:45.000Z","updated":"2017-08-08T03:14:08.256Z","comments":true,"path":"tags/index.html","permalink":"https://yjhmelody.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"IEEE802.11概述","slug":"IEEE802.11无线LANs概述","date":"2017-12-26T14:03:46.000Z","updated":"2017-12-31T11:35:00.116Z","comments":true,"path":"2017/12/26/IEEE802.11无线LANs概述/","link":"","permalink":"https://yjhmelody.github.io/2017/12/26/IEEE802.11无线LANs概述/","excerpt":"IEEE 802.11无线 LANs 概述无线链路和网络特征802.11协议簇是国际电工电子工程学会（IEEE）为无线局域网络制定的标准。虽然WI-FI使用了802.11的媒体访问数据链路层（DLL）和物理层（PHY），但是两者并不完全一致。 这里主要概述了 802.11无线LAN的特征。不过首先应该关注的是无线网络与有线网络的区别。 当我们寻找有线和无线网络的重要区别时，应该重点关注链路层，我们确实可以发现有线链路和无线链路有许多重要区别： 递减的信号强度。信号强度受到发送方和接收方距离的增加而递减。 来自其他源的干扰。在同一个频段发送信号的电波源会互相干扰。 多径传播（multipath propagation）。当电磁波受到反射，会在发送方和接收方之间走过多条不同长度的路径，此时出现多径传播，这使得接受的信号变得模糊。","text":"IEEE 802.11无线 LANs 概述无线链路和网络特征802.11协议簇是国际电工电子工程学会（IEEE）为无线局域网络制定的标准。虽然WI-FI使用了802.11的媒体访问数据链路层（DLL）和物理层（PHY），但是两者并不完全一致。 这里主要概述了 802.11无线LAN的特征。不过首先应该关注的是无线网络与有线网络的区别。 当我们寻找有线和无线网络的重要区别时，应该重点关注链路层，我们确实可以发现有线链路和无线链路有许多重要区别： 递减的信号强度。信号强度受到发送方和接收方距离的增加而递减。 来自其他源的干扰。在同一个频段发送信号的电波源会互相干扰。 多径传播（multipath propagation）。当电磁波受到反射，会在发送方和接收方之间走过多条不同长度的路径，此时出现多径传播，这使得接受的信号变得模糊。 IEEE 802.11 标准常见的关于LAN的802.11标准包括 802.11b、802.11a、802.11g等。它们使用相同的媒体访问协议CSMA/CA，并且使用相同的链路层帧格式。 802.11 体系结构802.11 体系结构的基本构建模块是基本服务集（Basic Service Set BSS），一个BBS包含若干个无线站点和一个被称为接入点（AP）的中央基站。与以太网设备类似，每个802.11无线站点都具有6字节的MAC地址。每个AP的无线接口也具有一个MAC地址。这些MAC地址也由IEEE管理，理论上全球唯一。 信道与关联在802.11中，每个无线站点在发送或者接受网络层数据前，必须与一个AP相关联。而安装一个AP时，需要为其分配单字或者双字的服务集标识（SSID）。除了SSID，还需要分配一个信道号。 信道： 802.11定义了11个部分重叠的信道，当且仅当2个信道由4个或更多的信道隔开时，它们才不重叠。比如 1、6、11是唯一的3个非重叠信道集合。这样就可以在同一个物理网络安装3个AP。 关联： 为了获得互联网接入，你的无线站点需要加入其中一个子网并需要与其中一个AP相关联（associate）。关联意味着该无线站点在自身与该AP之间创建了一个虚拟线路。 然而，首先得回答：你的无线站点如何与某个特定的AP相关联？或者你的无线站点如何知道当前位置哪个AP可以使用？ 802.11标准要求每个AP周期性地发送信标帧（beacon frame），每个信标帧包括该AP的SSID和MAC地址。你的无线站点为了得知正在发送信标帧的AP，扫描11个信道，找到可能来着可能位于该区域的AP所发出的信标帧。 不过，802.11标准没有指定选择哪个可用的AP进行关联的算法，这个由802.11固件和无线主机的软件设计者来设计。比如，主机选择接收到的具有最高信号强度的信标帧，但可能过载。 这里扫描分两种： 被动扫描（passive scanning），自AP发送信标帧，无线站点扫描信道和监听信标帧，然后向选择的AP发送关联请求帧，最后AP响应关联响应帧 主动扫描（active scanning），无线站点广播探测请求帧，AP发送探测响应，然后无线站点选择AP发送关联请求帧，最后AP响应关联响应帧 鉴别与接入为了与特定的AP创建关联，某无线站点可能需要向该AP鉴别它自身。 802.11 提供了几种不同的鉴别和接入方法 基于站点的MAC地址允许其接入网络 基于用户名和口令 上述两种情况下，可能使用的RADIUS（基于UDP）和DIAMETER（最初作为RADIUS的改良）这样的协议。","categories":[{"name":"study","slug":"study","permalink":"https://yjhmelody.github.io/categories/study/"}],"tags":[{"name":"network","slug":"network","permalink":"https://yjhmelody.github.io/tags/network/"}]},{"title":"JS函数式编程笔记1","slug":"JS函数式编程笔记1","date":"2017-12-11T13:47:31.000Z","updated":"2017-12-31T11:37:27.419Z","comments":true,"path":"2017/12/11/JS函数式编程笔记1/","link":"","permalink":"https://yjhmelody.github.io/2017/12/11/JS函数式编程笔记1/","excerpt":"JS函数式编程我们在做什么 “we both know what happens when you assume”，源自一句名言“When you assume you make an ASS of U and ME”，意思是“让两人都难堪”）。但我猜想你在使用可变状态（mutable state）、无限制副作用（unrestricted side effects）和无原则设计（unprincipled design）的过程中已经遇到过一些麻烦。 现在已经有一些通用的编程原则了，各种缩写词带领我们在编程的黑暗隧道里前行：DRY（不要重复自己，don’t repeat yourself），高内聚低耦合（loosecoupling high cohesion），YAGNI （你不会用到它的，ya ain’t gonna need it），最小意外原则（Principle of least surprise），单一责任（single responsibility）等等。这些原则同样适用于函数式编程(FP)。","text":"JS函数式编程我们在做什么 “we both know what happens when you assume”，源自一句名言“When you assume you make an ASS of U and ME”，意思是“让两人都难堪”）。但我猜想你在使用可变状态（mutable state）、无限制副作用（unrestricted side effects）和无原则设计（unprincipled design）的过程中已经遇到过一些麻烦。 现在已经有一些通用的编程原则了，各种缩写词带领我们在编程的黑暗隧道里前行：DRY（不要重复自己，don’t repeat yourself），高内聚低耦合（loosecoupling high cohesion），YAGNI （你不会用到它的，ya ain’t gonna need it），最小意外原则（Principle of least surprise），单一责任（single responsibility）等等。这些原则同样适用于函数式编程(FP)。 先看基本的例子 1234567891011let add = function (x, y) &#123; return x + y&#125;;let multiply = function (x, y) &#123; return x * y&#125;;let a = 4;let b = 2;let c = 0;let result = add(multiply(b, add(a, c)), multiply(a, b));//=&gt;16 你会发现我们不过是在运用古人早已获得的知识 1234567891011121314// 结合律（assosiative）add(add(x, y), z) == add(x, add(y, z));// 交换律（commutative）add(x, y) == add(y, x);// 同一律（identity）add(x, 0) == x;// 分配律（distributive）multiply(x, add(y, z)) == add(multiply(x, y), multiply(x, z));// 原有代码add(multiply(b, add(a, c)), multiply(a, b));// 应用同一律，去掉多余的加法操作（add(a, c) == a）add(multiply(b, a), multiply(a, b));// 再应用分配律multiply(b, add(a, a)); 当然这里我们定义 add 和 multiply 是为了代码完整性，实际上并不必要——在调用之前它们肯定已经在某个类库里定义好了。 我们希望去践行每一部分都能完美接合的理论，希望能以一种通用的、可组合的组件来表示我们的特定问题，然后利用这些组件的特性来解决这些问题。相比命令式编程的那种“某某去做某事”的方式，函数式编程将会有更多的约束，不过你会震惊于这种强约束、数学性的“框架”所带来的回报。 一等公民的函数先看看常见的JS风格的一个例子 1234567891011121314151617181920function ajaxCall(func) &#123; //...&#125;// 用一个函数把另一个函数包起来，目的仅仅是延迟执行，真的是非常糟糕的编程习惯let getServerStuff = function (callback) &#123; return ajaxCall(function (json) &#123; return callback(json); &#125;);&#125;;// 这行// return ajaxCall(function (json) &#123;// return callback(json);// &#125;);// 等价于这行// return ajaxCall(callback);// 应该改成这样let getServerStuff = ajaxCall; 再看另一个例子 12345678910111213141516171819202122232425262728293031323334353637383940414243class Views &#123; //...&#125;class Db &#123; //...&#125;// 这样做除了徒增代码量，提高维护和检索代码的成本外，没有任何用处let BlogController = (function () &#123; let index = function (posts) &#123; return Views.index(posts); &#125;; let show = function (post) &#123; return Views.show(post); &#125;; let create = function (attrs) &#123; return Db.create(attrs); &#125;; let update = function (post, attrs) &#123; return Db.update(post, attrs); &#125;; let destroy = function (post) &#123; return Db.destroy(post); &#125;; return &#123; index, show, create, update, destroy &#125;;&#125;)();// 我们可以把它重写成这样：let BlogController = &#123; index: Views.index, show: Views.show, create: Db.create, update: Db.update, destroy: Db.destroy&#125;;// 或者直接全部删掉，因为它的作用仅仅就是把视图（Views）和数据库（Db）打包在一起而已 另外，如果一个函数被不必要地包裹起来了，而且发生了改动，那么包裹它的那个函数也要做相应的变更。 12345678910111213httpGet('/post/2', function(json)&#123; return renderPost(json);&#125;);// 如果 httpGet 要改成可以抛出一个可能出现的 err 异常，那我们还要回过头去把“胶水”函数也改了// 把整个应用里的所有 httpGet 调用都改成这样，可以传递 err 参数。httpGet('/post/2', function(json, err)&#123; return renderPost(json, err);&#125;);// 写成一等公民函数的形式，要做的改动将会少得多httpGet('/post/2', renderPost); // renderPost 将会在 httpGet 中调用，想要多少参数都行 除了删除不必要的函数，正确地为参数命名也必不可少。当然命名不是什么大问题，但还是有可能存在一些不当的命名，尤其随着代码量的增长以及需求的变更，这种可能性也会增加。 如果一个底层函数使用了 this，而且是以一等公民的方式被调用的，那你就等着 JS 这个蹩脚的抽象概念发怒吧 12345const fs = require('fs');// 太可怕了fs.readFile('freaky_friday.txt', Db.save);// 好一点点fs.readFile('freaky_friday.txt', Db.save.bind(Db)); 纯函数的好处 纯函数是这样一种函数，即相同的输入，永远会得到相同的输出，而且没有任何可观察的副作用。 比如 slice 和 splice。我们说 slice 符合纯函数的定义是因为对相同的输入它保证能返回相同的输出。而 splice 却会嚼烂调用它的那个数组，然后再吐出来；这就会产生可观察到的副作用，即这个数组永久地改变了。 123456789101112131415let xs = [1,2,3,4,5];// 纯的xs.slice(0,3);//=&gt; [1,2,3]xs.slice(0,3);//=&gt; [1,2,3]xs.slice(0,3);//=&gt; [1,2,3]// 不纯的xs.splice(0,3);//=&gt; [1,2,3]xs.splice(0,3);//=&gt; [4,5]xs.splice(0,3);//=&gt; [] 下一个例子 1234567891011121314151617// 不纯的let minimum = 21;let checkAge = function (age) &#123; return age &gt;= minimum;&#125;;// 纯的let checkAge = function (age) &#123; let minimum = 21; return age &gt;= minimum;&#125;;// 纯的const minimum = 21;let checkAge = function (age) &#123; return age &gt;= minimum;&#125;; 在不纯的版本中， checkAge 的结果将取决于 minimum 这个可变变量的值。换句话说，它取决于系统状态（system state）；这一点令人沮丧，因为它引入了外部的环境，从而增加了认知负荷（cognitive load）。 另一方面，使用纯函数的形式，函数就能做到自给自足。我们也可以让 minimum成为一个不可变（immutable）对象，这样就能保留纯粹性，因为状态不会有变化。要实现这个效果，必须得创建一个对象，然后调用 Object.freeze 方法 123let immutableState = Object.freeze(&#123; minimun: 21&#125;); 副作用是在计算结果的过程中，系统状态的一种变化，或者与外部世界进行的可观察的交互。 副作用可能包含，但不限于： 更改文件系统 往数据库插入记录 发送一个 http 请求 可变数据 打印/log 获取用户输入 DOM 查询 访问系统状态 这个列表还可以继续写下去。概括来讲，只要是跟函数外部环境发生的交互就都是副作用——这一点可能会让你怀疑无副作用编程的可行性。函数式编程的哲学就是假定副作用是造成不正当行为的主要原因。 这并不是说，要禁止使用一切副作用，而是说，要让它们在可控的范围内发生。后面讲到 functor 和 monad 的时候我们会学习如何控制它们。 追求“纯”的理由 可缓存性（Cacheable） 可移植性／自文档化（Portable / SelfDocumenting） 可测试性（Testable） 合理性（Reasonable） 并行性(Parallelism) 可缓存性（Cacheable）首先，纯函数总能够根据输入来做缓存。实现缓存的一种典型方式是 memoize 技术 123456789let squareNumber = memoize(function()&#123;return x * x&#125;);squareNumber(4);//=&gt; 16squareNumber(4); // 从缓存中读取输入值为 4 的结果//=&gt; 16squareNumber(5);//=&gt; 25squareNumber(5); // 从缓存中读取输入值为 5 的结果//=&gt; 25 下面的代码是memoize一个简单的实现，尽管它不太健壮 123456789function momoize(f) &#123; let cache = &#123;&#125; return function () &#123; let arg_str = JSON.stringify(arguments) // 有cache则返回cache里的，总是保存到cache里 cache[arg_str] = cache[arg_str] || f(...arguments) return cache[arg_str] &#125;&#125; 值得注意的一点是，可以通过延迟执行的方式把不纯的函数转换为纯函数 123let pureHttpCall = memoize(function(url, params)&#123; return function() &#123; return $.getJSON(url, params); &#125;&#125;); 这里有趣的地方在于我们并没有真正发送 http 请求——只是返回了一个函数，当调用它的时候才会发请求。这个函数之所以有资格成为纯函数，是因为它总是会根据相同的输入返回相同的输出：给定了 url 和 params 之后，它就只会返回同一个发送 http 请求的函数 我们的 memoize 函数工作起来没有任何问题，虽然它缓存的并不是 http 请求所返回的结果，而是生成的函数。 重点是我们可以缓存任意一个函数，不管它们看起来多么具有破坏性。 可移植性／自文档化（Portable / SelfDocumenting）纯函数是完全自给自足的，它需要的所有东西都能轻易获得。仔细思考思考这一点…这种自给自足的好处是什么呢？首先，纯函数的依赖很明确，因此更易于观察和理解。 12345678910111213141516171819202122232425262728// 不纯的let signUp = function (attrs) &#123; let user = saveUser(attrs); welcomeUser(user);&#125;;let saveUser = function (attrs) &#123; let user = Db.save(attrs); //...&#125;;let welcomeUser = function (user) &#123; // Email(user, ...); //...&#125;;// 纯的let signUp = function (Db, Email, attrs) &#123; return function () &#123; // 参数绑定 let user = saveUser(Db, attrs); welcomeUser(Email, user); &#125;;&#125;;let saveUser = function (Db, attrs) &#123; //...&#125;;let welcomeUser = function (Email, user) &#123; //...&#125;; 这个例子表明，纯函数对于其依赖必须要诚实，这样我们就能知道它的目的。仅从纯函数版本的 signUp 的签名就可以看出，它将要用到 Db、Email 和 attrs ，这在最小程度上给了我们足够多的信息。 其次，通过强迫“注入”依赖，或者把它们当作参数传递，我们的应用也更加灵活；因为数据库或者邮件客户端等等都参数化了。 命令式编程中“典型”的方法和过程都深深地根植于它们所在的环境中，通过状态、依赖和有效作用（available effects）达成；纯函数与此相反，它与环境无关，只要我们愿意，可以在任何地方运行它。 可测试性（Testable）第三点，纯函数让测试更加容易。 我们不需要伪造一个“真实的”支付网关，或者每一次测试之前都要配置、之后都要断言状态（assert the state）。只需简单地给函数一个输入，然后断言输出就好了。 合理性（Reasonable）很多人相信使用纯函数最大的好处是引用透明性（referential transparency）。如果一段代码可以替换成它执行所得的结果，而且是在不改变整个程序行为的前提下替换的，那么我们就说这段代码是引用透明的。 由于纯函数总是能够根据相同的输入返回相同的输出，所以它们就能够保证总是返回同一个结果，这也就保证了引用透明性。我们来看一个例子。 12345678910111213141516171819202122let Immutable = require('immutable');let decrementHP = function(player) &#123; return player.set(\"hp\", player.hp-1);&#125;;let isSameTeam = function(player1, player2) &#123; return player1.team === player2.team;&#125;;let punch = function(player, target) &#123; if(isSameTeam(player, target)) &#123; return target; &#125; else &#123; return decrementHP(target); &#125;&#125;;let jobe = Immutable.Map(&#123;name:\"Jobe\", hp:20, team: \"red\"&#125;);let michael = Immutable.Map(&#123;name:\"Michael\", hp:20, team: \"green\"&#125;);punch(jobe, michael);//=&gt; Immutable.Map(&#123;name:\"Michael\", hp:19, team: \"green\"&#125;) decrementHP 、 isSameTeam 和 punch 都是纯函数，所以是引用透明的。我们可以使用一种叫做“等式推导”（equational reasoning）的技术来分析代码。所谓“等式推导”就是“一对一”替换，有点像在不考虑程序性执行的怪异行为（quirks of programmatic evaluation）的情况下，手动执行相关代码。我们借助引用透明性来剖析一下这段代码。 1234567let punch = function(player, target) &#123; if(player.team === target.team) &#123; return target; &#125; else &#123; return decrementHP(target); &#125;&#125;; 因为是不可变数据，我们可以直接把 team 替换为实际值： 1234567let punch = function(player, target) &#123; if(\"red\" === \"green\") &#123; return target; &#125; else &#123; return decrementHP(target); &#125;&#125;; if 语句执行结果为 false ，所以可以把整个 if 语句都删掉： 123let punch = function(player, target) &#123; return decrementHP(target);&#125;; 如果再内联 decrementHP ，我们会发现这种情况下， punch 变成了一个让 hp 的值减 1 的调用： 123let punch = function(player, target) &#123; return target.set(\"hp\", target.hp-1);&#125;; 总之，等式推导带来的分析代码的能力对重构和理解代码非常重要。事实上，我们重构海鸥程序使用的正是这项技术：利用加和乘的特性。 并行性(Parallelism)最后一点，也是决定性的一点：我们可以并行运行任意纯函数。因为纯函数根本不需要访问共享的内存，而且根据其定义，纯函数也不会因副作用而进入竞争态（race condition）。 并行代码在服务端 js 环境以及使用了 web worker 的浏览器那里是非常容易实现的，因为它们使用了线程（thread）。 柯里化（curry）不可或缺的 curry 我父亲以前跟我说过，有些事物在你得到之前是无足轻重的，得到之后就不可或缺了。微波炉是这样，智能手机是这样，互联网也是这样——老人们在没有互联网的时候过得也很充实。对我来说，函数的柯里化（curry）也是这样。 curry 的概念很简单：只传递给函数一部分参数来调用它，让它返回一个函数去处理剩下的参数。 你可以一次性地调用 curry 函数，也可以每次只传一个参数分多次调用。 12345678910let add = function(x) &#123; return function(y) &#123; return x + y &#125;&#125;let inc = add(1)inc(10)// 11 这里我们定义了一个 add 函数，它接受一个参数并返回一个新的函数。调用add 之后，返回的函数就通过闭包的方式记住了 add 的第一个参数。一次性地调用它实在是有点繁琐，好在我们可以使用一个特殊的 curry 帮助函数（helperfunction）使这类函数的定义和调用更加容易。 123456789101112131415161718const curry = require('lodash/curry')let match = curry(function(what, str)&#123; return str.match(what)&#125;)let replace = curry(function(what, replacement, str)&#123; return str.replace(what, replacement)&#125;)let filter = curry(function(f, ary)&#123; return ary.filter(f)&#125;)let map = curry(function(f, ary)&#123; return ary.map(f)&#125;) 我在上面的代码中遵循的是一种简单，同时也非常重要的模式。即策略性地把要操作的数据（String， Array）放到最后一个参数里 12345678910111213141516171819202122match(/\\s+/g, \"hello world\")// [ ' ' ]match(/\\s+/g)(\"hello world\")// [ ' ' ]let hasSpaces = match(/\\s+/g)// function(x) &#123; return x.match(/\\s+/g) &#125;hasSpaces(\"hello world\")// [ ' ' ]hasSpaces(\"spaceless\")// nullfilter(hasSpaces, [\"tori_spelling\", \"tori amos\"])// [\"tori amos\"]let findSpaces = filter(hasSpaces)// function(xs) &#123; return xs.filter(function(x) &#123; return x.match(/\\s+/g) &#125;) &#125;findSpaces([\"tori_spelling\", \"tori amos\"])// [\"tori amos\"]let noVowels = replace(/[aeiou]/ig);// function(replacement, x) &#123; return x.replace(/[aeiou]/ig, replacement) &#125;let censored = noVowels(\"*\");// function(x) &#123; return x.replace(/[aeiou]/ig, \"*\") &#125;censored(\"Chocolate Rain\");// 'Ch*c*l*t* R**n' 这里表明的是一种“预加载”函数的能力，通过传递一到两个参数调用函数，就能得到一个记住了这些参数的新函数。 不仅仅是双关语／咖喱用 map 简单地把参数是单个元素的函数包裹一下，就能把它转换成参数为数组的函数。 12345let getLength = function(x)&#123; return x.length&#125;let getAllLength = map(getLength) 只传给函数一部分参数通常也叫做局部调用（partial application），能够大量减少样板文件代码（boilerplate code）。 通常我们不定义直接操作数组的函数，因为只需内联调用 map(getChildren) 就能达到目的。这一点同样适用于 sort 、 filter 以及其他的高阶函数（higherorder function）（高阶函数：参数或返回值为函数的函数）。 当我们谈论纯函数的时候，我们说它们接受一个输入返回一个输出。curry 函数所做的正是这样：每传递一个参数调用函数，就返回一个新函数处理剩余的参数。这就是一个输入对应一个输出啊。哪怕输出是另一个函数，它也是纯函数。当然 curry 函数也允许一次传递多个参数，但这只是出于减少 () 的方便。 curry 函数用起来非常得心应手，通过简单地传递几个参数，就能动态创建实用的新函数；而且还能带来一个额外好处，那就是保留了数学的函数定义，尽管参数不止一个。 代码组合（compose）函数饲养12345let compose = function(f, g)&#123; return function(x)&#123; return f(g(x)) &#125;&#125; f 和 g 都是函数， x 是在它们之间通过“管道”传输的值在 compose 的定义中， g 将先于 f 执行，因此就创建了一个从右到左的数据流。 组合看起来像是在饲养函数让它们结合，产下一个崭新的函数。组合的用法如下： 1234567891011121314151617let toUpperCase = function(str)&#123; return str.toUpperCase()&#125;let toLowerCase = function(str)&#123; return str.toLowerCase()&#125;let exclaim = function(str)&#123; return str + '!'&#125;let reverse = function(str)&#123; return str.split('').reverse().join('')&#125;let angry = compose(exclaim, toUpperCase)let shout = compose(exclaim, toUpperCase)console.log(exclaim(toUpperCase(\"hello\")))console.log(shout(\"hello\")) 这个组合中函数的执行顺序应该是显而易见的。尽管我们可以定义一个从左向右的版本，但是从右向左执行更加能够反映数学上的含义——是的，组合的概念直接来自于数学课本。 现在是时候去看看所有的组合都有的一个特性了。 1234567// 结合律（associativity）// let associative = compose(f, compose(g, h)) == compose(compose(f, g), h) // true// 结合律的一大好处是任何一个函数分组都可以被拆开来，然后再以它们自己的组合方式打包在一起console.log(compose(reverse, compose(toUpperCase, exclaim))(\"world\"))console.log(compose(compose(reverse, toUpperCase), exclaim)(\"world\")) 符合结合律意味着不管你是把 g 和 h 分到一组，还是把 f 和 g 分到一组都不重要。 结合律的一大好处是任何一个函数分组都可以被拆开来，然后再以它们自己的组合方式打包在一起。 pointfreepointfree 模式指的是，永远不必说出你的数据。它的意思是说，函数无须提及将要操作的数据是什么样的。一等公民的函数、柯里化（curry）以及组合协作起来非常有助于实现这种模式。 1234567// 非 pointfree，因为提到了数据：wordlet snakeCase = function (word) &#123; return word.toLowerCase().replace(/\\s+/ig, '_');&#125;// pointfreesnakeCase = compose(replace(/\\s+/ig, '_'), toUpperCase) 这里所做的事情就是通过管道把数据在接受单个参数的函数间传递。利用 curry，我们能够做到让每个函数都先接收数据，然后操作数据，最后再把数据传递到下一个函数那里去。 另外，pointfree 模式能够帮助我们减少不必要的命名，让代码保持简洁和通用。对函数式代码来说，pointfree 是非常好的石蕊试验，因为它能告诉我们一个函数是否是接受输入返回输出的小函数。比如，while 循环是不能组合的。不过你也要警惕，pointfree 就像是一把双刃剑，有时候也能混淆视听。并非所有的函数式代码都是 pointfree 的，不过这没关系。可以使用它的时候就使用，不能使用的时候就用普通函数。 debug组合的一个常见错误是，在没有局部调用之前，就组合类似 map 这样接受两个参数的函数。 12345678// 错误做法：我们传给了 `angry` 一个数组，根本不知道最后传给 `map` 的是什么东西。let latin = compose(map, angry, reverse)latin([\"frog\", \"eyes\"])// error// 正确做法：每个函数都接受一个实际参数。let latin = compose(map(angry), reverse)latin([\"frog\", \"eyes\"])// [\"EYES!\", \"FROG!\"]) 如果在 debug 组合的时候遇到了困难，那么可以使用下面这个实用的，但是不纯的trace 函数来追踪代码的执行情况。 123456789101112let trace = curry(function(tag, x)&#123; console.log(tag, x) return x&#125;)let dasherize = compose(join('-'), toLower, split(' '), replace(/\\s&#123;2,&#125;/ig, ' '))dasherize('The world is a vampire')// TypeError: Cannot read property 'apply' of undefinedlet dasherize = compose(join('-'), toLower, trace(\"after split\"), split(' '), replace(/\\s&#123;2,&#125;/ig, ' '));// after split [ 'The', 'world', 'is', 'a', 'vampire' ] trace 函数允许我们在某个特定的点观察数据以便 debug。像 haskell 和 purescript 之类的语言出于开发的方便，也都提供了类似的函数。 组合将成为我们构造程序的工具，而且幸运的是，它背后是有一个强大的理论做支撑的。 范畴学（category theory）范畴学（category theory）是数学中的一个抽象分支，能够形式化诸如集合论（settheory）、类型论（type theory）、群论（group theory）以及逻辑学（logic）等数学分支中的一些概念。范畴学主要处理对象（object）、态射（morphism）和变化式（transformation），而这些概念跟编程的联系非常紧密。下图是一些相同的概念分别在不同理论下的形式： 在范畴学中，有一个概念叫做…范畴。有着以下这些组件（component）的搜集（collection）就构成了一个范畴： 对象的搜集 态射的搜集 态射的组合 identity 这个独特的态射 范畴学抽象到足以模拟任何事物，不过目前我们最关心的还是类型和函数，所以让我们把范畴学运用到它们身上看看。 对象的搜集 对象就是数据类型，例如 String 、 Boolean 、 Number 和 Object 等等。通常我们把数据类型视作所有可能的值的一个集合（set）。像 Boolean 就可以看作是 [true, false] 的集合， Number 可以是所有实数的一个集合。把类型当作集合对待是有好处的，因为我们可以利用集合论（set theory）处理类型。 态射的搜集 态射是标准的、普通的纯函数。 态射的组合 这就是本章介绍的新玩意儿—— 组合 。我们已经讨论过compose 函数是符合结合律的，这并非巧合，结合律是在范畴学中对任何组合都适用的一个特性。 123let g = function(x)&#123; return x.length &#125;let f = function(x)&#123; return x === 4; &#125;let isFourLetterWord = compose(f, g) identity 这个独特的态射 让我们介绍一个名为 id 的实用函数。这个函数接受随便什么输入然后原封不动地返回它： 1let id = function(x) &#123;return x&#125; id 函数跟组合一起使用简直完美。下面这个特性对所有的一元函数（unary function）（一元函数：只接受一个参数的函数） f 都成立： 123// identitycompose(id, f) == compose(f, id) == f// true 这就是实数的单位元（identity property）嘛！慢慢理解它的无用性，很快我们就会到处使用 id 了，不过暂时我们还是把它当作一个替代给定值的函数。这对写 pointfree 的代码非常有用。 除了类型和函数，还有什么范畴呢？还有很多，比如我们可以定义一个有向图（directed graph），以节点为对象，以边为态射，以路径连接为组合。还可以定义一个实数类型（Number），以所有的实数为对象，以 &gt;= 为态射（实际上任何偏序（partial order）或全序（total order）都可以成为一个范畴）。范畴的总数是无限的，但我们只需要关心上面定义的范畴就好了。 总结组合像一系列管道那样把不同的函数联系在一起，数据就可以也必须在其中流动——毕竟纯函数就是输入对输出，所以打破这个链条就是不尊重输出，就会让我们的应用一无是处。我们认为组合是高于其他所有原则的设计原则，这是因为组合让我们的代码简单而富有可读性。另外范畴学将在应用架构、模拟副作用和保证正确性方面扮演重要角色。 示例应用声明式代码我们要开始转变观念了，从本章开始，我们将不再指示计算机如何工作，而是指出我们明确希望得到的结果。 与命令式不同，声明式意味着我们要写表达式，而不是一步一步的指示。 以 SQL 为例，它就没有“先做这个，再做那个”的命令，有的只是一个指明我们想要从数据库取什么数据的表达式。至于如何取数据则是由它自己决定的。以后数据库升级也好，SQL 引擎优化也好，根本不需要更改查询语句。这是因为，有多种方式解析一个表达式并得到相同的结果。 1234567// 命令式let makes = []for (i = 0; i &lt; cars.length; i++) &#123; makes.push(cars[i].make)&#125;// 声明式let makes = cars.map(function(car)&#123; return car.make &#125;) 命令式的循环要求你必须先实例化一个数组，而且执行完这个实例化语句之后，解释器才继续执行后面的代码。然后再直接迭代 cars 列表，手动增加计数器，把各种零零散散的东西都展示出来…实在是直白得有些露骨。 使用 map 的版本是一个表达式，它对执行顺序没有要求。而且， map 函数如何进行迭代，返回的数组如何收集，都有很大的自由度。它指明的是 做什么 ，不是怎么做。因此，它是正儿八经的声明式代码。 再看一个例子。 12345678let authenticate = function(form)&#123; let user = toUser(form) return logIn(user) // return logIn(toUser(form))&#125;let authenticate = compose(logIn, toUser) 虽然命令式的版本并不一定就是错的，但还是硬编码了那种一步接一步的执行方式。而 compose 表达式只是简单地指出了这样一个事实：用户验证是 toUser和 logIn 两个行为的组合。这再次说明，声明式为潜在的代码更新提供了支持，使得我们的应用代码成为了一种高级规范（high level specification）。 因为声明式代码不指定执行顺序，所以它天然地适合进行并行运算。它与纯函数一起解释了为何函数式编程是未来并行计算的一个不错选择——我们真的不需要做什么就能实现一个并行／并发系统。","categories":[{"name":"study","slug":"study","permalink":"https://yjhmelody.github.io/categories/study/"}],"tags":[{"name":"JS","slug":"JS","permalink":"https://yjhmelody.github.io/tags/JS/"},{"name":"FP","slug":"FP","permalink":"https://yjhmelody.github.io/tags/FP/"}]},{"title":"DNS知识点","slug":"计算机网络实验","date":"2017-11-11T01:40:04.000Z","updated":"2017-12-10T09:17:52.896Z","comments":true,"path":"2017/11/11/计算机网络实验/","link":"","permalink":"https://yjhmelody.github.io/2017/11/11/计算机网络实验/","excerpt":"DNSDNS 是域名系统（Domain Name System）的缩写，它是一种用于 TCP/IP 应用程序的分布式数据库，它提供主机名字和 I P 地址之间的转换及有关电子邮件的选路信息。所谓“分布式”是指在 Internet 上的单个站点不能拥有所有的信息。每个站点（如大学中的系、校园、公司或公司中的部门）保留它自己的信息数据库，并运行一个服务器程序供 Internet上的其他系统（客户程序）查询。 DNS 命名方式中，采用了分散和分层的机制来实现域名空间的委派授权，以及域名与地址相转换的授权。通过使用 DNS 的命名方式来为遍布全球的网络设备分配域名，而这则是由分散在世界各地的服务器实现的。","text":"DNSDNS 是域名系统（Domain Name System）的缩写，它是一种用于 TCP/IP 应用程序的分布式数据库，它提供主机名字和 I P 地址之间的转换及有关电子邮件的选路信息。所谓“分布式”是指在 Internet 上的单个站点不能拥有所有的信息。每个站点（如大学中的系、校园、公司或公司中的部门）保留它自己的信息数据库，并运行一个服务器程序供 Internet上的其他系统（客户程序）查询。 DNS 命名方式中，采用了分散和分层的机制来实现域名空间的委派授权，以及域名与地址相转换的授权。通过使用 DNS 的命名方式来为遍布全球的网络设备分配域名，而这则是由分散在世界各地的服务器实现的。 DNS 工作流程域名服务分为客户端和服务器端，客户端提出请求，询问一个 Domain Name 的 IP 地址，服务器端必须回答客户端的请求。本地 DNS 首先查询自己的数据库，如果自己的数据库中没有对应的 IP 地址，则向本地 DNS 上所设的上一级 DNS 询问，得到结果之后，将收到的结果保存在高速缓冲区，并回答给客户端。标识字段由客户程序设置并由服务器返回结果。客户程序通过它来确定响应与查询是否匹配。16 bit 的标志字段被划分为若干子字段 DNS 协议标志中每一位的含义如下： QR：是 1 bit 字段，0 表示查询报文，1 表示响应报文。 Opcode：报文类型，是一个 4 bit 字段，通常值为 0（标准查询），其他值为 1（反向查询）和 2（服务器状态请求）。 AA：是 1 bit 字段，表示“授权回答（authoritative answer）”，如果此位为 1，表示服务器对问题部分的回答是权威性的。 TC：是 1 bit 字段，表示“可截断的（truncated）”。使用 UDP 时，它表示当应答的总长度超过 512 字节时，只返回前 512 个字节。 RD：是 1 bit 字段，表示“期望递归（recursion desired）”。该比特能在一个查询中设置，并在响应中返回。这个标志告诉名字服务器必须处理这个查询，也称为一个递归查询。如果该位为 0，且被请求的名字服务器没有一个授权回答，它就返回一个能解答该查询的其他名字服务器列表，这称为迭代查询。 RA：是 1 bit 字段，表示“可用递归”。如果名字服务器支持递归查询，则在响应中将该比特设置为 1。 Zero：随后的 3 bit 字段必须为 0。 Rcode：是一个 4 bit 的返回码字段。通常的值为 0（没有差错）和 3（名字差错）。名字差错只有从一个授权 DNS 服务器上返回，它表示在查询中制定的域名不存在。随后的 4 个 16 bit 字段说明最后 4 个变长字段中包含的条目数。对于查询报文，问题（question）数通常是 1，而其他 3 项则均为 0。类似地，对于应答报文，回答数至少是 1，剩下的两项可以是 0 或非 0。 DNS 抓包这里DNS报文首部前16bit格式如下： DNS服务器通过Transaction ID来分辨同台主机的不同请求。 我打开浏览器访问百度 我请求百度dns的字段如下： 这里的前16bit是0x0100 而dns的响应如下： 如果请求的域名不存在，则如下：","categories":[],"tags":[]},{"title":"2017-11-5","slug":"2017-11-5","date":"2017-11-05T12:17:06.000Z","updated":"2017-11-05T13:44:44.010Z","comments":true,"path":"2017/11/05/2017-11-5/","link":"","permalink":"https://yjhmelody.github.io/2017/11/05/2017-11-5/","excerpt":"10月10月就这样过去了。当我觉得这样的10月是多么繁忙的时候，11月时候的我告诉自己，其实大部分时间是自己折腾自己罢了，而且毫无意义。","text":"10月10月就这样过去了。当我觉得这样的10月是多么繁忙的时候，11月时候的我告诉自己，其实大部分时间是自己折腾自己罢了，而且毫无意义。 10月的作业债11月总是要还的。现在回想起来，好像是各个方面都在学习，但如蜗牛一般，于是乎，忘记自己走到了哪里。唯一欣喜的是，我开始接受其他书籍的洗礼了。之前从同学那儿借来了东野圭吾的《解忧杂货店》，每天熄灯后一小时都在咀嚼此书。跟计算机书籍比起来，这样的书确实容易读很多，不费太多的时间就能通读一遍，而且相当的有收获。当然，如果一本书没有从思想层面影响你，或者没有改变你的某个观念，或者没有改变你的一个习惯，或者没有引起你一点儿的思考，这样的书大概是不值得去读的。如某些读者所述，这本书确实在某方面治愈了我，让我感觉到人间一些温情，人与人之间不可思议的联系，很多无意义之事其实蕴含意义，不论好坏。其实我极力追求做一个足够纯粹的人，而我理解的纯粹是指一个的品质而不是性格。这本书里的人有些已经达到了我想追求的纯粹了。而我对纯粹的一些理解就是看淡大部分事情，对人的追求做到坦实真诚，对事的追求做到实事求是，对物的追求做到己所不欲。当然这里的追求比较广义。可能思想更接近于道家，当然我完全不懂这些思想家的大道理。讽刺的是，我一直没有做好，不然可能也不会写这些了。 刚刚提到走着走着忘记走到了哪里。其实就是没有把握本心的缘故吧。原因无外乎就是意志力的不坚定，间接性情绪调节不良。10月份做了什么自己满意的事情？有把该完成的都及时正确地完成吗？除了课内，课外又扩展了多少？写到这里，能回答上的也就是10月份开始的跑步吧，频率还是挺高的，1周能跑4，5次。作业能提早完成的后来质量都出了问题……没有提早完成的基本拖到deadline。满意的事情，还真没有，我这个人不容易对事情感到满足，因为很容易就松懈下来，但实际上大部分时候我松懈的原因是因为失去了耐性或者热情，在这点上，还不如中学时代的自己，中学的我虽然在各方面的都很平庸（虽然现在依然），但做什么事情都饱含热情，是真的喜欢生活，喜欢与人交流。 11月11月的第一周，基本宣告了学期过半。有什么收获自己心里清楚，没有完成什么自己心里清楚，还想做什么自己心里也清楚。人啊，总是会把生活中大部分事情当作理所当然，所以才不容易改变。我极力改变这样的思维，如何看待周围是一件非常严肃，非常需要思考，却被人忽略的事情啊。许多时候不能总保持着“我现在的这样状态真的很好啊”的想法，于是乎还想证明给其他人看。可能还有一种思考角度，我尝试往一个可能更好的习惯学习，如果反而没有帮助，再退回来。前者是以不变应万变，后者是以变应万变。最近开始尝试读一些文学作品，也是从此角度出发。一直读专业相关的作品或资料，当然对学习有很大帮助，但是人很难再脱离自己给自己建造的围墙了，它们既是梯子也是围墙啊。再谈到待人之事，摈弃过去看人优缺点的角度，而尝试以对方的角度看待对方自己，才能做到了解他人吧。 然后是想反思一下一直以来的一些学习缺陷。看比较费理解的书，还是容易倾向于不去深思，原因嘛，就是怕麻烦，觉得这是一场时间黑洞的无意义之旅，然后是有点急于求成，过分看轻一些基本之理。不过还是要承认下自己的记忆力不行，对经历的事情还算有正常的记忆力，但对文字和视频的记忆力特别差，目前感觉原因尚不明朗。 昨天把从同学那里借到了《独唱团》读了半本，今天才查了一下，原来是只出过一期，这个期刊本身还颇有一些故事。当然本身的内容也是不错，作者大多是80后70后吧？聚焦于上个世纪的中国，讲述各种经历，每篇都能透入一些观点，或批判或赞扬。 而我已经好久没有看过这样的散文、诗句、问答、半小说半叙事的文章了。看来我还是不排斥文学作品的，高中每周也会去买杂志跟期刊，看看感兴趣的内容，喜欢看里面待人处事的观点。我敢说我现在的思想跟性格被高中时代接触的内容严重影响了，即使现在我已经几乎想不起那些内容。只记得，他们在谈论生活，他们在谈论文化，透露着思想…… 里面的图片也总是能触动许多，有时候能盯着图片一分钟，思考着什么我早已经忘记了，大概在思考着他们所思考的事情。我想这本书，我应该也会尝试读完吧，感觉把过去的习惯捡起来，就好像在跟老朋友拥抱一般，非常的踏实。 发现写到这里，已经过去整整一小时了。大概我的脑子里又本能的计算着花一小时做这些事情的利弊了。看过的许多作品，我发现我都不能很好地把他们表达出来，或者借用一下，我想有机会写写探讨某些作品本身的博客，目前来说可能还不合适，阅历可能还不够。 已经九点半了，面对着书架上近百本的书，其实心里滋味很难描述。","categories":[{"name":"life","slug":"life","permalink":"https://yjhmelody.github.io/categories/life/"}],"tags":[{"name":"life","slug":"life","permalink":"https://yjhmelody.github.io/tags/life/"}]},{"title":"Install MySQL5.7","slug":"Install-MySQL5-7","date":"2017-10-26T05:30:18.000Z","updated":"2017-10-26T05:37:01.302Z","comments":true,"path":"2017/10/26/Install-MySQL5-7/","link":"","permalink":"https://yjhmelody.github.io/2017/10/26/Install-MySQL5-7/","excerpt":"大二数据库原理的作业，发现还存在电脑里，于是稍微整理下放到博客里。","text":"大二数据库原理的作业，发现还存在电脑里，于是稍微整理下放到博客里。 windows下mysql安装我下载的是zip压缩包，假设解压到D:\\software目录下，官网下载的只有服务端和简单的shell，可以去搜索Navicat客户端，支持多种数据库管理系统，下载MySQL客户端，也支持MariaDB使用。 注意：Navicat是收费的，免费使用14天。你还可以使用其他客户端，比如phpAdminMySQL。 mysql的简单配置mysql的目录结构如下 环境变量需要配置bin目录，mysql的程序都在bin目录下，所以环境变量是在PATH变量上新增D:\\software\\mysql\\bin。 各个版本目录可能不同，data目录存放系统数据，一开始应该没有，需要自己新建。 另一个重要的是my.ini文件(或my-default.ini)，需要进行一些简单的配置才能使用。 123456789101112131415161718[mysql]# 设置mysql客户端默认字符集default-character-set = utf8[mysqld]#skip_grant_tables 这个先不要，这是忽略权限。#设置3306端口port = 3306# 设置mysql的安装目录basedir = \"D:/software/mysql-5.7.16-winx64\"# 设置mysql数据库的数据的存放目录datadir = \"D:/software/mysql-5.7.16-winx64/data\"# 允许最大连接数max_connections = 200# 服务端使用的字符集默认为8比特编码的latin1字符集，我们改为通用的utf8character-set-server = utf8# 创建新表时将使用的默认存储引擎default-storage-engine = INNODB 常用的2个程序是mysql和mysqld，需要给服务端设置一些配置，如上，语法跟作用一目了然。 windows下运行mysql需要使用cmd管理员权限，使用net start mysql 启动mysql服务，net stop mysql 关闭服务。 一开始需要使用root帐号登录，不需要密码，在命令行下使用mysql -u root -p登录，进入shell后可以使用help命令查看简单的用法。 具体其他使用方法请查看文档手册或者搜索引擎。 Navicat 客户端简单使用按照安装包的引导安装即可。 注意使用期限是14天。 Navicat界面比较简洁，也只提供基本的功能，如需要使用强大的功能可能需要购买或使用其他客户端。 Navicat窗口帮助下可以打开本地中文文档，可以查看各种数据库的使用。 在文件下建立数据库连接写好帐号密码即可连接（不要忘记先在cmd管理员权限下启动mysql服务）。 然后可以简单的开始操作DBMS了。 linux ubuntu 命令行下mysql安装我选择最简单的安装方式 如上，用sudo apt install mysql-server-5.7下载 当然你可以先用图形界面找到需要的版本的包或者路径，用wget等工具下载，这里不详细说明，总体思路跟windows下是一致的。 用 whereis mysql 命令查看mysql等命令已经添加到环境变量中。 如图，用sudo service mysql start 启动，即使远程连接断开也在后台运行，mysql -u root -p登录root用户连接mysql，可以用ps -ef | grep mysql 来查看。 登录以后，在mysql下用show variables like &#39;character% 可以查看一些字符相关的环境配置，发现很多是latin1编码，我们需要改为utf8编码 我们来到/etc/mysql目录下，这里存放许多mysql的配置。我们修改一些配置sudo vim mysql.conf.d/mysqld.cnf，在[mysqld]下配置character-set-server = utf8，当然在这里你还可以修改一些其他配置。 继续修改另外一个配置sudo vim conf.d/mysql.cnf，在[mysql]下配置default-character-set = utf8，这样重启mysql后字符编码就改变了如下： 以上，最基本的配置就完成了，大概作为学习用途已经足够了，再复杂的配置得参考相应的资料了。","categories":[{"name":"study","slug":"study","permalink":"https://yjhmelody.github.io/categories/study/"}],"tags":[{"name":"Database","slug":"Database","permalink":"https://yjhmelody.github.io/tags/Database/"}]},{"title":"VAET阅读稿","slug":"VAET阅读稿","date":"2017-10-11T10:53:34.000Z","updated":"2017-10-13T08:13:56.481Z","comments":true,"path":"2017/10/11/VAET阅读稿/","link":"","permalink":"https://yjhmelody.github.io/2017/10/11/VAET阅读稿/","excerpt":"VAET: A Visual Analytics Approach for E-transactions Time-Series 电子交易时间序列的一种可视化分析方法 Cong Xie, Wei Chen, Member, IEEE, Xinxin Huang, Yueqi Hu, Scott Barlowe, and Jing Yang 创新实践的论文阅读作业，翻译了主要内容，加上部分自己的理解并修改部分内容，保留了部分原文，有时候阅读原文更好理解。全文按照论文组织的方式编写，主要是为了理清楚论文的主要思想，并可以简单地给其他人做宏观上的解释，达到基本的教学目的。此文将作为ppt的前稿。","text":"VAET: A Visual Analytics Approach for E-transactions Time-Series 电子交易时间序列的一种可视化分析方法 Cong Xie, Wei Chen, Member, IEEE, Xinxin Huang, Yueqi Hu, Scott Barlowe, and Jing Yang 创新实践的论文阅读作业，翻译了主要内容，加上部分自己的理解并修改部分内容，保留了部分原文，有时候阅读原文更好理解。全文按照论文组织的方式编写，主要是为了理清楚论文的主要思想，并可以简单地给其他人做宏观上的解释，达到基本的教学目的。此文将作为ppt的前稿。 INTRODUCTION 介绍The E-transaction time-series contains transactions among multiple users in a time range. Each record contains a time stamp, the IDs of the seller and buyer, and the associated attributes of the commodities. Each record is an atomic element representing an online transaction among a seller and a buyer. 电子交易时间序列包含时间范围内的多个用户之间的交易。 每个记录都包含时间戳，卖方和买方的ID以及相关联的商品的属性。每个卖方和买方之间的网上交易记录都是一个原子元素。 在时间上下文中分析电子交易时间序列至关重要了解交易行为，学习用户偏好和发现时间趋势。 通过面试分析师发现以下一些问题常常很难回答： 卖方的多个交易之间的时间和上下文关系是什么？比如短时间内有个卖家有大量的交易，而且可能来自某个买家，分析师需要发现这样的交易的各种属性。 最常见的交易模式是什么？比如平时工作日交易比较稀疏，但是圣诞节交易比较频繁。 如何识别有趣的交易？比如买家勾结来加速影响卖家的信用，一旦这样的模式定义好了，分析师需要在相关的大量数据集中找到特定属性值来挖掘这样的交易。 如何在上下文检测某个交易？比如小额购买巨量的商品可能是个假的交易，利用它来提升卖家排名。为了确认交易是假的，分析师需要将交易与买方和卖方的信息关联。 我们认为，自动数据挖掘过程在回答上述问题时并没有足够的灵活性和准确性。因此迫切需要视觉分析方法，使分析人员能够通过集成计算能力，人类知觉能力和领域知识，通过即时视觉反馈灵活地形成和测试假设。 目前还没有适用于上述情况的可视化方法。多变量时间序列可视化的现有研究工作主要集中在总结多维度的全局和/或时间趋势或发现个体维度的模式，如Sparklines。 因此提出了一种新颖的视觉分析系统，称为电子交易时间序列的视觉分析（VAET），旨在探索电子交易时间序列，以便在时间上下文中分析多个用户之间的交易模式。VAET有如下2个主要的可视化分析组件： Overview: This component helps the analysts effectively identify salient transactions from a large dataset. 该组件帮助分析师在大量数据集中快速找到突出的交易。VAET使用概率决策树学习器首先计算每个交易的显着性值，以揭示其与分析目标的相关性（例如，作为假交易的可能性）。然后，显着值显示在一个称为显着时间映射（TOS）的像素方向的显示中。该映射提供了一个工作空间来探索和选择不同时间粒度的潜在有趣的交易; Detail view：This component allows the analysts to conduct detailed examination on interesting transactions for insights. 该组件允许分析人员对感兴趣的交易进行仔细检查，以获取见解。特别地，从概述中选择的交易使用称为KnotLines的新的视觉隐喻来显示。协调TOS映射和KnotLines，以便分析人员可以快速识别来自大型数据集的有趣事务。 A case study and a user study with a real online transaction dataset demonstrated that VAET was effective in supporting a variety of analysis tasks.真实在线交易数据集的案例研究和用户研究表明，VAET有效支持各种分析任务。 VAET的主要贡献包括： 视觉分析系统，允许分析人员在时间上有效地分析大型电子交易时间序列; 从大型数据集中检测和可视化突出事务的方法; 一种新颖的视觉隐喻，用于紧凑地放置和编码特征属性以及多用户事务的时间和上下文相关性 RELATED WORK 相关工作Visual Analysis of E-transaction Data 电子交易数据的可视化分析以下是之前一些人做的过相关研究（寻找合适的可视化方法来展示交易数据） The transaction data contains various types of attributes, such as numerical, temporal and categorical. The Sparklines [23] can be used to visualize multiple trends in financial data. Liu et al. [11] proposed a visualization system called SellTrend for analyzing airline travel purchase requests. WireVis [3] was proposed to search on predefined patterns in large wire transaction datasets. Visual analytics approaches have been proposed to explore web clickstreams of online transactions [26]. Our approach is among the earliest visual analytics approaches for the exploration of temporal and contextual connections in multiuser transactions. Transaction data often have multi-dimensional attributes. Analyzing them often requires the integration of well-designed data mining models. Probabilistic models are employed to model user behavior [12], resulting in user clusters. This scheme has been successfully applied to classify E-transaction data into different types [2]. Association analysis is another widely used model for transaction data. Hao et al. [7] proposed the DAV system to visualize the relationships of associated products. Visual Analysis of User Behavior Time-Series 用户行为时间序列的可视化分析以前有很多关于用户行为时间序列分析和可视化的作品。这里我们只总结最相关的一些，并将它们分类为分析个人行为，用户交互和组行为的技术。 Temporal Individual Behavior Patterns Many visualization approaches designed to analyze user behavior data are focused on exploring the temporal behavior patterns of individuals.旨在分析用户行为数据的许多可视化方法都集中在探索个人的时间行为模式。TimeSearcher [8] allows users to select interesting time-series using a rectangular query region. LifeLines [16] visualizes health-related incidents of patients along a timeline. Most previous works utilize high-dimensional visual exploration tools such as parallel coordinates [4] to explore extracted patterns. Density-based display techniques [6], [10] are capable of showing large time-series datasets for real-time monitoring. Additional visual exploration techniques include time trajectory [21] and [13]. User Interaction Patterns conventional solutions consider the user network as a social network and analyze its global structure常规解决方案将用户网络视为社会网络，并分析其全局结构。Sallaberry et al. [20] provide an overview of dynamic network evolution over time. Other approaches emphasize the user interaction characteristics such as email connections [25] and instant messages [27]. However, these methods are focused on the structural changes rather than the temporal variations of the interactions. Other approaches aim to reveal the relationships among multiple users in a temporal context. For instance, Storyline [22] shows the narrative threads that form a plot or a subplot in works of fiction. The history flow approach successfully reveals author collaboration patterns [24]. Code Swarm [15] visualizes the animated histories of software project evolution. VAET reveals both the temporal patterns of multi-user behavior and their atomic level correlations. It improves the above approaches by allowing the analysts to explore a large number of transactions at different granularities. Problem definition 问题定义 Multi-user transaction data is a special type of user behavior data with a focus on characterizing raw, detailed, and subtle inter-user transactions. An E-transaction time-series dataset contains information about each E-transaction, including information about transaction time, the buyer, and the seller. Each E-transaction records a transaction between a buyer and a seller. 多用户事务数据是一种特殊类型的用户行为数据，重点是描述原始，详细和微妙的用户间交易。电子交易时间序列数据集包含每个电子交易的信息，包括有关交易时间，买方和卖方的信息。 每个电子交易记录买方和卖方之间的交易。 一般来说，一个电子交易包含以下属性： User information includes the IDs and other information about the buyer and the seller who make the transaction, e.g., their age group, gender, and location. Transaction information includes the time stamp and other information about the commodities, e.g., the payment amount, the number, and the sales category of the commodity. The above attributes can be numerical, ordinal, categorical, textual, or temporal.上述属性可以是数字，序数，分类，文本或时间。 分析师通常通过一系列低级别任务进行复杂的任务。 这些任务通常关注卖方的行为，例如： 识别感兴趣的时段和/或销售类别。 识别具有特定属性的有趣模式的交易（例如，支付金额≥500）并检查其详细信息。 识别具有有趣交易模式的卖家，例如卖家以小额付款金额进行频繁交易。 检查特定卖家的交易模式 The analysts usually conduct a complex task through a set of low level tasks. These tasks typically focus on the behavior of the seller, such as: T1 Identifying time periods and/or sales categories of interest. T2 Identifying transactions with interesting patterns in specific attributes(e.g., payment amount ≥ 500) and examining their detailed information. T3 Identifying sellers with interesting transaction patterns, such as a seller making frequent transactions with small payment amounts. T4 Examining the transaction patterns of a specific seller 我们使用术语显着定量地描述交易与分析师定义的目标的相关程度。根据调查，识别和审查突出交易是电子交易时间序列探索中至关重要但具有挑战性的任务。通常，分析师需要通过迭代查询数据集并检查检索到的事务之间的属性值和关系来手动识别突出事务。此外，分析师经常需要检查突出交易以及用户的历史数据等信息，以证明其行为或揭示有趣的模式。这个过程通常是费力和乏味的。VAET旨在简化此过程，提高整体运行效率。（VAET is designed to ease this process and improve the overall operation efficiency.） APPROACH OVERVIEW 方法概览The goal of VAET is to identify and explore interesting transactions by selecting those with high saliency and studying them. This is accomplished by integrating the capabilities of both data mining and visualization techniques within the following iterative visual exploration pipeline. VAET的目标是通过选择具有高度显着性并研究它们来识别和探索有趣的交易。这是通过将数据挖掘和可视化技术的功能集成在以下迭代视觉探索流程中来实现的。 步骤1，使用决策树的显着计算：从每个事务中提取一组特征。分析人员将某些交易的功能手动标记为训练数据。使用这些特征，在训练数据上构建概率决策树学习器。然后用它来产生每个未标记交易的显着值（图2（b））。 步骤2，使用TOS映射进行浏览和选择：所有事务的显着性值映射到紧凑的基于密度的生存时间（TOS）映射。在此映射中，交易按时间和类别排序，并以颜色对应于显着值的像素表示。分析师可以交互地探索映射，调查全球分布和地域格局，并选择根据这个观点的显着性值，有趣的交易。（图2（c））。 步骤3，使用KnotLines进行详细分析：分析师选择的交易通过一种新颖的视觉隐喻KnotLines可视化，允许研究多个属性和上下文连接（图2（d））。分析人员确定为突出事务的交易可以被标记并反馈到步骤1以继续迭代过程（图2（e））。 Step 1 Saliency computation with decision tree: A set of features are extracted from each transaction. The analysts manually label the features of some transactions as the training data. Using these features, a probabilistic decision tree learner is constructed upon the training data. It is then employed to produce the saliency values for each unlabeled transaction (Figure 2 (b)). Step 2 Browsing and selection using the TOS map: The saliencyvalues of all transactions are mapped to a compact, density-based Time-Of-Saliency (TOS) map. In this map, transactions are ordered by time and categories and represented by pixels whose colors correspond to saliency values. The analysts can interactively explore the map, investigate the global distribution and local patterns, and select interesting transactions according to the saliency values from this view. (Figure 2 (c)). Step 3 Detailed analysis using KnotLines: The analyst-selected transactions are visualized with a novel visual metaphor, KnotLines, that allows the study of multiple attributes and contextual connections (Figure 2 (d)). The transactions identified as salient by the analysts can be labeled and fed back into Step 1 to continue the iterative process (Figure 2 (e)). 分析人员可以通过调整标记的数据集，导航映射和探索有趣的交易来迭代地循环上述步骤。 TOS映射和KnotLines可视化提供可扩展的探索，如时间间隔选择和详细审查。 SALIENCY COMPUTATION WITH DECISION TREE 决策树的显著性计算计算显着性值本质上是上下文感知和任务定位的。 对于许多任务，显着性值不能直接从事务属性导出。 例如，当分析师搜索异常交易时，往往需要考虑卖方的交易频率。 让分析人员手动指定每个交易的显着性值也是不切实际的。 因此，我们建议通过定义和计算一组交易的特征来计算每个记录的显着性值。特别地，我们的方法通过概率决策树计算显着值作为概率估计问题。 我们选择决策树，因为它可以处理连续和分类的属性，很容易解释。 决策树最初由一组分析师确定的训练数据的特征构建。 将决策树应用于每个未标记事务的特征，产生的概率范围为0到1，用作底层事务的显着值。 分析师手动标记为交易的交易可以在随后的分析中添加到训练数据集中（图2（e））。 Feature Extraction 特征提取VAET计算一组分析师指定的每个事务的时间和上下文特征作为一组特征。 一般来说，定义了三种类型的特征： 基本特征 确定交易是否有趣的一个直接方法是使用指定属性的值作为基本特征，例如商品的支付金额。另外，分析人员可以定义新的属性。例如，如果卖家在分析师给出的有趣的列表中，则他或她被视为显着的卖家，如图3所示。这些属性的集合构成一组基本功能。 文本功能 交易可以包含文本信息，例如商品的评论。 VAET检查文本信息是否包含分析师指定列表中的敏感词。分析人员保留一个字典，用于从过去几个月手动收集敏感的词汇和短语。例如，在一种欺诈交易中，买家希望尽快回收现金。 “现金回馈”是一个敏感的短语。敏感词在不同的情况下有所不同，可以视为文字特征。 时间特征 交易序列的时间模式对于识别数据集中的有趣模式至关重要。例如，卖方在时间间隔内的交易金额表示他或她的受欢迎程度。然而，以传统的决策树方法难以发现面向时间的关系。为了解决这个问题，VAET使用卖方在每个时间间隔的交易频率作为衡量时间趋势。时间间隔的大小取决于数据收集配置。 Basic Features One straightforward way to determine whether a transaction is interesting is to use the values of specified attributes as basic features, such as the payment amount of a commodity. In addition, the analysts can define new attributes. For example, if a seller is in the interesting list given by the analyst, he or she is considered as a salient seller, as shown in Figure 3. The collection of these attributes constructs a set of basic features. Textual Features A transaction may contain textual information, such as the comment of a commodity. VAET examines whether the textual information contains sensitive words in a analyst-specified list. The analysts keep a dictionary for sensitive words and phrases collected manually from the past several months. For example, in a kind of fraud transactions, the buyers want their cash back as soon as possible. “cash back” is a sensitive phrase here. Sensitive words vary in different situations, and can be regarded as textual features. Temporal Features Temporal patterns of a sequence of transactions are essential for identifying interesting patterns in the datasets. For example, the transaction amount of a seller in a time interval indicates his or her popularity. However, time-oriented relations are difficult to discover with conventional decision tree approaches. To address this problem, VAET uses the transaction frequency of the seller in every time interval as a measure of the temporal trend. The size of the time interval depends on the data collection configuration. Estimating saliency using Probabilistic Decision Tree 使用概率决策树估计显着性决策树最初使用训练数据集构建，该数据集由分析师标记的交易的提取功能组成。 如图2（e）所示，可以通过添加分析员标识的事务，在可视化探索过程中手动更新训练数据集。 在我们的方法中，使用完善的C4.5算法从训练数据中自动构建决策树，其根据特征将训练集递归地分解为子集。 在决策树中（参见图4的示例），叶子节点表示类（显着或非显着2个类别），内部节点对应于特征。 在每个内部节点处，C4.5根据产生最高归一化信息增益（highest normalized information gain）的特征将样本分解为子集，并将特征分配给该节点 概率根据决策树叶上的交易进行估计。 我们将FP表示为叶上的假阳数，T P表示真阳数（见图5中的混淆矩阵）。 叶上的概率分布估计由下式给出: P(y|x) = TP / (TP + FP) TIME-OF-SALIENCY MAP: BROWSING A LARGE SET OF TRANSACTIONS 显著性时间映射：浏览大量交易集 Generation of Time-Of-Saliency Map 生成显著性时间映射TOS映射是基于2D密度的展示，沿着水平轴的时间和由销售类别（例如，“电子配件”和“衣服”）组织的垂直轴TOS映射均匀分割成行，每个行代表一个销售类别。在图（a）中，由蓝色框突出显示的颜色矩形提供了垂直轴上类别的视觉索引。此外，每行按照时间间隔水平分割。根据其时间戳和销售类别每个事务被投射到相应的单元。投射到同一个单元格的所有事务的显着值相加，并将总和映射到单元格的颜色。色彩映射可以使用默认颜色标度或分析器指定的色标。所得到的TOS映射可视化地为分析任务编码事务的相关性。黑暗区域意味着一组潜在有趣的交易。特别地，连续的暗带表示在一段时间内相应的销售类别中的高度突出的交易（参见图1（a）中的TOS映射中的所选区域）。 Time-Of-Saliency Exploration 探索显著性时间映射TOS映射视图中提供了以下交互，可用于完成第一个任务（即识别感兴趣的时段和/或销售类别。） 时间窗口 TOS映射以分析人员可调节的时间间隔显示交易。可以使用额外的时间窗口小部件来定位视图的特定区域以进行进一步和详细的研究。分析师可以在时间选择栏上单击并拖动以设置TOS映射的时间窗口，如图5（a）中TOS映射顶部的突出显示。图5（b）显示了分析师设置时间窗口后的TOS映射。 感兴趣的区域 分析师可以点击类别索引（图5（a）中的蓝框）来选择同一类别的交易。 也可以使用套索工具来选择有趣的区域。 当选择一个区域时，将出现一个浮动文本框以显示有关该区域的信息。 所选交易的详细信息可以在第7节中描述的KnotLines视图中进一步可视化和探索。此外，还提供了一个条形图视图（图1（c））以显示所选数据中的类别的销售量。 Time Windowing The TOS map shows the transactions in an analyst-adjustable time interval. An additional time windowing widget can be used to locate a specific region of the view for further and detailed study. Analysts can click and drag on the time selection bar to set the time window of the TOS map, as highlighted over the top of the TOS map in Figure 5 (a). Figure 5 (b) shows the TOS map after the analyst sets the time window. Region-Of-Interest Selection The analysts can click on the category index (the blue box in Figure 5 (a)) to choose the transactions of the same category. A lasso tool can also be used to select interesting regions. When a region is selected, a floating text box will appear to show the information about the region. The detailed information of selected transactions can be further visualized and explored in the KnotLines view described in Section 7. In addition, a bar chart view (Figure 1 (c)) is provided to show the sales volume of categories in the selected data. KNOTLINES: EXAMINING TRANSACTIONS IN DETAIL 详细审查交易KnotLines允许分析师对从TOS映射中选择的显着交易进行详细分析。它旨在解决任务2到任务4。KnotLines可视化显示两种类型的信息：属性和交易的时间趋势。 Data Organization and Visual Layout 数据组织和视觉布局为了研究交易之间的属性相似性和时间相关性，所选择的交易集被组织成一个三级分层树（图6）。 首先，我们使用矩阵表来可视化事务的组织，如图7（a）所示。 一级 整个选定的交易集根据不同的卖家分为N组（1级）。图7（a）中的每一行代表一个组。一组包含卖家的所有交易。这些组沿垂直轴从上到下列出。 二级 一组中的交易根据其时间戳进一步分为子组（2级）。图7（a）中的水平轴表示时间。每行分别对应于M个时间间隔的时间轴上的M个正方形。所有间隔的长度相同，可以调整以探索不同粒度的数据。属于相同时间间隔的卖方的交易形成一个子组（2级）。 三级 根据销售类别（例如，“图书”），将一个子组进一步分为（级别3）。在图7（a）中，每个正方形被分割成K个细胞，每个细胞代表一个部分。同一部分的交易由同一卖方作出，在同一时间间隔内进行，属于同一销售类别。 PS：即三级严格递增，一级是属于同一个卖家，二级属于同一个卖家某段时间，三级属于同一个卖家某段时间的某类销售类别。分别按行，按列，按细胞划分。这个是交易集逻辑上关系，而图7是一种为了方便分析而设计表示该关系的可视化方法。 Level One The whole selected transaction set is divided into N groups (level 1) according to different sellers. Each row in Figure 7 (a) represents a group. A group contains all transactions of a seller. The groups are listed from top to bottom along the vertical axis. Level Two The transactions in a group are further divided into subgroups (level 2) according to their time stamps. The horizontal axisin Figure 7 (a) represents the time. Each row is split into M squaresalong the time axis which correspond to M time intervals. The lengthsof all intervals are the same and can be adjusted to explore the data at different granularities. Transactions of a seller which fall into the same time interval form a sub-group (level 2). Level Three A subgroup is further divided into sections (level 3)according to the sales categories (e.g., “Books”). In Figure 7 (a), each square is segmented into K cells, each of which represents a section. Transactions in the same section are made by the same seller, take place in the same time interval, and belong to the same sales category. 因为大部分卖家的交易量在一天的时间内可能很高，因此图7（a）所示矩阵中的交易密度可能非常稀疏。 另外，组N的数量可能很大（例如，100万）。 为了使探索更加有效，矩阵式布局应重新设计为更紧凑。 VAET采用一个简单的两步启发式方案，对每个组进行操作。 在第一步中，删除第一个非空子组之前和最后一个非空子组之后的空子组。 这个步骤导致许多组仅覆盖水平空间的一小部分，因为它们中的大多数具有短的时间跨度。 为了增加空间效率，在第二步中启发式优化组的布局。 迭代布局策略用于满足以下原则： 整洁：组不应重叠; 紧凑型：空间利用率高; 代表：重要群体应优先展示 VAET employs a simple two-step heuristic scheme that operates on each group. In the first step, empty sub-groups before the first nonempty sub-group and after the last non-empty sub-group are removed. This step results in many groups that only cover a small portion of the horizontal space, because most of them have a short time span. To increase the space efficiency, the placement of the groups is heuristically optimized in the second step. An iterative layout strategy is used to satisfy the following principles: Uncluttered: groups should not overlap; Compact: space utilization should be high; Representative: important groups should have a display priority 分析人士指出， 这种设计（即图7） 有几个主要缺点： 由于部分可能包含数百个交易，可视化严重凌乱。 分析师建议在同一部分内汇总交易; 分析师认为，紧凑的布局是空间效率高的必需品。 然而，他们很难从这个视图来识别同一卖家所做的交易。需要额外的视觉属性来强调这一重要关系; 在此视图中没有提供有关交易的重要信息，如付款金额，交易是否缺少值，以及是否经常发生相同的交易。 The analysts pointed out that there were several major drawbacks in this design: (1) The visualization was seriously cluttered since a section may contain hundreds of transactions. The analysts suggested aggregating transactions within the same section; (2) The analysts agreed that the compact layout was necessary for high space efficiency. However, it was difficult for them to identify transactions made by the same seller from this view. Additional visual attributes were desired to emphasize this important relationship; (3) Important information about the transactions such as payment amount, whether a transaction had missing values, and if identical transactions occurred frequently, was not presented in this view. KnotLines （结线）为了解决上述问题，我们设计了一个增强的视觉隐喻调用KnotLines。它受到音乐符号的启发，这可以被看作是一个改进的散点图，它沿着时间轴放置不同类型的点（音符）。它是时间序列（例如节拍和节奏）及其连接的复杂视觉表示。 这里就不详细说明了，具体见表2。 表2 是Knotlines的可视化各个部分的含义| 图形部分 | 含义||—-|—-|| 视觉编码 | 交易数据|| 一个knotline | 同一卖家在不同时间的交易（一组）|| 一个knotbunch | 同一卖家在一段时间间隔内进行交易（一个子组）|| 茎长 | 在一段时间内同一卖家的交易总支付金额|| 一个结 | 在一段时间内，同一卖家与同名销售类别的交易（一节）|| 结的颜色 | 该结点的销售类别|| 结的大小 | 该结点的商品数量|| 一个未填充的结点 | 与卖方或买方位置异常的交易| Visual Exploration 可视化探索除了布局模式的规范和详细结点的调查之外，KnotLines还提供了一套用于分析多个结线的交互。 显着调制 KnotLines视图中显示的每个事务都包含一个显着值。 分析师可以显示从2D TOS映射中选择的所有交易，或仅显示显着值大于给定阈值（例如，0.8）的所选交易。 当视图中显示许多节线时，此过滤操作非常有用，因此它支持T2和T3。 图9显示了显着性调制的影响。 查看导航 KnotLines视图可以水平放大，以清楚说明，这对T2和T3有帮助。 时间间隔的长度将相应调整。 分析师可以垂直或水平滚动查看更多的knotlines。 兴趣选择结 我们可以通过单击或使用套索工具拖动来选择一组结。 指定结时，会以黄色的圆圈突出显示。 由同一买家制造的相关结也用灰色的圆环突出显示分析师的注意力（图1（b））。 将出现一个浮动文本框，显示所选结的详细信息，如卖方的位置，支付金额和销售类别。 分析员可以在详细视图（图1（e））中的所选结（部分）中检查交易的信息（例如，买方和卖方的位置，子类别和商品编号），其中 是为T2设计的。 对T4有帮助的统计视图（图1（f））用于提供所选结的统计信息，如交易频率和卖方支付金额的趋势。 标签 当特定交易被分析师识别为突出显示时，可以将其添加到标记数据中进行迭代视觉分析和探索。 Saliency Modulation Each transaction shown in the KnotLines view contains a saliency value. The analysts can show either all transactions selected from the 2D TOS map or only the selected transactions whose saliency values are larger than a given threshold (e.g., 0.8).This filtering operation is useful when there are many knotlines shown in the view, so it supports T2 and T3. Figure 9 demonstrates the effect of saliency modulation. View navigation The KnotLines view can be zoomed horizontally for clear illustration, a function helpful for T2 and T3. The lengths of the time intervals will be adjusted accordingly. Analysts can scroll vertically or horizontally to see more knotlines. Knots of Interest Selection We can select a set of knots by clicking, or dragging using a lasso tool. When a knot is specified, it is highlighted with a yellow ring. Related knots which are made by the same buyer are also highlighted with grey rings to draw the analysts’ attention (Figure 1 (b)). A floating text box will appear displaying the detailed information of the selected knot such as the location of the seller, the payment amount, and the sales category. Analysts can check the information of the transactions (e.g., the location of the buyer and the seller, the sub-category, and the commodity number) in the selected knot (section) in the detail view (Figure 1 (e)), which is designed for T2. A statistic view (Figure 1 (f)), which is helpful for T4, is used to present statistical information for the selected knot, such as the trend of the transaction frequency and the payment amount of the seller. Labeling When a specific transaction is identified as salient by analysts,it can be added to the labeled data for iterative visual analysis and exploration. CASE STUDY 案例研究PS: 这部分应该是论文作者讲述一次真实分析过程，里面有分析的思路与流程，还有该系统的使用特点。 来自我们的客户 - 客户（C2C）零售业务的数据部门的分析师参与了这项研究。 该公司提供了一个数据集，其中包含2600万个在线电子交易，从中他们想要检测假交易。 约有930万卖家和买家参与了数据集。 他有兴趣通过与合作伙伴买家建立假交易来识别卖家何时积累信用。 异常交易行为的一些指标可能是异常大量的商品，支付金额的大幅变化，特定卖方和买方之间的频繁交易，以及价值超出其正常范围的属性。 Construction of Decision Tree 决策树构建计算每个时间间隔内的卖方的交易频率并将其用作时间特征。 根据由分析者提供并用作文本特征的敏感字典，提取了评论中的关键词和短语（例如“信用”）。我们标注了大约300笔交易，这些交易是使用分层抽样从每个类别中选出的。 我们用标记的数据训练了决策树。 将要分析的交易的提取特征描述作为决策树的输入，并为每个输入产生显着性值。 我们使用精确率p和召回率r来评估决策树的效率：p = TP /（TP + FP）= 0.89，r = TP /（T P + FN）= 0.92，其中TP，TN，FP和FN 从训练数据的预测结果中计数（见表1）。PS: 这几个概念一般用于评估机器学习算法的性能等指标，也是广泛用于信息检索和统计学分类领域的两个度量值，用来评价结果的质量。 决策树算法在这里的作用只是用来找到显著性值。 Abnormal Frequency and Locations of Transactions 异常频率和交易地点分析师在简短的培训课程之后开始在TOS映射中进行勘探。他注意到一个具有高度显着性的长长的地区（图10）。随后，分析师指定了时间窗口，并放大到所需的区域。为了进一步研究交易行为，分析师选择了这个地区，发现许多交易在9月19日上午10点被分类为“图书”。分析师注意到连续的红色结在图1（b）的连线上连接。他告诉我们，这种模式表明卖家在选定的时间间隔内频繁交易。经查核详细资料后，分析师发现这些交易属于“充值卡”类别，并由不同的买家组成。他评论说，这可能是一个促销活动，因为他在这些交易中没有发现异常信息。分析师将显着阈值增加到0.8，显着调制滑块（Saliency Modulation）。这使得分析师能够有效地过滤出许多不太显着的交易。分析师立即找到多个未填充的结，指示具有缺失值的交易。他评论说，使用未填充的结来呈现缺失值的交易有效地吸引了他的注意。为了进一步调查这些记录是否表示促销或假交易，分析师进行了进一步分析。当他点击这条结线中的一个未填充的结时，同一个结线中的许多其他结被突出显示（图1（b）），表明这些结的大部分交易是由同一买方和卖方进行的。分析师注意到，详细信息视图中，买方的交货地址为空，通过查看此结线中的结的详细信息（图1（e））。他评论说这是可疑的，因为如果买方不填写他的地址，买家就不会买到这个商品。通过查看结的交易历史（图1（f）），分析师发现，卖方的销售额在一段时间内急剧增加。 分析师认为这些交易可能与赚取信用有关。 该结论由数据提供商的商业智能部门的分析师进行了验证，他们检查了与交易相关的其他信息，如卖方和买方的IP地址。 他们解释说，这些交易是由一组帮助卖家增加信用的买家进行的，卖家没有真正交付产品。 分析师将这一结线中的交易标示为显着，并将它们添加到训练数据集中。 Abnormal Attribute Values of Transactions 交易的异常属性值分析师在TOS映射中选择了另一个时间窗口。通过查看条形图视图中的销售类别信息（图11（a）），分析师发现，“电子配件”类别中销售的商品总数远远大于其他销售类别。分析师认为，由于商品数量庞大，这将是促销活动。他在knotline视图中调查了这个假设，但没有发现任何包含频繁连续的“电子配件”结。 分析师在TOS映射中选择了此类别，以过滤掉不相关的类别。通过仔细检查剩余的结，分析师发现一个具有短茎的极大的结（图11（b）），表明一个较大的商品数量，总支付金额却较低。分析师告诉我们，“通过这种knotline，我第一眼就注意到支付和商品数量之间的异常关系”。结的详细视图（图11（c））显示，本节包含单笔交易，支付金额仅为10美分，商品却为22万件。通过进一步调查卖方的交易历史（图11（d）），分析师消除了销售促销的概率，因为卖方在一段时间内交易次数很少，表明销售商品很少。分析师认为这是一个事件，卖方试图根据已经出售的商品数量增加他们的互联网搜索排名。分析师还将该交易标注为显着，并将其添加到训练数据集。 USER STUDY 用户研究我们进行了用户研究，以评估VAET支持低级别分析任务的能力，即第3节中讨论的T1-T4。所使用的数据集是第8节（即上一节）中探讨的交易数据集。 Design 设计10名参与者（6名男性和4名女性）年龄介于21岁至35岁之间进行了用户研究。 其中两人是分析师，其他人则是研究生。 所有参与者都具有使用在线商务的经验，并且对电脑熟悉。 学生的专业包括计算机科学，设计，数学和生物学。 以前他们都没有使用过VAET。 参与者一个接一个地参加研究。 对于每个参与者，在测试部分之前进行了短暂的训练。 在培训部分，教练首先向参与者介绍了VAET的25分钟演示。 在演示过程中，教练解释了VAET的视觉设计和功能。 在演示之后，参与者在教练的帮助下练习了VAET提供的互动5分钟。 在测试部分，参与者被要求完成9次练习，与实际分析中遇到的练习类似，无需教练帮助。 然后，他们被要求通过回答问卷并提供主观反馈来评估系统。 这9个练习是针对三种不同的具体情况而设计的，其中两项是在第8节中描述的。每个低级任务都通过一项或多项练习进行评估。 他们评估的练习和任务如下（任务显示在括号中）： E1 “使用TOS映射，从9月21日上午9点至10点选择具有最高显着性的销售类别。”目标：确定TOS映射（T1）中感兴趣的时间段和销售类别。 E2和E3来自第8.3节所述的情况。 E2 “选择商品数量最大的销售类别”。目标：解释条形图并确定感兴趣的销售类别（T1）。 E3 “查找KnotLines中商品数量最多的交易。”目标：解释一个结的视觉编码，并识别具有特定属性（T2）的有趣模式的交易。 要完成E4-E6，参加者将被要求在9月19日晚上18点至19点之间设置时间，并选择“充值卡”。 E4 “从KnotLines视图中找到具有最高交易频率的卖家（knotline）”。目标：解释knotline的视觉编码，并识别有趣的交易模式（T3）的卖家。 E5 “在KnotLines视图中，哪些卖方的交易模式不会发生？ （a）大量商品的单笔交易，但支付金额很小。 （b）低频连续交易。 （c）持续交易频率高，支付金额小。 （d）我不知道。“目标：解释这个knotlines和识别有趣的卖家交易模式（T3）。 E6 “E4中卖方交易历史的一个特征是什么？ （a）持续，频繁的交易。 （b）偶尔交易。 （c）突然，频繁的交易。“目标：以统计视角检查卖方的行为（T4）。 E7 - E9与8.2节描述的情况相同。 E7 “在KnotLines视图中查找未填充的结，并报告买方城市。”目标：使用详细信息视图（T2）检查事务的属性值。 E8 “E7中确定的卖方交易历史的主要特点是什么？ （a）持续，频繁的交易。 （b）偶尔交易。 （c）突然，频繁的交易。“目标：使用统计视图检查卖方的行为（T4）。 E9 “E7中确定的卖方的交易行为是什么？ （a）具有大量商品编号的单一交易。 （b）付款金额低的频繁交易。 （c）与异常购买城市频繁交易。“目标：解释和检查KnotLines（T4）的卖方交易模式。 E1 “Choose the sales category with the highest saliency value from 9 am to 10 am on September 21 using the TOS map.” Objective: Identify time periods and sales categories of interest in the TOS map (T1). E2 and E3 were from the case described in Section 8.3. E2 “Choose the sales category with the largest commodity number.” Objective: Interpret the bar chart and identify sales categories of interest (T1). E3 “Find the transaction with the largest number of commodities in KnotLines.” Objective: Interpret the visual encoding of a knot and identify transactions with interesting patterns in specific attributes (T2). To finish E4 - E6, participants were asked to set the time between 18 pm and 19 pm on September 19, and choose “Top-up Card”. E4 “Find the seller (knotline) with the highest transaction frequency from the KnotLines view.” Objective: Interpret the visual encoding of a knotline and identify sellers with interesting transaction patterns (T3). E5 “In the KnotLines view, which transaction pattern of the seller does not occur? (a) Single transaction with a large number of commodities but a small payment amount. (b) Continuous transactions with low frequency. (c) Continuous transactions with high frequency and a small payment amount. (d) I don’t know.” Objective: Interpret the knotlines and identify interesting seller transaction patterns (T3). E6 “Which is a feature of the seller transaction history of the knot in E4? (a) Continuous, frequent transactions. (b) Occasional transactions. (c) Sudden, frequent transactions.” Objective: Examine the seller’s behavior in the statistic view (T4). E7 - E9 were the same case described in Section 8.2. E7 “Find unfilled knots in the KnotLines view and report the buyer cities of them.” Objective: Examine the attribute values of the transactions using the detailed information view (T2). E8 “What is the main feature of the seller’s transaction history of the knot identified in E7? (a) Continuous, frequent transactions. (b) Occasional transactions. (c) Sudden ,frequent transactions.” Objective: Examine the seller’s behavior using the statistic view (T4). E9 “What is the transaction behavior of the seller identified in E7? (a) A single transaction with a large commodity number. (b) Frequent transactions with low payment amounts. (c) Frequent transactions with abnormal buyer cities.” Objective: Interpret and examine the seller’s transaction patterns from KnotLines (T4). 练习结束后，参加者完成了由6个问题组成的调查问卷（Q1〜Q6）。 要求用1到5评估等级（1=非常简单或高效，5=非常困难或低效），学习该VAET系统的难度和VAET的效率。 这些问题也收集到主观反馈。 六个问题如下： 容易或难以学习TOS映射？ 使用TOS映射探索显着数据是否有效？ 单个结的视觉编码是否容易或难以解释？ KnotLines的视觉编码和布局是否容易或难以解释？ 使用KnotLines分析用户交易模式是否有效？ 使用VAET整体分析多用户行为是否容易或困难？ Is it easy or hard to learn the TOS map? Is it efficient or not to explore salient data with the TOS map? Is it easy or hard to interpret the visual encoding of a single knot? Is it easy or hard to interpret the visual encoding and layout of KnotLines? Is it efficient or not to analyze the user transaction patterns with KnotLines? Is it easy or hard to analyze multi-user behavior with VAET as a whole? Results 结果收集9次练习的准确性和时间进行评估 总体来说，参加者完成了练习，在90次全部练习中产生5次错误（精准度为94.4％）。分析师正确回答了所有问题。对于学生参与者，其中两人在E5上出错，E8上有两个错，E9上有一个错。 我们采访了E5的错误参与者。他们都说他们“只注意到knotlines的主要模式，忽略了发生较少的模式”。但是，他们没有解释用户行为的问题。回答E8的参与者错误地提到他们忘记检查统计视图中显示的卖家历史信息（图1（f）），而是根据KnotLines视图回答问题。在E9错误的参与者认为，零买家城市（null buyer cities）对于不需要交货地址的电子书等虚拟商品的交易是正常的。事实上，“图书”类别中的所有商品都是实物。虽然一些参与者有T3和T4的问题，但总体准确性表明VAET可以很好地支持任务。 Overall, the participants completed the exercises, yielding 5 mistakes out of the 90 total exercises (94.4% accuracy). The analysts answered all questions correctly. As for the student participants, two of them erred on E5, two erred on E8 and one erred on E9. We interviewed the participants who erred on E5. They both said that they “only noticed the main patterns of the knotlines and ignored the patterns with fewer occurrence”. However, they had no problem interpreting the user behavior from them. The participants who answered E8 incorrectly mentioned that they forgot to check the seller history information shown in the statistic view (Figure 1(f)) and answered the question based on the KnotLines view instead. The participant who erred on E9 thought that null buyer cities are normal for transactions of virtual commodities such as E-books, which do not need delivery addresses. In fact, all commodities in the “Books” category are real items. Although some participants had problems with T3 and T4, the overall accuracy indicates that VAET supports the tasks well. 图12显示了每次锻炼的完成时间的平均值和标准偏差。 E3，E4和E7的时间比其他问题的时间长。 这三个练习要求参与者搜索具有特定特征的knotlines，这可能需要更多的时间仔细检查视图。 参与者能够快速完成其他6个问题。 完成时间为5.14s至44.22s。 分析师花费的时间远远少于学生参与者。 总体而言，VAET允许大多数参与者在90秒内完成复杂的练习和任务（如E4和T3）。 在考虑交易的各种属性以及它们之间的关系时，这是相当快的。 分析师的反馈与讨论 在用户研究结束时，我们采访了分析师和其他参与者。 两位分析师都对分类结果感到满意。 他们评论说，在分析不同类型的交易时，特征提取和决策树的集成是高效和灵活的。 我们要求他们将决策树与之前使用的逻辑回归进行比较。 他们认为我们的模型有几个优点： 决策树对于分析师来说很简单易懂。 在处理属性的缺失值时，决策树比逻辑回归更强大。 使用决策树的分类比逻辑回归更快。 他认为我们可以通过使用更多的信息来改进我们的方法，比如今后买家和卖家的IP地址。 他们评论说，TOS映射是直观的，勘探方便。一位分析师表示：“KnotLines的多用户行为的可视化是创造性和生动的。”虽然一位分析师认为该系统很难学习，但很少参与者难以解释可视化，大多数人能够在简短的培训会议后找到有趣的交易。两位分析师都提到，VAET能够帮助他们探索未被发现的交易模式。这是至关重要的，因为“欺骗人总是不断改变伎俩”。他们认为，VAET有能力找到新兴的交易模式，并帮助他们改进数据模型。分析师们渴望将VAET用于实际的多用户应用程序，在这些应用程序中，他们发现许多高维度事务之间的上下文和时间相关性。有趣的是，一些参与者最初认为使用我们的系统需要基本的音乐知识。其他与会者提到，他们对音乐笔记的了解影响了对设计的理解。例如，茎长度固定在音符中，但在我们的设计中是可变的。此外，有些人认为与其他knotbunches（类似于四分之一音符）没有连接的单个knotbunch持续比连接的knotbunch短（类似于第八个音符）。事实是，交易没有持续时间，因为它们都是立即在线发生的。与会者告诉我们，在他们学会如何解读设计之后，这种差异并没有妨碍他们的分析。 CONCLUSION 总结本文提出了一种用于识别基本交易数据和研究大片碎片记录的时间或集体行为的新型视觉探索方案。 在对KnotLines进行选择交易的详细探索和推理之前，执行决策树算法和TOS映射的过滤过程，以从大量记录中选择潜在有趣的交易。 案例研究和用户研究证实，VAET可以有效地支持大部分任务。 根据结果，一些任务如T3需要更好地解决，因为交易模式可以是动态的和多层次的。 为了使VAET更容易学习和使用，我们希望使TOS映射和KnotLines的设计更加直观。 对于未来的工作，我们也希望对更多的数据集进行扩展。 This paper presents a novel visual exploration scheme for identifying elementary transaction data and studying the temporal or collective behavior from large pieces of fragmented records. Prior to detailed exploration and reasoning of the chosen transactions with KnotLines, a decision tree algorithm and a filtering process by TOS map are performed to choose potentially interesting transactions from a huge amount of records. The case study and the user study verify that VAET can effectively support most of the tasks. According to the result, some tasks such as T3 need to be better addressed as the patterns of transactions can be dynamic and multi-level. To make VAET easier to learn and use, we would like to make the design of TOS map and KnotLines more intuitive. For future work, we also would like to extend our approach for more datasets.","categories":[{"name":"study","slug":"study","permalink":"https://yjhmelody.github.io/categories/study/"}],"tags":[{"name":"数据可视化","slug":"数据可视化","permalink":"https://yjhmelody.github.io/tags/数据可视化/"},{"name":"论文阅读","slug":"论文阅读","permalink":"https://yjhmelody.github.io/tags/论文阅读/"}]},{"title":"计算机系统结构","slug":"计算机系统结构","date":"2017-09-28T11:48:52.000Z","updated":"2017-11-20T08:18:15.382Z","comments":true,"path":"2017/09/28/计算机系统结构/","link":"","permalink":"https://yjhmelody.github.io/2017/09/28/计算机系统结构/","excerpt":"系统结构的部分课后作业，因为需要写成电子版，故稍微整理下也放入到博客中，方便你我他。答案是自己组织的，不保证一定正确。书是张晨曦的《计算机系统结构（第2版）》","text":"系统结构的部分课后作业，因为需要写成电子版，故稍微整理下也放入到博客中，方便你我他。答案是自己组织的，不保证一定正确。书是张晨曦的《计算机系统结构（第2版）》 第一章1翻译技术是先把 L+1 级程序全部变换成 L 级程序后，再去执行新产生的 L 级程序，在执行过程中 L+1 级程序不再被访问。 解释技术是每当一条 L+1 级指令被译码后，就直接去执行一串等效的 L 级指令，然后再去取下一条 L+1 级指令，以此重复进行。 计算机系统结构是程序员所看到的计算机属性，即概念性结构与功能特性。 Amdahl定律：加快某部件执行速度所能获得的系统性能加速比，受限于该部件的执行时间占系统中总执行时间的百分比。 程序的局部性原理是指程序执行时所访问的存储器地址不是随机的，而是相对簇聚。局部性原理又表现为：时间局部性和空间局部性。时间局部性是指如果程序中的某条指令一旦执行，则不久之后该指令可能再次被执行；如果某数据被访问，则不久之后该数据可能再次被访问。空间局部性是指一旦程序访问了某个存储单元，则不久之后。其附近的存储单元也将被访问。 CPI(Cycles Per Instruction，每天指令的周期数) CPI = 执行程序所需要的时钟时间 / 执行指令个数。 模拟是指用软件的方法在一台现有的计算机（宿主机host）上实现另一台计算机（虚拟机VM）的指令集。 仿真是指用一台计算机（宿主机host）上的微程序去解释实现另一台计算机（目标机）的指令集。 3Flynn分类法是按照指令流和数据流的多倍性进行分类的。 Flynn分类法把计算机系统结构分为以下4类： 单指令流单数据流（SISD） 单指令流多数据流（SIMD） 多指令流单数据流（MISD） 多指令流多数据流（MIMD） 4计算机系统设计经常使用的4个定量原理 以经常性事件为重点，按照对经常性情况采用优化方法的原则，能得到更多整体上的改进。 Amdahl定律，加快某部件执行速度所能获得的系统性能加速比，受限于该部件的执行时间占系统中总执行时间的百分比。 CPU性能公式，CPU时间 = 时钟周期时间 x CPI x IC，只要改进任何一个参数都能提高CPU性能 程序的局部性原理，程序执行时所访问的存储器地址不是随机的，而是相对簇聚。 6执行时间 = 0.000575(s) CPI =(45000 + 750000 2 + 8000 4+ 1500 * 2) / （45000 + 75000 + 8000 + 1500）~= 1.776(s) MIPS = 400Mhz / (CPI * 10^6) ~= 225.217 执行时间 = 总时钟周期时间数 * 平均周期时间 = 0.000575(s) 8 加速比= 1/((0.9-x)+0.3/30+0.3/20+x/10)=10 x=0.361 加速比=1/(0.2+0.3/30+0.2/20+0.2/10)=1/0.245 比例=0.2 * 加速比 ~= 0.816 第二章1CISC Complex Instruction Set Computer 复杂指令集计算机 复杂指令集计算机的设计策略是使用大量的指令，包括复杂指令。与其他设计相比，在CISC中进行程序设计要比在其他设计中容易，因为每一项简单或复杂的任务都有一条对应的指令。程序设计者不需要写一大堆指令去完成一项复杂的任务。 但指令集的复杂性使得CPU和控制单元的电路非常复杂。 RISC Reduced Instruction Set Computer 精简指令集计算机 精简指令集计算机是一种执行较少类型计算机指令的微处理器。它能够以更快的速度执行操作（每秒执行更多百万条指令，即MIPS）。因为计算机执行每个指令类型都需要额外的晶体管和电路元件，计算机指令集越大就会使微处理器更复杂，执行操作也会更慢。 数据表示(data representation) 是指计算机硬件能够直接识别和指令集可以直接调用的数据类型。 2区分指令集结构的主要因素：寻址方式，数据表示，指令格式。根据该因素可将指令集结构分为以下三类： 寄存器-寄存器类型(R-R) 寄存器-存储器类型(R-M） 存储器-存储器类型(M-M) 4指令集应满足的基本要求：完备性，规整性，高效性，兼容性 5指令集结构设计所涉及的内容有指令功能设计，数据表示，寻址方式，指令格式 9表示寻址方式的主要方法有2种： 把寻址方式编码于操作码中，操作码在描述指令功能的同时也描述了相应的寻址方式，这缩短了指令长度，译码快，但增加了指令的条数和多样性，也增加了CPU对指令译码的难度。 把寻址方式跟操作码分离，为每个操作数设置一个地址描述符，由地址描述符表示相应操作数的寻址方式，这扩大了寻址的范围，但增大了指令长度。这种方式译码较慢，但操作码和寻址互相独立，易于扩展。 10体系结构中常用的指令格式有变长编码格式，定长编码格式，混合编码格式。分别是考虑目标代码的大小，性能，和兼顾两者。 第三章1流水线：把一个重复的过程分解为若干个子过程，每个子过程由专门的功能部件实现。把多个处理过程在时间上错开，依次通过各功能段，这样每个子过程及其功能部件可以与其他子过程并行进行。 流水线的特点： 把大的处理功能分解为多个独立的功能部件，依靠他们的并行工作来缩短程序的执行时间 流水线中的各段的时间尽可能相等，否则会引起流水线的阻塞、断流，因为时间长的段会成为流水线的瓶颈 流水线每个功能部件后面都要有一个锁存器，作用是在相邻两段传送数据，保证后面要用到的数据，并把各段处理的工作隔离 适用于大量重复的时序过程，只有在输入不断地提供任务，才能充分发挥效率 流水线需要通过时间跟排空时间。 流水线的分类： 单功能流水线，多功能流水线 静态流水线，动态流水线 部件级流水线，处理机级流水线，处理机间流水线 线性流水线，非线性流水线 顺序流水线，乱序流水线 吞吐率ThroughPut是指单位时间内流水线所完成的任务数量或输出结果的数量。 流水线加速比Speedup是指完成同一批任务，不使用流水线与使用流水线所用时间之比。 流水线的效率Efficiency是指流水线中的设备实际使用时间与整个运行时间比值，即流水线设备的利用率。 名相关是指某两条指令使用相同的名，但是它们之间没有数据流动。 定向技术：在某条指令产生计算结果之前，其他指令并不真正理解需要该计算结果，如果能够将该计算结果从其产生的地方直接送到其他指令需要它的地方，那么就可以避免停顿。 链接技术是指具有先写后读相关的两条指令，在不出现功能部件冲突 和源向量冲突的情况下，可以把功能部件链接起来进行流水处理，以达到加快执行的目的。 分段开采技术：当向量长度大于向量寄存器长度时，必须把长向量分成长度固定的段，然后循环分段处理 ，每次循环只处理一个向量段。 3解决流水线瓶颈问题通常有两种方法：细分瓶颈段，重复设置瓶颈段 4减少分支延迟的静态方法：预测分支失败；预测分支成功；延迟分支。 73种向量处理方式： 横向处理方式：向量长度为N时，则相当于N次循环，使用流水线时，在每次循环会出现数据相关和功能转换的问题，所以不适合流水线处理。 纵向处理方式：将整个向量按相同的运算符处理完后，再去进行别的运算。无论N多大，相同运算都用一条向量指令完成。因此需要采用存储器-存储器结构流水线。 纵横处理方式：结合以上两种，它把向量分成若干组，组内用纵向处理方式，依次处理各组。可以设置能快速访问的向量寄存器，用于存放源向量、目的向量、中间结果，构成了寄存器-寄存器结构流水线。 91)TP = 10 / (50 + 50 + (10-1) * 200) = 1/220 E = (10 (50 + 50 + 100 + 200)) / ((50 + 50 + 200 + (10 – 1) 200) * 4) = 5/11 2)该流水线的瓶颈在第四段。分别采用细分瓶颈段和重复设置瓶颈段。两种的吞吐率和效率相同 TP = 10 / (50 + 50 + 100 + 100 + 100 + (10 – 1) * 100) = 1/130 E = 10 400 / (1300 5) = 8/13 11TP = 8 / ((1+2+1+1) + 23 + (1+1+1) + 1 3) = 8 / 17 S = 4 (1+2+1+1) + 4 (1+1+1) / 17 = 32/17 E = S/TP = 1/4","categories":[{"name":"study","slug":"study","permalink":"https://yjhmelody.github.io/categories/study/"}],"tags":[{"name":"系统结构","slug":"系统结构","permalink":"https://yjhmelody.github.io/tags/系统结构/"}]},{"title":"人工智能发展报告","slug":"人工智能发展报告","date":"2017-09-24T11:29:28.000Z","updated":"2017-09-24T11:33:42.785Z","comments":true,"path":"2017/09/24/人工智能发展报告/","link":"","permalink":"https://yjhmelody.github.io/2017/09/24/人工智能发展报告/","excerpt":"人工智能的发展数据挖掘课程的作业报告，放在这里分享一下，都是一些蠢话。","text":"人工智能的发展数据挖掘课程的作业报告，放在这里分享一下，都是一些蠢话。 符号主义（逻辑主义）推理期二十世纪五十年代到七十年代初，人工智能的研究处于“推理期”，那时候人们普遍认为机器获得智能的方法是赋予机器逻辑推理的能力。该阶段比较有代表的工作比如在1955年12月，赫伯特·西蒙（Herbert Simon）和艾伦·纽厄尔（Allen Newell）开发出逻辑理论家，这是世界上第一个人工智能程序，有能力证明罗素和怀特海《数学原理》第二章52个定理中的38个定理，甚至在后来证明了全部52个定理。这两位也因此在1975年获得了图灵奖。 知识期随着后来的研究进展，人们意识到仅仅具有逻辑推理能力是远远无法实现人工智能的。从二十世纪七十年代中期开始，人工智能的研究进入知识期。专家系统被大量开发出来，E.A. Feigenbaum 作为“知识工程”之父在1994年获得图灵奖。不过后来人们又意识到专家系统的“知识工程瓶颈”，把人类总结的知识教授给计算机是相当困难的。 机器学习八十年代左右，“从样例中学习”的一大主流还是符号主义的思想，其代表如决策树（Decision tree）和基于逻辑的学习。决策树以信息论为基础，而基于逻辑的学习是归纳逻辑程序设计，可看作是机器学习与逻辑程序设计的交叉。 神经网络（联结主义）九十年代中期之前，“从样例中学习”的另一个主流是基于神经网络的联结主义。1986年，D.E. Rumelhart 等人重新发明了BP算法，产生了深远影响，如今的深度学习最基本的概念便是BP算法。不过联结主义产生的是“黑箱”模型，从知识获取角度看有明显的弱点。然而，由于BP算法，深度学习算法在实际中非常有用，在2006年开始第三次神经网络高潮以深度学习之名重新爆发而来，在2012年之后成为人工智能的主流算法。当然，在九十年代统计学习兴起时，而又因为当时的局限性，曾经落入低潮。 统计学习九十年代中期，“统计学习”迅速兴起并成为主流，如今依然是主流的机器学习算法，典型代表是SVM。早在九十年代之前，统计学习的许多基础理论已经出现，但因为联结主义的神经网络在九十年代具有局限性而没落后，统计学习被人瞄准目光而迅速流行起来。 谈谈自己的理解前面的一些概括是我认为比较重要的历史的整理。如今看来，人工智能的历史虽然不漫长，却可以说的上丰富与多变。现如今，从学术、商业、工业角度来审视的人工智能，占据主流的是传统机器学习跟深度学习，然后才是强化学习跟规则学习（个人看法）。当然，如今的机器学习算法或多或少都用上了概率统计的知识。 深度学习如今越来越火热，在我自己开始留意机器学习的内容开始，深度学习相关的文章跟新闻就狂轰滥炸地映入我的眼里，以至于我没法不正视它。后来我也简单的接触深度学习，才发现它确实不可思议。在训练深度学习时，它就是在特征空间里不断逼近然后拟合到数据特征的“万能函数”，怪不得说它是万能近似图灵机。把它应用到许多领域感觉也就不奇怪了，当然它不一定比传统的机器学习和其他人工智能算法要更有效。 不过可能是因为深度学习的万用性导致它的黑箱性，大部分人在使用它来解决问题的时候，没有获得很好的解释，无论是从深度学习结构模型的理论角度还是问题本身特性的角度。只是在设计网络架构时，粗略地分析问题的特性，然后改良别人成功的架构跟“合理”的解释来解决问题。在问题较满意的解决后，没有合理的可解释性或者干脆从他人理解的来解释。 当然，从实用性跟工作角度来说，我觉得这样没错。深度学习跟你的剪刀和锤子一样，只是解决问题跟生产的工具，并不需要在意内部机理。这样的比喻来解释可能非常不妥当，不过我想表达的是，如今主流的人工智能方法：深度学习，虽然在构造时有比较好的理论基础，但在优化模型，优化架构，并且在解释优化可能性与优化思路上，缺乏理论。（对深度学习接触才几周，造成这样可能有偏差的认知，如有错误，欢迎指出） 相比于深度学习，传统偏向于统计的机器学习，可解释性就比较强了，而且在很多时候，从各种角度上与深度学习比较，丝毫不逊色。而且现实的问题非常复杂，蕴含大量不确定跟随机的事件，而概率论与数理统计就是对现实世界建立这样的一些模型跟假设，这方面的理论也算比较完善了，所以传统机器学习更能在数学理论上解释一些模型的行为。 而最近神经网络之父 Geoffrey Hinton 也表明对BP非常怀疑，应该抛弃它。BP如今是深度学习最常用的算法了，如果丢弃它，深度学习大概会大变样吧。如此看来，深度学习没有那么“完美”，仍需要大量基础研究甚至真的可能在未来被更好的算法替代。 展望未来的人工智能如果想到三十年以后，大街小巷到处有序而不拥挤地行驶着无人车，载着乘客去景点；天空上时不时飞过一架架无人机，拍摄城市风景与监控城市安全；新闻报道是程序根据许多视频、图片。文本而撰写的；家里布满了传感器跟智能家具，许多繁琐事情可以通过简单对话跟指令来处理。想到如果真能如此，不由对如今的人工智能算法寄予厚望并抱有乐观的想法。 但在我看来，上面的美好描述可能还是过于乐观了。 之前曾在哪里看到一个观点，20年后人工智能将会代替80%的工作。我觉得这个也太乐观了。当今现实生活中，从事脑力活动的人已经多余从事体力活动的人了，未来这个趋势应该更加明显。而以目前人工智能的能力，很多脑力活动不能代替（或者说，实现这些能代替脑力活动的程序需要更多脑力的脑力活动）。若人工智能的算法更加成熟稳定，许多人应该会从事人工智能相关的工作，建设基础设施（其实现在就有很多人往机器学习这个方向转业），普及人工智能，而只有少部分人推动人工智能的发展。 其实我一直不敢想像未来，这对于我太难了。十年前我对未来的展望似乎跟如今的现实大相径庭了。自己觉得可能出现的东西往往没有到来，反而出现一些超出以前认知的意外事物。说不定，以后深度学习也不再是实现人工智能的主力了，出现一些特定算法可以实现以前难以实现的智能，但却无法较好完成如今研究的方向。最坏的情况就是深度学习仍然是主流，而其理论仍然不明朗，调参也没有完备的方法论，人工智能发展停滞了几十年，直到我们这一代人死去，这后面的事情我也不想展望了。","categories":[{"name":"study","slug":"study","permalink":"https://yjhmelody.github.io/categories/study/"}],"tags":[{"name":"ML","slug":"ML","permalink":"https://yjhmelody.github.io/tags/ML/"}]},{"title":"暑假机器学习总结","slug":"暑假机器学习总结","date":"2017-09-13T12:49:32.000Z","updated":"2017-09-24T11:34:31.239Z","comments":true,"path":"2017/09/13/暑假机器学习总结/","link":"","permalink":"https://yjhmelody.github.io/2017/09/13/暑假机器学习总结/","excerpt":"机器学习学习总结经过两个月的学习，从对机器学习一点点懵懂认知，到现在对机器学习的基础知识跟体系有一定的认知。如今学习暂告一段落，总结如今学习过的重点知识，可以起到很好的复习作业，也是对两个月以来的交代。以下，我将按照每周学习进度来总结回顾机器学习的知识。","text":"机器学习学习总结经过两个月的学习，从对机器学习一点点懵懂认知，到现在对机器学习的基础知识跟体系有一定的认知。如今学习暂告一段落，总结如今学习过的重点知识，可以起到很好的复习作业，也是对两个月以来的交代。以下，我将按照每周学习进度来总结回顾机器学习的知识。 第一周我们先学习了python编程基础，之后的学习是基于python各种库来实验的。我之前已经比较熟悉python，所以很快就完成这段学习；之后接触numpy，这是C语言为python编写的底层矩阵库，我之前也接触过，但比较浅，不过numpy封装的很好，用起来门槛也很低，很快就上手了；numpy之后就是pandas,它是数据分析常用的库，基于numpy，非常全面，我之前也用过，但基本需要重新学习，pandas比较难用，尤其是IO部分，有细粒度的操作，文档看起来也比较麻烦，没有示例，所以学习的过程中，是遇到问题再去查找方法，后面的学习pandas其实用到的也比较少。 以上内容大概花了两天，算是对机器学习的预备知识的准备。当然期间也学习了简单的使用anaconda，jupyter等工具，不再一一总结了。 之后开始学习基本的图像知识： 颜色直方图,它所描述的是不同色彩在整幅图像中所占的比例，而并不关心每种色彩所处的空间位置。之后有几次作业中要编写颜色直方图的处理，一开始还是挺棘手的。不过跟后来的图像特征提取就是小巫见大巫了 HOG特征，主要思想：在一副图像中，局部目标的表象和形状（appearance and shape）能够被梯度或边缘的方向密度分布很好地描述。（本质：梯度的统计信息，而梯度主要存在于边缘的地方）。这也是之后的需要实现的任务，学习过程中，只有这个博客资料可以参考，其他资料要么可能是全英文晦涩难懂，要么还不如这个。这个博客写的太精炼了，初学的时候，实现起来非常困难，以至于有些同学对根据这个博客写出算法的可能性产生怀疑。我这周的作业也卡在这里了，学习机器学习的时候反而是因为图像处理知识不过关。后来经过讲解对这些特征有更新的认识了，不过可能实现起来对于现在我的依然有些困难。 LBP特征、Haar特征也是这周的基础知识，我稍微学习了下LBP，发现比HOG要好懂，Haar并没有怎么看，最后这2个内容没有出现在作业里，我对这些也只有粗浅的认识。LBP（Local Binary Pattern，局部二值模式）是一种用来描述图像局部纹理特征的算子；它具有旋转不变性和灰度不变性等显著的优点。而Haar特征值反映了图像的灰度变化情况。 图像的部分内容学习后，开始了最基础的机器学习的内容： Regression，先从最简单的线性回归开始，线性回归可能是踏入机器学习世界的第一步吧，它教你如何做最简单的预测和机器学习比较本质的思维。前几周学习的知识大多是看李宏毅的视频，前几周感觉还是不错的。从线性回归开始，学到了基本的梯度下降思想和度量性能的代价函数。 Error，第二节就是深入理解各种模型评估的知识，讲授了误差,偏差,方差的区别与联系。 Gradient Descent，最后是深入学习梯度下降，学习推导基本的梯度下降，然后提出随机梯度下降(SGD)，从大量数据中随机选择一定量数据来训练，提高学习效率。除此之外，讲解了梯度下降的问题：学习率的选择。然后基于此讲解了一个算法Adagrad来控制学习率。 最后还有一些比较杂的知识，了解了K折交叉验证的思想与作用，把数据集分割为训练集，验证集，测试集的思想与作用。各种距离度量，可以衡量样本近似度。最后完成两个作业 图像知识可能在第一周学习，有点不知其有何用的感觉，就算想在思想上重视它，但没有实际用起来，还是难以深刻理解它的重要性吧。谈点个人感受，我其实挺不擅长也不太喜欢处理图像的，大一自己有简单接触过图像处理（跟现在的学的不太一样，而是常规的图像处理，不跟特征，知识等内容关联），虽然不比这些难，但也很吃力。 第二周第二周主要学习的是分类的基础算法，分别学习K近邻,决策树,逻辑斯蒂回归： KNN的思想就是把某个样本跟其他所有样本进行距离度量并总和，该样本离哪个类别’最近’，就标记为该类别。KNN是懒惰学习的典型算法，即到需要分类的时候才使用上训练集。在学习KNN的时候还了解到矢量化编程的重要性，减少不必要的python for 循环可以利于底层numpy优化为并行代码，在我这次实验里速度提升了近百倍。 Logistic Regression 该算法跟线性回归（跟感知器也类似）基本类似，不过它加入了sigmoid函数来进行分类而不是回归。后来学习深度学习才知道这里有神经网络的最基本的思想，或者说可能是最简单的神经网络了。 Decision Tree决策树可能是到这周为止最难的算法了，写起来会特别长。思想其实很简单，就是树的思想 + 人类决策过程。常用的决策树有ID3,C4.5,C5.0,CART等，不过我只编写了最简单的ID3，其他决策树进行了简单的了解。学习过程中认识到如果生成完整的决策树，那会变得非常耗时，后面学习到剪枝知识（我在作业里面编写预剪枝了，不过效果很差）。学习决策树里面知道了一些信息论的知识，如信息熵,信息增益,纯度等知识，决策的依据便是依据这些数值来找到最佳决策特征。 最后根据学到的知识完成一个井字棋胜负预测，不过我的模型很一般。这周的知识量不是很大，更多侧重机器学习基础编程，但是感觉学到东西很多的一周。 第三周这周学习的机器学习非常强大和实用，是现在也很常用的模型： SVM非常理论，学到这里，我感觉我的高数白学了。其实现在我也不是很了解SVM，只对基本概念有了解。基本思想应该是把低维空间的非线性问题映射到高维空间线性问题来解决。然后里面概念非常多：支持向量的概念，距离度量，核函数，核方法，对偶问题，KKT等。 集成学习非常实用且广用。许多一般模型集成后都可以大幅度提升性能。这周接触了许多集成学习算法： bagging，非常简单的集成学习算法，我后来实现了bagging决策树，性能提升了许多。基本思想是自助采样一些样本后，分别训练n个模型，然后进行投票决策。这样可以大大减少过拟合而提升性能。 Random Forest，bagging算法的变体，基于决策树实现的。它与bagging的区别在于特化了决策树，在节点决策时，加入属性扰动（即只从一部分特征里选择最优特征），而bagging只有样本扰动（随机采集样本）。它的性能一般来说比bagging要好，我猜大概是加入了新的扰动后，更能避免决策树容易过拟合的缺点吧。 其他如 boosting的adboost和xgboost，进行了简单了解，xgboost在kaggle里面很热门，因为性能特别好。不过这几个算法难度更大，我了解的也比较少。adboost的基本思想是让之前训练错误的部分对应的权重变大，让模型认识到这点。 最后是一点点图像特征的知识bag of words model，对此进行一些了解 第四周这周学习无监督学习，主要是聚类跟降维，不过主要是侧重分析并运用这些技术： 聚类可以按结构特性分为原型聚类，层次聚类，密度聚类。K-means是最基础的聚类算法，主要思想是通过多次迭代来把刻画原型，来使误差最小。K-means之后是了解了高斯混合聚类，也是原型聚类。 降维这部分主要接触了PCA，不过这部分理论特别麻烦，我只搞懂了基本思想，并学会基本运用。总之，它变换了基底，并把比重最小的维度去掉来降维，这样能最大程度的保存原始样本的信息。 这周机器学习的知识就进入进阶难度了，说实话，我掌握的不太会好，不过算是对机器学习的体系更加了解了。除了以上还有一些其他知识，不过比较零碎，我就不一一总结了。 第五六七八周这里把深度学习一起总结了吧，这几周感觉学习的不太好。 第六周开始，陆陆续续有许多高大上的报告可以听讲了，了解了许多前沿的应用，思想，算法。从第五周开始也步入了深度学习的大门。 这几周主要学习全连接网络跟卷积神经网络跟循环神经网络，理解了很久反向传播，卷积运算。其实参考过很多资料，这里就不一一列出了。目前对反向传播也有稍微清晰的了解了，对于卷积，了解了其思想，但对其运算还是只有抽象的认识，反向传播可以说是目前神经网络的基础。全连接网络在理论上是近似的图灵机，不过实用性很差，一般是配合其他网络而使用。而卷积神经网络是针对图像而发明的，在图像处理跟机器视觉应用广泛。而循环神经网络更适用于序列数据，如文本，这在自然语言处理很常用，而由于一般的RNN有局限性，所以有一个LSTM的变体，这个模型我不是很清楚，但它的表现更好（最近好像又出现一个比较厉害的新变体SRU）。 由于神经网络的代码非常难写，写出来也基本不可以重用，我们学习了keras这个基于tensorflow的深度学习框架来实现一些经典的模型。后来应用keras解决了一些基本的图像分类问题。 第七八周陆陆续续的讲座也应该提一提，感觉大大开阔了我的视野。原来我以为深度学习基本都是在对图像上进行工作，好像这样也没什么意思。不过后来发现图像几乎是实现人工智能最基本的办法。而老师们的方向五花八门，问题的复杂度也远超过我的脑容量，深度学习反而变成了实现人工智能的基本工具而已，更多在于对于问题的深入研究和对特征的深入探究。 这几周的学习积极性变得比较差劲了，不过收获还是很多。至少认识到机器学习跟深度学习的基本思想，我觉得未来的程序员都或多或少需要了解这些知识，因为他们可能会用到相应的算法来部署一些人工智能应用到各种设备、各种网站、各种系统中去，而懂得这些知识的人显然能在工作中更胜一筹。 总结以上大概就是我的总结了，很粗略，也没有讲到具体的数学知识，更多是讲到自己的小小收获跟感受。我不清楚以后是否会从事机器学习相关的工作，但我以后肯定会抱着好奇心继续完善我对这些知识认识，了解里面的新思想，跟上人工智能的潮流，做一个终身学习的人吧。","categories":[{"name":"study","slug":"study","permalink":"https://yjhmelody.github.io/categories/study/"}],"tags":[{"name":"ML","slug":"ML","permalink":"https://yjhmelody.github.io/tags/ML/"}]},{"title":"从《线性代数应该这样学》到《Redis的设计与实现》","slug":"从《线性代数应该这样学》到《Redis的设计与实现》","date":"2017-09-08T13:50:19.000Z","updated":"2017-09-24T11:34:14.204Z","comments":true,"path":"2017/09/08/从《线性代数应该这样学》到《Redis的设计与实现》/","link":"","permalink":"https://yjhmelody.github.io/2017/09/08/从《线性代数应该这样学》到《Redis的设计与实现》/","excerpt":"从《线性代数应该这样学》到《Redis的设计与实现》《线性代数应该这样学》","text":"从《线性代数应该这样学》到《Redis的设计与实现》《线性代数应该这样学》 起因这本是好早之前买的，当时想着是时候复习一下线性代数了，因为以后要用到（那时候想到可能需要接触一下机器学习了）。买来之后发现太数学了，怕是数学系的许多人也吃不消看这种书。 收获与总结这本书可以作为第一本或者第二本线性代数的学习材料。但这本书远比本科工科的要求要高出许多，以致于作者在前言里说，这本每页应当至少投入一小时来学习。匆匆看完前两章我便放下了，书里充斥着大量证明，从最基本的向量空间的定义引出基本的性质，然后用证明的方式来引出各种向量空间的特点跟性质。而这是第一章的内容，读起来还是挺熟悉的感觉的，只是换了一种表述各种概念的方式和术语，但深入思考便与之前学习的向量没有多少差别。这里与我大一学的过程就差别大了，大一是直接从矩阵着手，最后讲到向量，但那时候觉得向量是很突兀的东西，甚至跟高中学习的向量完全不一样，没有从几何的角度稍加解释向量的一些特点。而这本书从标量开始，比较了标量跟向量的异同，开阔了一下从标量到向量的思路，本质上它们比自己想像的还要相似。 第二章接着从有限维度引申到无限维度，在讨论了无限维度的一些异同后，继续深入了向量空间的各种特性，譬如张成空间，直和这些陌生的概念。而这些与普通向量之间的联系是很紧密的，但读到后面愈发抽象，我其实也忘记了七七八八了，惭愧。 后面几章暂时就没有看了，基本应该是从向量空间引申到线性映射，然后引出矩阵及其性质。在我看来矩阵是向量空间的更一般化，所以以后继续讨论的大量性质，也可能用于向量。最后从矩阵的一般运算到各种算子，最后才提出行列式相关内容，大概就是这样（痛苦）的过程吧。 《TCP/IP网络编程》感获这本书是一本网络编程的入门书，整体难度跟CSAPP的第三部分差不多，各有侧重吧，内容有比较多重叠，但内容稍微多于CSAPP。用的是C语言的socket接口来讲解基本原理。用C语言讲有个好处，就是能直接从操作系统级别来思考网络编程，而且能使用全部的socket模式、所有并发模型、IO模型。缺点是这些socket api相当底层，以至于一个api只做一个最基本的逻辑操作。学习起来特别费劲，而且高度依赖一部分操作系统的知识。因为之前接触过golang和nodejs，分别也简单用过相应的socket接口。这两个语言的并发模型比较受限，nodejs目前只能异步跟简单多进程（将来应该可以用上简单多线程），golang倒是用goroutine（线程调度），通道（或者叫信道），多进程。而它们的socket接口便是跟这些模型有一定的耦合，而且高度精简，有更好的语义，从底层上面减去一些麻烦。当然，nodejs因为更单一，比golang还精简。 言归正传，这本书其实重点不在于讲解socket api，而是从使用api来达到理解socket编程思维，socket的各种核心功能，tcp/udp基本原理，并发模型的基本原理，可能还有较多的跨平台编程思路（因为C语言的socket各个系统有差异）。 一点思考因为听说是隔壁网络工程的网络编程课的教材（说实话，这本书可能稍微浅了点，除非隔壁把这本书全部内容都讲授，我觉得才比较合理），我也无意中看到并且有打算看看。不过这本书实际比较狭窄，基本专注于tcp和并发这两个内容了，虽然充分且不错地解释了部分tcp知识，部分操作系统知识，对应用层太忽略了（提了下dns跟http），我个人认为把传输层跟应用层稍加紧密地联系起来，更能激发学习兴趣，也使这本书更加实用。 其实大概就匆匆看了半本，而且忽略了windows下的实现，自己也就琢磨了下源码，基本没有自己去写过（坏习惯啊），算是预热计算机网络跟操作系统的知识了。 《Redis的设计与实现》这是图书馆借的到好书，其实早想看了，之前拿起来抱怨看不懂，这次再拿起来，已经不是那么可怕了。 收获不过现在我看的比较少，数据结构篇还没有看完，也算是温习一些数据结构的知识了。这本书把redis相关的数据结构的内存模型很漂亮的表述清楚了，特别是hash和跳表，结合了许多图片来理解。也讲清楚了实现该数据结构的动机，复杂度，和一些特性。但大部分的数据结构基础操作忽略了，有些地方比较好奇，却没有讲到。这是我目前读到第六章的感受。但快开学了，这些书不太可能全看完，接下来怎么看还是得看心情咯。 最近的读书总结其实还看了不少书，比如《深度学习》、《机器学习》、《统计学习方法》、《流畅的python》等，这些应该是一两个月前开始看的，都是需要长期学习的知识，在暑假机器学习集训班学习时来补充机器学习相关知识的。之后会写机器学习相关的总结，所以没有在这里总结一些知识跟感想。还有部分书是跟下学期的课比较紧密的，提前看看也算是预习，本身也挺感兴趣的，也不一一列出了，读的篇幅不多，列出意义也不大。","categories":[{"name":"study","slug":"study","permalink":"https://yjhmelody.github.io/categories/study/"}],"tags":[{"name":"summary","slug":"summary","permalink":"https://yjhmelody.github.io/tags/summary/"}]},{"title":"最近一周总结","slug":"最近一周总结","date":"2017-08-20T12:57:17.000Z","updated":"2017-09-24T11:34:44.874Z","comments":true,"path":"2017/08/20/最近一周总结/","link":"","permalink":"https://yjhmelody.github.io/2017/08/20/最近一周总结/","excerpt":"总结近况杂项发现已经漏了好几天没有写博客，也不知道自己能否坚持下去，最近可能也比较懈怠。C++的模版接触了下，发现使用还是容易掌握的，但后来就没有学下去了。靠这样学是永远学不完的，甚至学了一部分忘记七七八八了。打算下学期如果适合C++编程的任务，尽量使用C++来完成，这样有任务的使用C++应该能学的更快，更扎实，C++学好了，很多东西也都能学好，果然不是空穴来风。 兴趣方向最近学习深度学习的基础，主要是全连接神经网络(FNN)和卷积神经网络(CNN)，其实正式学习是下周，所以这周在机器学习方面懈怠很多。其他方面呢，编译原理，《编译器设计》看了些内容，感觉很有收获，虽然不是直接关于编译原理的收获，但对计算机的全貌有了更进一步的了解。期间，查了一些关于链接的知识，也看了一点点关于linux kernel的知识，总的来说，收获了一点点学习这些知识的方法论。","text":"总结近况杂项发现已经漏了好几天没有写博客，也不知道自己能否坚持下去，最近可能也比较懈怠。C++的模版接触了下，发现使用还是容易掌握的，但后来就没有学下去了。靠这样学是永远学不完的，甚至学了一部分忘记七七八八了。打算下学期如果适合C++编程的任务，尽量使用C++来完成，这样有任务的使用C++应该能学的更快，更扎实，C++学好了，很多东西也都能学好，果然不是空穴来风。 兴趣方向最近学习深度学习的基础，主要是全连接神经网络(FNN)和卷积神经网络(CNN)，其实正式学习是下周，所以这周在机器学习方面懈怠很多。其他方面呢，编译原理，《编译器设计》看了些内容，感觉很有收获，虽然不是直接关于编译原理的收获，但对计算机的全貌有了更进一步的了解。期间，查了一些关于链接的知识，也看了一点点关于linux kernel的知识，总的来说，收获了一点点学习这些知识的方法论。 英文资料这两天买了两本英文书，打算常常读一读，一本是go圣经，一本是《编译原理与实践》。两本对于我来说应该难度不大。go圣经我看过半本中文版，虽然还掌握的不好，但有了前置知识，看英文只是时间问题。《编译原理与实践》比《龙书》跟《编译器设计》要简单些，所以买了英文版，打算下学期的编译原理课配合着读。这么说来了，在编译原理相关的书里，我已经接触了6，7本了，但其实没有精读过任何一本。下学期重点看这三本吧，以我现在半吊子的水平，希望本科基本能看完3，4本。毕竟最想从事的就是编译器相关工作，也说不清为什么偏爱这个。 不足与改正最近越来越意识到自己在编程方面的缺陷，不论编写代码量小还是较大，自己思路一直不清晰。说来，很多我编写的代码，是根据别人的来编写的，自己完全独立从头开始写的几乎没有。上大学以后，在学业方面的作业情况也有类似，就是没有自己的思路，得照着例题来“复现”思路，但这终归不是自己学到的知识。这个问题从大学开始的，已经遗留比较久了，严重影响了自己的思维。趁着下学期的许多有趣课程跟高难度的高强度的课表，改着这个坏毛病，尽量做到把书本知识深刻映入大脑，然后根据自己的思路来完成作业而不依赖参考答案。","categories":[{"name":"study","slug":"study","permalink":"https://yjhmelody.github.io/categories/study/"}],"tags":[{"name":"summary","slug":"summary","permalink":"https://yjhmelody.github.io/tags/summary/"}]},{"title":"关于未来方向学习的思考","slug":"关于未来方向学习的思考","date":"2017-08-11T15:43:14.000Z","updated":"2017-09-24T11:34:24.519Z","comments":true,"path":"2017/08/11/关于未来方向学习的思考/","link":"","permalink":"https://yjhmelody.github.io/2017/08/11/关于未来方向学习的思考/","excerpt":"总结现状如今今天是周五，等会就周六了。虽然平时经常忙活在电脑前，但好像没在做什么特别有意义的事情，看到github一个好项目就想着去看它的源码，然后呢，迫于水平不够，一堆工具也掌握的不利索，怎么也没法开始学。其实大概是自己内心不知道以后到底想去做什么吧。其实2，3w行的代码如果搞懂它的流程，而且又熟悉相关语言并懂得一些相关方向的知识，应该是可以开始看了，起码不应该想到看就害怕而退缩，这连第一步都没有走呢。 想法比较坚定的想法呢是以后做编译相关工作，这个工作岗位很少，要求也很高，许多大神高中就开始接触了。我呢，现在还半吊子，最近才开始理解了最基本的知识，将来如果要考研，这应该是计算机体系结构这个方向的研究生吧。而这2个月一直在学机器学习，但我只是对它的原理好奇，又想着机器学习方向能跟数学打交道，又能锻炼算法能力，便很欣然地参加了这次暑假集训。最近这个月有些忙坏了，但又觉得自己学到知识很少，可能是太贪心了吧，自己对计算机许多方向都感兴趣，但未曾深入其中一个。我还经常跟同学说那句话：不要总想着以后要干嘛，你现在在干嘛，以后很可能就在干嘛，所以要干嘛就赶紧现在就开始干。其实我自己也没有做到我所说，虽然我非常希望自己能做到。","text":"总结现状如今今天是周五，等会就周六了。虽然平时经常忙活在电脑前，但好像没在做什么特别有意义的事情，看到github一个好项目就想着去看它的源码，然后呢，迫于水平不够，一堆工具也掌握的不利索，怎么也没法开始学。其实大概是自己内心不知道以后到底想去做什么吧。其实2，3w行的代码如果搞懂它的流程，而且又熟悉相关语言并懂得一些相关方向的知识，应该是可以开始看了，起码不应该想到看就害怕而退缩，这连第一步都没有走呢。 想法比较坚定的想法呢是以后做编译相关工作，这个工作岗位很少，要求也很高，许多大神高中就开始接触了。我呢，现在还半吊子，最近才开始理解了最基本的知识，将来如果要考研，这应该是计算机体系结构这个方向的研究生吧。而这2个月一直在学机器学习，但我只是对它的原理好奇，又想着机器学习方向能跟数学打交道，又能锻炼算法能力，便很欣然地参加了这次暑假集训。最近这个月有些忙坏了，但又觉得自己学到知识很少，可能是太贪心了吧，自己对计算机许多方向都感兴趣，但未曾深入其中一个。我还经常跟同学说那句话：不要总想着以后要干嘛，你现在在干嘛，以后很可能就在干嘛，所以要干嘛就赶紧现在就开始干。其实我自己也没有做到我所说，虽然我非常希望自己能做到。 兴趣最近查阅了一些资料，其实都是自己以前都看过，只是很多不少都忘记了或者没有重视起来。记得大一时候对C++兴趣特别浓厚，买了C++primer和STL源码剖析，还很认真看过（但没怎么敲代码），后来接触了的其他编程语言，C++也基本没有使用过了。现在要是还记得大一C++要求的内容就不错咯。 但我知识，我想以后从事的方向很可能离不开C++（至少10年是这样）。不论是机器学习或者是编译原理，还是网络编程，在高性能的场合，永远需要它。 目前我比较感兴趣的是编译原理，机器学习，网络编程吧。这三个都可以学很深，无底洞。编译原理简单的话，写个编译器前端，嗯，可能还不如自动化工具强，学的深入了，就得学后端了，代码优化，无底洞，国内好像没几家公司需要这种人才（大概更不需要应届生吧）。不过作为“永不失业的职业”，我对编译原理里面的设计成分更感兴趣，即自制语言。而如果作为一个应用开发者的话，约束应该大很多了，很多时候公式让你做的跟你的兴趣点完全无关，再考虑国内公司的尿性，不让你干缺德的就不错了。 扯远了，关于“设计”这种概念，对于机器学习跟网络编程，也同样适用吧。机器学习重在解决问题，而设计算法是重要环节，这里设计部分可能要发挥脑力，而不是跟风地调用算法，我觉得这里也是很有意思的部分，但如果是解决商业问题，或者做项目的话，自由度感觉不大，倒是留在实验室研究什么的更有意思（虽然最近就是留实验室学习，有点小枯燥）。而网络编程，重点是在网络层以上的开发，打交道是传输层跟应用层，设计协议是很有意思的一环。怎么样的协议更有语义，更高效，更安全，扩展性强，我觉得这部分自制性很强，也是很有研究的感觉。 隐隐约约觉得以上三个方向深入研究最后都要跟并发或性能打交道。毫无疑问，编译原理，优化部分，可能需要把代码隐式转换为并发代码，或者编译成适合并发的机器代码。机器学习虽然是高阶算法，但依托于大数据平台，而底层需要高并发，分布式的架构。有时候算法策略本身可能需要考虑容易并发，而如果设计一个机器学习平台或者系统，底层模型肯定到处是并发，也明显需要分布式。网络编程呢，则是为了更好的进行数据交换，也是并发的策略之一，更是分布式计算的基石。 以上三个方向，在底层方面最需要的便是C++了。众所周知C++是比较难掌握的，虽然现在情况在变好，但我其实还蛮担心自己毕业前连基本的STL都用不好。 基本规划好像写了这么多，还是不清楚接下来应该如何去学习知识，在这最后的2年时间。其实下个学期课挺多的，而且都是比较难的课，虽然大部分我都挺感兴趣的，但精力一旦不足，我容易对计算机感到一种疲惫，然后几天不碰。而下个学期如此多的课的情况，在学好课内之余如何把课外感兴趣的知识补上呢（C++，机器学习，网络编程深入学习）。可能就只能在周末多抽出时间学习了吧。算上准备考研，也该多用用C++了，这样数据结构也算复习了。而下学期部分课需要一些数学知识，正好稍微注重一下复习数学，也算事半功倍了。网络编程这部分，即使只是简单地学习，对计算机网络这门课帮助也不少啊，感觉咬咬牙，下学期真的能学到很多知识吧，只希望自己不要自暴自弃，要善于总结学到的知识。 如此一想，感觉也想通了一些，毕竟学习编译原理，计算机网络，操作系统的时候是完全可以用C++来实现相应的功能的，这也锻炼了我C++的水平，但可能目前完全达不到吧，所以打算暑假稍微学习一下C++。 嗯，决定了，暑假还剩一个月，我打算学习以下内容： 机器学习跟深度学习，这部分跟实验室进度来，尽量不落下吧 C++，看些简单的源码，学习一下STL，把忘记的捡起来 简单的编译原理学习，实现几个简单的玩具编译器（已经跟着博客抄了一个，感觉还需要再写一个），如果可能的话，用C++来实现 网络编程，这一块，估计很麻烦，我基本没有实战过，倒是有基本的理论知识，理想情况下应该是golang或者nodejs来写（它们封装的很好，写起来难度容易接受），C++写的话，估计写一个月也写不出什么。所以尽量暑假尝试写个简单协议（突然想起来之前写redgo，有机会完善下） 其实对C++一向是又爱又恨，爱是觉得这门语言太重要了，感觉如果掌握，用其他语言也就游刃有余了，而且有些语法感觉真的很厉害。恨是觉得这门语言太复杂了，我掌握很吃力，而且有些语法根本就是坑，基础设施STL也是非常复杂。写项目的话，我连相应的流程，工具，环境都不了解。 现在已经周六了，大概就写到这里了，其实我也是编写边梳理自己的思路。写出来呢，可以经常看，这样自己就不会忘记对自己的严格要求了。每次面对电脑，2小时也不做点有意义的事情的时候，你就会觉得自己真是非常没用，那句话怎么说来着：回首过去，尽是些可耻往事。 大概不论爱好什么，最简单证明自己的热情，是不留余力地享受自己的爱好吧","categories":[{"name":"life","slug":"life","permalink":"https://yjhmelody.github.io/categories/life/"}],"tags":[{"name":"summary","slug":"summary","permalink":"https://yjhmelody.github.io/tags/summary/"}]},{"title":"机器学习里的一些小概念","slug":"机器学习里的一些小概念","date":"2017-08-10T05:53:56.000Z","updated":"2017-09-19T15:00:13.982Z","comments":true,"path":"2017/08/10/机器学习里的一些小概念/","link":"","permalink":"https://yjhmelody.github.io/2017/08/10/机器学习里的一些小概念/","excerpt":"","text":"机器学习里的一些小概念轮廓系数轮廓系数（Silhouette Coefficient），是聚类效果好坏的一种评价方式。最早由 Peter J. Rousseeuw 在 1986 提出。它结合内聚度和分离度两种因素。可以用来在相同原始数据的基础上用来评价不同算法、或者算法不同运行方式对聚类结果所产生的影响。 计算过程假设我们已经通过一定算法，将待分类数据进行了聚类。常用的比如使用K-means，将待分类数据分为了k个簇。对于簇中的每个向量。分别计算它们的轮廓系数。对于其中的一个点 i 来说： 计算 a(i) = average(i向量到所有它属于的簇中其它点的距离) 计算 b(i) = min(i向量到所有其他簇的点的平均距离) 那么i向量轮廓系数就为： $$ S(i) = \\frac{b(i) - a(i)}{max\\{a(i), b(i)\\}} $$ 判断可见轮廓系数的值是介于 [-1,1] ，越趋近于1代表内聚度和分离度都相对较优。将所有点的轮廓系数求平均，就是该聚类结果总的轮廓系数。 a(i) ：i向量到同一簇内其他点不相似程度的平均值 b(i) ：i向量到其他簇的平均不相似程度的最小值 S(i)接近1，则说明样本i聚类合理； S(i)接近-1，则说明样本i更应该分类到另外的簇； 若S(i)近似为0，则说明样本i在两个簇的边界上。 未完待续","categories":[],"tags":[{"name":"ML","slug":"ML","permalink":"https://yjhmelody.github.io/tags/ML/"}]},{"title":"k-means","slug":"k-means","date":"2017-08-08T08:18:09.000Z","updated":"2017-09-19T14:59:58.696Z","comments":true,"path":"2017/08/08/k-means/","link":"","permalink":"https://yjhmelody.github.io/2017/08/08/k-means/","excerpt":"","text":"公式排版目前没法解决唉，博客写个数学公式怎么这么揪心，好不容易解决了公式显示问题，但排版又很难控制 k-means基本原理这个星期学无监督学习(unsuperviser-learning), 最基本的是k-means算法.k-means属于原型聚类:假设聚类结构能通过一组原型刻画. 而聚类本身是根据数据相似度来划分的,即”距离”.我这里不展开讲距离计算, 姑且用欧几里德距离来理解，即平时最常见的公式. 我们先写出k-means的最小化平方误差: 给定样本集合 $D = {x_1, x_2, \\dots, x_m}$, k-means对聚类所得的簇划分 $C = {C_1, C_2, \\dots, C_k}$ 的最小平方误差是 $E = \\sum_{i=1}^k \\sum_{x \\in C_i} ||x - \\mu_i||_2^2 $, 其中 $\\mu_i = \\frac{1}{|C_i|} \\sum_{x \\in C_i}x$是簇$ C_i $ 的均值向量 可以看出来, 求该式子的最优解并不容易, 因为需要考察每个样本可能的划分情况(NP难问题),所以k-means 采用贪心策略, 通过迭代优化求近似解. 感觉学完以上内容, 脑子里好像有了这个算法的轮廓了, 但仔细一想, 还有很多地方需要斟酌.k-means是划分为k个类别, 那一开始怎么划分呢?我们想到一开始需要选k个样本当作中心点来计算距离, 根据这k个位置就求得每个样本离哪个位置最接近. 这样,第一次聚类就完成了, 但是由于这次聚类造成每个类别的中心点发生了偏移, 我们需要重新计算这时候的中心点,而由于发生了偏移,我们又需要重新计算所有样本最接近的… 如何反复,直到中心点(即均值向量)不变, 便完成了聚类. 如此说来, 这个算法还是蛮朴素的, 但是还有需要斟酌的地方.如何选初始点呢？万一2个点靠得近，或者是离群点, 岂不是可能会出现很极端的聚类(比如一个人一个类别).个人觉得这有时候很难避免, 但可以通过多次随机选取初始点并评估各个聚类的效果(最小平方误差),来减少这种情况. 嗯,好像对算法的轮廓又清晰了, 但可能又有疑问, 到底如何选择k值呢?这里应该要具体问题具体分析了.如果只是想得到更好的性能, 把应该把k值从小到大递增来比较选择最优, 但这不一定给你带来实际上的最大收益.比如你是服装商, 把消费者划分了不同尺码的类别来针对客户进行推荐商品, 那到底多细才最优呢,反正我是不知道, 这得根据实际情况和专家经验来决策,而k-means在这里不充当决策者, 更像是打下手的. 扯远了,总之对于k-means, 知道k值我觉得往往很关键. 算法伪代码这样子算法应该比较清晰了,我下来列出(抄上)伪代码, 参考自周志华的西瓜书: 输入: 样本集 $D = \\{x_1, x_2, ..., x_m\\}$, 聚类簇数 k 过程: 从D中随机选择k个样本作为初始均值向量 $\\{\\mu_1, \\mu_2, ..., \\mu_k\\}$ repeat $C_i = \\emptyset$ for j = 1, 2, ..., m do 计算$x_j$与各均值向量 \\mu_i(1 \\leq i \\leq k)的距离: $d_ji = ||x_i - \\mu_i||_2$; 根据距离最近的均值向量确定x_j的簇标记: $\\lambda_j = argmin_{i \\in \\{1,2,...,k\\}}d_ij$; 将样本$x_j$划入相应的簇: $C_{\\lambda_j} = C_{\\lambda_j} \\bigcup\\{x_j\\}$ end for for i = 1, 2, ..., k do 计算新均值向量: $\\mu_i^` = \\frac{1}{|C_i|}\\sum_{x \\in C_i|}x$; if $\\mu_i^ \\ne\\mu_i$ then 更新 $\\mu_i$ else 保持当前均值向量不变 end if end for util 当前均值向量都没有更新 输出: 簇划分 $C = \\{C_1, C_2, ..., C_k\\}$ 扯淡暂时先写到这里,第一次写latex,第一次写博客,写的不好请见谅,以上大段文字有些是建立在自己的理解写的,如果有错误,希望能不吝赐教.写的比较浅,其实也没什么意思啊,就是梳理下自己跟其他同学的思路.","categories":[],"tags":[{"name":"ML","slug":"ML","permalink":"https://yjhmelody.github.io/tags/ML/"}]},{"title":"《Web API的设计与开发》读书笔记","slug":"《Web API的设计与开发》读书笔记","date":"2017-08-08T04:11:10.000Z","updated":"2017-09-19T14:59:46.213Z","comments":true,"path":"2017/08/08/《Web API的设计与开发》读书笔记/","link":"","permalink":"https://yjhmelody.github.io/2017/08/08/《Web API的设计与开发》读书笔记/","excerpt":"","text":"第1章 什么是 Web APIWeb API 的重要性一些在线服务能够对那些公开的API进行会话控制、访问控制、服务分析，提供面向用户的仪表盘，以及发布文档等，承接了各种各样的工作。 通过对外公开 Web API，同外部其他服务的集成变得更加便捷，并从中衍生出了新的价值，使在线服务以及业务不断发展，逐步形成了“API经济学”的景象，并在这几年受到相当大的关注。 各种各样的 API 模式 将已发布的 Web 在线服务的数据或功能通过 API 公开 将附加在其他网页上的微件公开 构建现代 Web 应用 开发智能手机应用 公司内部多个系统的集成 应该通过 API 公开什么 最简洁的答案是将你的在线服务所能做的事情全部通过API公开 不存在彻底屏蔽搜集信息的行为，所以无需担心盗用，公开 API并不意味着毫无限制的访问 公开 API 将原来的服务组合成新的应用来为用户提供服务的“间接销售”模式 设计优美 API 的重要性 易于使用 便于更改 健壮性好 不怕公之于众 如何美化 Web API两个重要原则： 设计规范明确的内容必须遵守相关规范 没有设计规范的内容必须遵守相关事实标准 REST 与 Web APIREST 依次一般指下面两种意思： 符合 Fielding 的 REST 架构风格的Web服务系统 符合 RPC风格的 XML (或JSON) + HTTP 接口的系统(不使用SOAP) 小结 如果尚未公开 Web AP，则应立即考虑公开 设计优美的 Web API 不用过分拘泥于 REST 一词","categories":[{"name":"study","slug":"study","permalink":"https://yjhmelody.github.io/categories/study/"}],"tags":[{"name":"web","slug":"web","permalink":"https://yjhmelody.github.io/tags/web/"}]},{"title":"欢迎，刚搭建好的博客","slug":"欢迎，刚搭建好的博客","date":"2017-08-08T01:11:10.000Z","updated":"2017-10-24T09:26:44.991Z","comments":true,"path":"2017/08/08/欢迎，刚搭建好的博客/","link":"","permalink":"https://yjhmelody.github.io/2017/08/08/欢迎，刚搭建好的博客/","excerpt":"","text":"分享知识与见解博客搭建的过程照理说用hexo搭建博客是一分钟的事情，结果我花了6小时，其中绝大部分时间花在了主题上面，想找个能写数学公式的主题不容易啊，又要符合自己的审美，而且文档要齐全没有bug。最后找到了这个，但是文档还是不够详解，有些功能不知道如何使用，但暂且将就下吧。这个主题自带了mathjax，不过我还没有测试过能不能写公式，我现在就来测试 $ (\\frac{1}{2})^2 = \\frac{1}{4} $ 顺便测试下代码效果吧 12345678class Person&#123; constructor(name, age)&#123; this.name = name this.age = age &#125;&#125;let person = new Person('yjh', 20) 我把这个博客发了一下，看见真的可以写公式了，很开心，但这6个小时的苦大概是说不清了，而现在已经9点多了，我应该要继续学习无监督学习了。暂且这样吧，回去再折腾折腾，现在赶紧发个url给同学来测试下效果吧！","categories":[{"name":"life","slug":"life","permalink":"https://yjhmelody.github.io/categories/life/"}],"tags":[{"name":"life","slug":"life","permalink":"https://yjhmelody.github.io/tags/life/"}]}]}