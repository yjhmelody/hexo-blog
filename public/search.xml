<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[实现Lua-AST]]></title>
    <url>%2F2019%2F03%2F24%2FImplement-Lua-AST%2F</url>
    <content type="text"><![CDATA[实现Lua解释器-AST调研最近终于开始做毕设了，于是开始考虑是否用一款语法分析生成器来写Lua前端，遂发现Pest, 一款Rust的语法分析器，使用PEG文法。这个文法有点像BNF和正则的融合，但是定义无歧义，表达能力小于等于CFG。写起来比较舒服，考虑到Lua官方文法整理出来有许多左递归文法，改写很麻烦（我不太会），虽然Pest本身编写很友好，但是感觉文档描述有点少，有些关键的地方使用不清楚，加上没有IDE的代码提示，遂放弃（不过写一些小语言的简单文法以后可以考虑）。之前用过Nom这款解析组合子库，这个库性能非常好，但是我觉得语法设计的非常怪异，之前用也没搞清楚，对于之前编写的某个玩具虚拟机的字节码编译器使用起来也是小题大做，反而搞复杂了。 于是乎，还是打算手写。考虑到Lua本身文法少，手写代码量不会多上几百行；自己之前也没有实现一个比较满意的解析器；如果用语法生成生成器，首先还得学，然后得花时间改写文法，有点得不偿失。 词法首先随便写了点词法这块，发现我抽象能力太弱了，本来想多用第三方库实现更抽象性能更好的词法分析类，但我有点不太熟悉泛型编程，而且前期具体工作不明确，写了很大概率后面要改。于是乎，就是很粗糙地使用了String类型。而对于具体的词法分析，感觉写过很多次，每次套路都写的不一样，参考别人的实现，也都是五花八门，我也没法判断哪种够优雅。而对于Lua语言，毕竟是工业级语言，词法分析实现细节一定要参考书或官方文档，语法细节非常多。 先从AST开始果然了解一门语言除了常见的语法特性见识了以外，看文法和AST感觉也不错。不过我个人对文法不太感兴趣，也不太擅长，而AST可以看清楚这个语言的很多语义。下面整理一下Lua的AST，可能设计的一般般，以后肯定还需要改动。下面我只列出AST，而不写出文法。 123456#[derive(Debug)]pub struct Block &#123; last_line: Line, stats: Vec&lt;Stat&gt;, ret_exps: Vec&lt;Exp&gt;,&#125; Block，或 Chunk，是Lua最小编译单位。Lua解释器通过传递Block给编译器，生成字节码后解释执行。Block必须包含语句和返回值，当然返回值可以有多个，而且可以是表达式，这个很自然。 接下来的 Stat有点长，它表示Lua的语句(statment)。语句包括了表达式(expression)的结构，这跟许多静态语言不一样所以会比较长，但这也是它的主体了。仔细一看，Stat也只有几个流程控制，赋值（包括变量定义），函数定义，无他。写过一点lua就知道，lua没有哪些地方的代码需要提升(lift)，都是在解释时进入到环境的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#[derive(Debug)]pub enum Stat &#123; Empty, Break &#123; line: Line, &#125;, Label &#123; name: String, &#125;, Goto &#123; name: String, &#125;, Do &#123; block: Box&lt;Block&gt;, &#125;, While &#123; exp: Exp, block: Box&lt;Block&gt;, &#125;, Repeat &#123; exp: Exp, block: Box&lt;Block&gt;, &#125;, If &#123; exps: Vec&lt;Exp&gt;, blocks: Vec&lt;Block&gt;, &#125;, ForNum &#123; line_of_for: Line, line_of_do: Line, var_name: String, exps: (Exp, Exp, Exp), block: Box&lt;Block&gt;, &#125;, ForIn &#123; line_of_do: Line, name_list: Vec&lt;String&gt;, exp_list: Vec&lt;Exp&gt;, block: Box&lt;Block&gt;, &#125;, LocalVarDecl &#123; last_line: Line, name_list: Vec&lt;String&gt;, exp_list: Vec&lt;Exp&gt;, &#125;, Assign &#123; last_line: Line, var_list: Vec&lt;Exp&gt;, exp_list: Vec&lt;Exp&gt;, &#125;, LocalFnDef &#123; name: String, exp: Box&lt;Exp&gt;, &#125;,&#125; 写到这里，说实话好像讲AST没什么意思，于是下面这个也没什么好总结了。不过要注意好一些值的限定，虽然有些地方使用了Exp，但是其实里面用的枚举值是固定的，其实可以提出来单独定义。我为了整体的简洁性，就没这样做。在后面的实现中，应该对性能和代码量有一定影响，不过到时候改也不迟。这里有个不好的地方就是Line几乎都用到了，为什么不封装一下呢？主要是还是为了简单而多了样板代码，到后期写的时候不会因为类型过多而处理麻烦。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#[derive(Debug)]pub enum Exp &#123; Nil &#123; line: Line, &#125;, True &#123; line: Line, &#125;, False &#123; line: Line, &#125;, Vararg &#123; line: Line, &#125;, Integer &#123; line: Line, val: i64, &#125;, Float &#123; line: Line, val: f64, &#125;, String &#123; line: Line, val: String, &#125;, Name &#123; line: Line, val: String, &#125;, Unop &#123; line: Line, op: usize, exp: Box&lt;Self&gt;, &#125;, Binop &#123; line: Line, op: usize, exp1: Box&lt;Self&gt;, exp2: Box&lt;Self&gt;, &#125;, Concat &#123; line: Line, exps: Vec&lt;Self&gt;, &#125;, TableConstructor &#123; line: Line, last_line: Line, keyExps: Vec&lt;Exp&gt;, valExps: Vec&lt;Exp&gt;, &#125;, FnDef &#123; line: Line, last_line: Line, par_list: Vec&lt;String&gt;, is_vararg: bool, block: Box&lt;Block&gt;, &#125;, Parens(Box&lt;Exp&gt;), TableAccess &#123; last_line: Line, // line of ']' prefix_exp: Box&lt;Exp&gt;, key_exp: Box&lt;Exp&gt;, &#125;, FnCall &#123; line: Line, // line of '(' last_line: Line, // line of ')' prefix_exp: Box&lt;Exp&gt;, name_exp: Box&lt;Exp&gt;, args: Vec&lt;Exp&gt;, &#125;,&#125; 最后发现AST和Lua的chunk二进制格式定义非常相近。大概原因是Lua字节码的抽象程度也非常高，这样代码生成难度也降低了很多。如果是把Lua编译到像WebAssembly这样的字节码中，可能得手写字节码先封装出类型Lua字节码层次的函数供代码生成，不然每行代码段的生成可能包括复杂的字节码。]]></content>
      <categories>
        <category>CS</category>
      </categories>
      <tags>
        <tag>Book</tag>
        <tag>Rust</tag>
        <tag>Lua</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018年总结和未来展望]]></title>
    <url>%2F2018%2F12%2F31%2F2018%E5%B9%B4%E6%80%BB%E7%BB%93%E5%92%8C%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B%2F</url>
    <content type="text"><![CDATA[昨天是我的公历生日，本作为总结2018这年的大好日子，可惜拖延症犯了，把博客一些地方小改一下就作罢了。 考研迷思虽然备考途中，多次心情波动，懒癌和睡眠等问题，几乎要放弃考研了，甚至有次备考资料丢失，还是多亏周围的同学对我的鼓励和支持，最后也算勉勉强强上道了。最后几天几乎也没法提升水平了，只好不停地看错题，记忆一些生僻的公式，翻翻书把近几年不常考的知识点也巩固下。那时候想着，只要好好发挥，不要再在计算上犯错就应该能够达成目标。 考研前到考研结束的那10天，几乎是我最放松的几天。这让想起来了高考，最后一个月也是这般，因为清楚的认识到自己的能力和短期的可能性，变得不会盲目地学习，而是以考研也是生活中的一部分这样的处事态度来度过。所以我每天照样追番，照样看其他感兴趣的领域知识，我觉得人生不过如此了。如果把目光聚焦于当前，“考研失败”，“失业”，“感染疾病”，“单身一辈子”，“没有朋友”等这些人们口中常道的事物都让我不觉得可怕。 不过我还是对一个里程碑式的事件的结束所产生的环境影响、人际影响和社会影响大意了。如果说考研时候周围大部分人与你是同一战线，那结束后，人们往往更容易走向对立面。当我对考研时人们相互鼓励而记忆犹新之时，周围人对同一件事物的分歧在考研结束后完全暴露出来了。在此我不具体展示讲述这些矛盾的变化，只总结一下，人们对同一事物产生的不同焦虑的原因。 拿我自己来讲，我只是个喜欢意淫美好生活，偶尔为之奋斗的人罢了。可能在别人眼里我也勉强算个理想主义者，不过目前还未闻有人如此道之。我也不太了解别人是如何评价我的，我这样的性格决定了我面对一件事情的时候，不太喜欢做评价（对好坏的事件均保留积极和消极的看法）。如果其他人过于消极或积极，我倾向于调和他的极端的想法。所以考研的时候，我自己虽然有自己一套协调的方法论，但自己仍然有许多焦虑，最后10多天才看开了。不过其他同学，大多数都持有消极的情绪，当我遇见他们时，总是设法通过谈话让他们积极点，虽然目前周围人总是该怎么做还是怎么做，什么样的三观，什么的方法论，什么的处事原则，在我看来并没有改变。 他们说自己考研是为了更好的学历，更好的工作，更好的发展。虽然我理解，但我总是像个乡巴佬一样想不明白————既然只是为了更好的东西，而从未明显表现对现状的不满（该打游戏还是打，整天吃喝玩乐还是吃喝玩乐，吃饭总要一起约的人，还有说有笑的，哪里看得出不幸福？），那么所谓的现状为何要在明面上要遭人唾弃，这让我想起来了“政治正确”这个东西。 那么他们焦虑的源头哪里来的？我稍作了思考还通过和一些同学的交流来总结。可能的原因是： 他们作为人，却无法独立。许多东西的价值要依托于人，才能发挥出来。拿考研来说，他们的主体不是自我，而是目标院校、家庭、优越的工作等（我不是说这些不重要，我强调的是人首先得重视自我本身，而不是被他人要求的自我）。脱离人肯定会产生焦虑和不安，但过分依赖也一样会因为波动而产生焦虑。我个人自己也在往看淡这些而努力中。 理想、物质等方面作为指导和追求，绑架了人本身。人们开始不经过思考还追逐东西，我觉得这是物化的表现，即非生物的一般物质的运动（首先表明我坚持唯物观点）。玩梗的话就是说“还没思考，身体已经动了起来”。当年“大妈”争相抢购大甩卖的商品这样的现象，如今在更广泛的群体中以更隐蔽的方式表现出来。最后人追求幸福变成了追求固化的，物化的事物，在人们眼里太具体了，以至于像物质的运动一般，眼光像光线一样直直地发射出来。我还记得以前考试的作文，“孩子的眼光是直的，不会转弯”，如今看来孩子的思维虽然也是直的，不过倒是比许多人深邃多了，着实让人捉摸不透。 笔可能要峰回路转了，才想起应该写的是总结。 对18年道声谢18年看过心理老师，18年焦虑过非常多次，18年参加过2次比赛，18年准备了半年考研，18年还看过许多书，18年下了两场雪，18年末尾还投了2个简历，18年父母又开店工作了。 18年我才知道我其实对很多甚至不局限于计算机的知识感兴趣，虽然一直没有学好相关知识，不过应该能保持学习热情。 对19年展望目前考研水平未知，我自己是不感兴趣也不喜欢校对考试答案的。所以在可能考上这个前提下，准备找份实习，如果考研失败，则实习转正或者继续找新的公司（当然目前还没有开始面试公司）。毕设选了一个很有挑战的题目，所以我也蛮忐忑的，他们总说为什么不选个简单的，现在想来我不应该那样回答他们。目前打算把最后一个作业赶紧完成，之后整理下19年想看的书目，为自己安排一个合理的学习时间。如果有机会去北京（实习需要，考研成功等），我想去清华和北大感受下，可能的话还想去天安门和长城。工作相关，我其实一直是抛开像简历那样具体的东西的，总体上我其实想做计算机基础设施相关的研究或者工程。 可能许多同学虽然在别人眼里已经是前程似锦，不过似乎他们也有难言之痛，似乎活在当下必要受到煎熬。我没有经历过大风大浪，只觉得生活依然美好。]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Graduate</tag>
        <tag>Life</tag>
        <tag>Future</tag>
        <tag>Job</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《DDIA》概念整理]]></title>
    <url>%2F2018%2F08%2F30%2F%E3%80%8ADDIA%E3%80%8B%E6%A6%82%E5%BF%B5%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[最近指忙里抽闲的看《Designing Data-Intensive Applications》（《设计数据密集型应用》）这本书，主要书看中文为主。这本书非常好，我现在刚看完一半。这本书让你认识到了数据库系统和分布式系统的大概貌，没有深入繁琐的具体细节也没有让人浅尝辄止。这本书网络上评价很高，甚至豆瓣评分有2个评分，一个是9.5分，一个是10分。现在即使想作为一个普通的后端工程师也需要学习分布式知识了，强烈推荐这本书。 我发现书后面的术语表非常精良，个人认为：虽然可能还涵盖不到一半的核心概念，但是如果能深刻的理解这术语表部分的，那对数据库系统和分布式系统就有了比较好的整体认识了。 顺便，光看这本书就需要接触过数据结构，操作系统，计算机网络，数据库原理，还有少部分组成原理的知识，不然只能知其然不知其所以然。限于自己的个人水平和这方面不多的实践，有些部分还没有理解到位。个人认为，学习分布式可能还需要学习一点并行编程和网络编程，分布式系统可能是综合性比较强的方向了。并且根据具体要做的分布式系统，可能还需要机器学习，搜索引擎等知识。 这本书配合着学习 MIT6.824 可能会非常好吧。 以下整理出了英文和中文的术语表。 英文asynchronousNot waiting for something to complete (e.g., sending data over the network to another node), and not making any assumptions about how long it is going to take. See Synchronous Versus Asynchronous Replication on page 153, Synchronous Versus Asynchronous Networks on page 284, and System Model and Reality on page 306. atomic In the context of concurrent operations:describing an operation that appears to take effect at a single point in time, soanother concurrent process can never encounter the operation in a halffinished state. See also isolation. In the context of transactions: grouping together a set of writes that must either all be committed or all be rolled back, even if faults occur. See Atomicity on page 223 and Atomic Commit and Two-Phase Commit (2PC) on page 354. backpressureForcing the sender of some data to slow down because the recipient cannot keep up with it. Also known as flow control. SeeMessaging Systems on page 441. batch processA computation that takes some fixed (and usually large) set of data as input and produces some other data as output, without modifying the input. See Chapter 10. boundedHaving some known upper limit or size. Used for example in the context of network delay (see Timeouts and Unbounded Delays on page 281) and datasets (see the introduction to Chapter 11). Byzantine faultA node that behaves incorrectly in some arbitrary way, for example by sending contradictory or malicious messages to other nodes. See Byzantine Faults on page 304. cacheA component that remembers recently used data in order to speed up future reads of the same data. It is generally not complete: thus, if some data is missing from the cache, it has to be fetched from some underlying, slower data storage system that has a complete copy of the data. CAP theoremA widely misunderstood theoretical result that is not useful in practice. See The CAP theorem on page 336. causalityThe dependency between events that arises when one thing happens before another thing in a system. For example, a later event that is in response to an earlier event, or builds upon an earlier event, or should be understood in the light of an earlier event. See Thehappens-beforerelationship and concurrency on page 186 and Ordering and Causality on page 339. consensusA fundamental problem in distributed computing, concerning getting several nodes to agree on something (for example, which node should be the leader for a database cluster). The problem is much harder than it seems at first glance. See Fault-Tolerant Consensus on page 364. data warehouseA database in which data from several different OLTP systems has been combined and prepared to be used for analytics purposes. See Data Warehousing on page 91. declarativeDescribing the properties that something should have, but not the exact steps for how to achieve it. In the context of queries, a query optimizer takes a declarative query and decides how it should best be executed. See Query Languages for Data on page 42. denormalizeTo introduce some amount of redundancy or duplication in a normalized dataset, typically in the form of a cache or index, in order to speed up reads. A denormalized value is a kind of precomputed query result, similar to a materialized view. See Single-Object and MultiObject Operations on page 228 and Deriving several views from the same event log on page 461. derived dataA dataset that is created from some other data through a repeatable process, which you could run again if necessary. Usually, derived data is needed to speed up a particular kind of read access to the data. Indexes, caches, and materialized viewsare examples of derived data. See the introduction to Part III. deterministicDescribing a function that always produces the same output if you give it the same input. This means it cannot depend on random numbers, the time of day, network communication, or other unpredictable things. distributedRunning on several nodes connected by a network. Characterized by partial failures: some part of the system may be broken while other parts are still working, and it is often impossible for the software to know what exactly is broken. See Faults and Partial Failures on page 274. durableStoring data in a way such that you believe it will not be lost, even if various faults occur. See Durability on page 226. ETLExtract–Transform–Load. The process of extracting data from a source database, transforming it into a form that is more suitable for analytic queries, and loading it into a data warehouse or batch processing system. See Data Warehousing on page 91. failoverIn systems that have a single leader, failover is the process of moving the leadership role from one node to another. See Handling Node Outages on page 156. fault-tolerantAble to recover automatically if something goes wrong (e.g., if a machine crashes or a network link fails). See Reliability on page 6. flow controlSee backpressure. followerA replica that does not directly accept any writes from clients, but only processes data changes that it receives from a leader. Also known as a secondary, slave, read replica, or hot standby. See Leaders and Followers on page 152. full-text searchSearching text by arbitrary keywords, often with additional features such as matching similarly spelled words or synonyms. A full-text index is a kind of secondary index that supports such queries. See Full-text search and fuzzy indexes on page 88. graphA data structure consisting of vertices (things that you can refer to, also known as nodes or entities) and edges (connections from one vertex to another, also known as relationships or arcs). See Graph-Like Data Models on page 49. hashA function that turns an input into a random-looking number. The same input always returns the same number as output. Two different inputs are very likely to have two different numbers as output, although it is possible that two different inputs produce the same output (this is called a collision). See Partitioning by Hash of Key on page 203. idempotentDescribing an operation that can be safely retried; if it is executed more than once, it has the same effect as if it was only executed once. See Idempotence on page 478. indexA data structure that lets you efficiently search for all records that have a particular value in a particular field. See Data Structures That Power Your Database on page 70. isolationIn the context of transactions, describing the degree to which concurrently executing transactions can interfere with each other. Serializable isolation provides the strongest guarantees, but weaker isolation levels are also used. See Isolation on page 225. joinTo bring together records that have something in common. Most commonly used in the case where one record has a reference to another (a foreign key, a document reference, an edge in a graph) and a query needs to get the record that the reference points to. See Many-to-One and Many-to-Many Relationships on page 33 and Reduce-Side Joins and Grouping on page 403. leaderWhen data or a service is replicated across several nodes, the leader is the designated replica that is allowed to make changes. A leader may be elected through some protocol, or manually chosen by an administrator. Also known as the primary or master. See Leaders and Followers on page 152. linearizableBehaving as if there was only a single copy of data in the system, which is updated by atomic operations. See Linearizability on page 324. localityA performance optimization: putting several pieces of data in the same place if they are frequently needed at the same time. See Data locality for queries on page 41. lockA mechanism to ensure that only one thread, node, or transaction can access something, and anyone else who wants to access the same thing must wait until the lock is released. See Two-Phase Locking (2PL) on page 257 and The leader and the lock on page 301. logAn append-only file for storing data. A write-ahead log is used to make a storage engine resilient against crashes (see Making B-trees reliable on page 82), a log-structured storage engine uses logs as its primary storage format (see SSTables and LSM-Trees on page 76), a replication log is used to copy writes from a leader to followers (see Leaders and Followers onpage 152), and an event log can represent a data stream (see Partitioned Logs on page 446). materializeTo perform a computation eagerly and write out its result, as opposed to calculating it on demand when requested. See Aggregation: Data Cubes and Materialized Views on page 101 and Materialization of Intermediate State on page 419. nodeAn instance of some software running on a computer, which communicates with other nodes via a network in order to accomplish some task. normalizedStructured in such a way that there is no redundancy or duplication. In a normalized database, when some piece of data changes, you only need to change it in one place, not many copies in many different places. See Many-to-One and Many-to-Many Relationships on page 33. OLAPOnline analytic processing. Access pattern characterized by aggregating (e.g., count, sum, average) over a large number of records. See Transaction Processing or Analytics? on page 90. OLTPOnline transaction processing. Access pattern characterized by fast queries that read or write a small number of records, usually indexed by key. See Transaction Processing or Analytics? on page 90. partitioningSplitting up a large dataset or computation that is too big for a single machine into smaller parts and spreading them across several machines. Also known as sharding. See Chapter 6. percentileA way of measuring the distribution of values by counting how many values are above or below some threshold. For example, the 95th percentile response time during some period is the time t such that 95% of requests in that period complete in less than t, and 5% take longer than t. See Describing Performance on page 13. primary keyA value (typically a number or a string) that uniquely identifies a record. In many applications, primary keys are generated by the system when a record is created (e.g., sequentially or randomly); they are not usually set by users. See also secondary index. quorumThe minimum number of nodes that need to vote on an operation before it can be considered successful. See Quorums for reading and writing on page 179. rebalanceTo move data or services from one node to another in order to spread the load fairly. See Rebalancing Partitions on page 209. replicationKeeping a copy of the same data on several nodes (replicas) so that it remains accessible if a node becomes unreachable. See Chapter 5. schemaA description of the structure of some data, including its fields and datatypes. Whether some data conforms to a schema can be checked at various points in the data’s lifetime (see Schema flexibility in the document model on page 39), and a schema can change over time (see Chapter 4). secondary indexAn additional data structure that is maintained alongside the primary data storage and which allows you to efficiently search for records that match a certain kind of condition. See Other Indexing Structures on page 85 and Partitioning and Secondary Indexes on page 206. serializableA guarantee that if several transactions execute concurrently, they behave the same as if they had executed one at a time, in some serial order. See Serializability on page 251. shared-nothingAn architecture in which independent nodes—each with their own CPUs, memory, and disks are connected via a conventional network, in contrast to sharedmemory or shared-disk architectures. See the introduction to Part II. skew Imbalanced load across partitions, such that some partitions have lots of requests or data, and others have much less. Also known as hot spots. See Skewed Workloads and Relieving Hot Spots on page 205 and Handling skew on page 407. A timing anomaly that causes events to appear in an unexpected, nonsequential order. See the discussions of read skew in Snapshot Isolation and Repeatable Read on page 237, write skew in Write Skew and Phantoms on page 246, and clock skew in Timestamps for ordering events on page 291. split brainA scenario in which two nodes simultaneously believe themselves to be the leader, and which may cause system guarantees to be violated. See Handling Node Outages on page 156 and The Truth Is Defined by the Majority on page 300. stored procedureA way of encoding the logic of a transaction such that it can be entirely executed on a database server, without communicating back and forth with a client during the transaction. See Actual Serial Execution on page 252. stream processA continually running computation that consumes a never-ending stream of events as input, and derives some output from it. See Chapter 11. synchronousThe opposite of asynchronous. system of recordA system that holds the primary, authoritative version of some data, also known as the source of truth. Changes are first written here, and other datasets may be derived from the system of record. See the introduction to Part III. timeoutOne of the simplest ways of detecting a fault, namely by observing the lack of a response within some amount of time. However, it is impossible to know whether a timeout is due to a problem with the remote node, or an issue in the network. See Timeouts and Unbounded Delays on page 281. total orderA way of comparing things (e.g., timestamps) that allows you to always say which one of two things is greater and which one is lesser. An ordering in which some things are incomparable (you cannot say which is greater or smaller) is called a partial order. See The causal order is not a total order on page 341. transactionGrouping together several reads and writes into a logical unit, in order to simplify error handling and concurrency issues. See Chapter 7. two-phase commit (2PC)An algorithm to ensure that several database nodes either all commit or all abort a transaction. See Atomic Commit and Two-Phase Commit (2PC) on page 354. two-phase locking (2PL)An algorithm for achieving serializable isolation that works by a transaction acquiring a lock on all data it reads or writes, and holding the lock until the end of the transaction. See Two-Phase Locking (2PL) on page 257. unboundedNot having any known upper limit or size. The opposite of bounded. 中文异步（asynchronous）不等待某些事情完成（例如，将数据发送到网络中的另一个节点），并且不会假设要花多长时间。请参阅第153页上的同步与异步复制，第284页上的同步与异步网络，以及第306页上的系统模型与现实。 原子（atomic） 在并发操作的上下文中：描述一个在单个时间点看起来生效的操作，所以另一个并发进程永远不会遇到处于半完成状态的操作。另见隔离。 在事务的上下文中：将一些写入操作分为一组，这组写入要么全部提交成功，要么遇到错误时全部回滚。参见第223页的原子性和第354页的原子提交和两阶段提交（2PC）。 背压（backpressure）接收方接收数据速度较慢时，强制降低发送方的数据发送速度。也称为流量控制。请参阅第441页上的消息系统。 批处理（batch process）一种计算，它将一些固定的（通常是大的）数据集作为输入，并将其他一些数据作为输出，而不修改输入。见第十章。 边界（bounded）有一些已知的上限或大小。例如，网络延迟情况（请参阅超时和未定义的延迟在本页281）和数据集（请参阅第11章的介绍）。 拜占庭故障（Byzantine fault）表现异常的节点，这种异常可能以任意方式出现，例如向其他节点发送矛盾或恶意消息。请参阅第304页上的拜占庭故障。 缓存（cache）一种组件，通过存储最近使用过的数据，加快未来对相同数据的读取速度。缓存中通常存放部分数据：因此，如果缓存中缺少某些数据，则必须从某些底层较慢的数据存储系统中，获取完整的数据副本。 CAP定理（CAP theorem）一个被广泛误解的理论结果，在实践中是没有用的。参见第336页的CAP定理。 因果关系（causality）事件之间的依赖关系，当一件事发生在另一件事情之前。例如，后面的事件是对早期事件的回应，或者依赖于更早的事件，或者应该根据先前的事件来理解。请参阅第186页上的发生之前的关系和并发性和第339页上的排序和因果关系。 共识（consensus）分布式计算的一个基本问题，就是让几个节点同意某些事情（例如，哪个节点应该是数据库集群的领导者）。问题比乍看起来要困难得多。请参阅第364页上的容错共识。 数据仓库（data warehouse）一个数据库，其中来自几个不同的OLTP系统的数据已经被合并和准备用于分析目的。请参阅第91页上的数据仓库。 声明式（declarative）描述某些东西应有的属性，但不知道如何实现它的确切步骤。在查询的上下文中，查询优化器采用声明性查询并决定如何最好地执行它。请参阅第42页上的数据的查询语言。 非规范化（denormalize）为了加速读取，在标准数据集中引入一些冗余或重复数据，通常采用缓存或索引的形式。非规范化的值是一种预先计算的查询结果，像物化视图。请参见单对象和多对象操作（第228页）和从同一事件日志中派生多个视图（第461页）。 派生数据（derived data）一种数据集，根据其他数据通过可重复运行的流程创建。必要时，你可以运行该流程再次创建派生数据。派生数据通常用于提高特定数据的读取速度。常见的派生数据有索引、缓存和物化视图。参见第三部分的介绍。 确定性（deterministic）描述一个函数，如果给它相同的输入，则总是产生相同的输出。这意味着它不能依赖于随机数字、时间、网络通信或其他不可预测的事情。 分布式（distributed）在由网络连接的多个节点上运行。对于部分节点故障，具有容错性：系统的一部分发生故障时，其他部分仍可以正常工作，通常情况下，软件无需了解故障相关的确切情况。请参阅第274页上的故障和部分故障。 持久（durable）以某种方式存储数据，即使发生各种故障，也不会丢失数据。请参阅第226页上的持久性。 ETL（Extract-Transform-Load）提取-转换-加载（Extract-Transform-Load）。从源数据库中提取数据，将其转换为更适合分析查询的形式，并将其加载到数据仓库或批处理系统中的过程。请参阅第91页上的数据仓库。 故障转移（failover）在具有单一领导者的系统中，故障转移是将领导角色从一个节点转移到另一个节点的过程。请参阅第156页的处理节点故障。 容错（fault-tolerant）如果出现问题（例如，机器崩溃或网络连接失败），可以自动恢复。请参阅第6页上的可靠性。 流量控制（flow control）见背压（backpressure）。 追随者（follower）一种数据副本，仅处理领导者发出的数据变更，不直接接受来自客户端的任何写入。也称为辅助、仆从、只读副本或热备份。请参阅第152页上的领导和追随者。 全文检索（full-text search）通过任意关键字来搜索文本，通常具有附加特征，例如匹配类似的拼写词或同义词。全文索引是一种支持这种查询的次级索引。请参阅第88页上的全文搜索和模糊索引。 图（graph）一种数据结构，由顶点（可以指向的东西，也称为节点或实体）和边（从一个顶点到另一个顶点的连接，也称为关系或弧）组成。请参阅第49页上的和图相似的数据模型。 散列（hash）将输入转换为看起来像随机数值的函数。相同的输入会转换为相同的数值，不同的输入一般会转换为不同的数值，也可能转换为相同数值（也被称为冲突）。请参阅第203页的根据键的散列值分隔。 幂等（idempotent）用于描述一种操作可以安全地重试执行，即执行多次的效果和执行一次的效果相同。请参阅第478页的幂等。 索引（index）一种数据结构。通过索引，你可以根据特定字段的值，在所有数据记录中进行高效检索。请参阅第70页的让数据库更强大的数据结构。 隔离性（isolation）在事务上下文中，用于描述并发执行事务的互相干扰程度。串行运行具有最强的隔离性，不过其它程度的隔离也通常被使用。请参阅第225页的隔离。 连接（join）汇集有共同点的记录。在一个记录与另一个记录有关（外键，文档参考，图中的边）的情况下最常用，查询需要获取参考所指向的记录。请参阅第33页上的多对一和多对多关系和第393页上的减少端连接和分组。 领导者（leader）当数据或服务被复制到多个节点时，由领导者分发已授权变更的数据副本。领导者可以通过某些协议选举产生，也可以由管理者手动选择。也被称为主人。请参阅第152页的领导者和追随者。 线性化（linearizable）表现为系统中只有一份通过原子操作更新的数据副本。请参阅第324页的线性化。 局部性（locality）一种性能优化方式，如果经常在相同的时间请求一些离散数据，把这些数据放到一个位置。请参阅第41页的请求数据的局部性。 锁（lock）一种保证只有一个线程、节点或事务可以访问的机制，如果其它线程、节点或事务想访问相同元素，则必须等待锁被释放。请参阅第257页的两段锁（2PL）和301页的领导者和锁。 日志（log）日志是一个只能以追加方式写入的文件，用于存放数据。预写式日志用于在存储引擎崩溃时恢复数据（请参阅第82页的使二叉树更稳定）；结构化日志存储引擎使用日志作为它的主要存储格式（请参阅第76页的有序字符串表和日志结构的合并树）；复制型日志用于把写入从领导者复制到追随者（请参阅第152页的领导者和追随者）；事件性日志可以表现为数据流（请参阅第446页的分段日志）。 物化（materialize）急切地计算并写出结果，而不是在请求时计算。请参阅第101页的聚合：数据立方和物化视图和419页的中间状态的物化。 节点（node）计算机上运行的一些软件的实例，通过网络与其他节点通信以完成某项任务。 规范化（normalized）以没有冗余或重复的方式进行结构化。 在规范化数据库中，当某些数据发生变化时，您只需要在一个地方进行更改，而不是在许多不同的地方复制很多次。 请参阅第33页上的多对一和多对多关系。 OLAP（Online Analytic Processing）在线分析处理。 通过对大量记录进行聚合（例如，计数，总和，平均）来表征的访问模式。 请参阅第90页上的交易处理或分析？。 OLTP（Online Transaction Processing）在线事务处理。 访问模式的特点是快速查询，读取或写入少量记录，这些记录通常通过键索引。 请参阅第90页上的交易处理或分析？。 分区（partitioning）将单机上的大型数据集或计算结果拆分为较小部分，并将其分布到多台机器上。 也称为分片。 见第6章。 百分位点（percentile）通过计算有多少值高于或低于某个阈值来衡量值分布的方法。 例如，某个时间段的第95个百分位响应时间是时间t，则该时间段中，95％的请求完成时间小于t，5％的请求完成时间要比t长。 请参阅第13页上的描述性能。 主键（primary key）唯一标识记录的值（通常是数字或字符串）。 在许多应用程序中，主键由系统在创建记录时生成（例如，按顺序或随机）; 它们通常不由用户设置。 另请参阅二级索引。 法定人数（quorum）在操作完成之前，需要对操作进行投票的最少节点数量。 请参阅第179页上的读和写的法定人数。 再平衡（rebalance）将数据或服务从一个节点移动到另一个节点以实现负载均衡。 请参阅第209页上的再平衡分区。 复制（replication）在几个节点（副本）上保留相同数据的副本，以便在某些节点无法访问时，数据仍可访问。请参阅第5章。 模式（schema）一些数据结构的描述，包括其字段和数据类型。 可以在数据生命周期的不同点检查某些数据是否符合模式（请参阅第39页上的文档模型中的模式灵活性），模式可以随时间变化（请参阅第4章）。 次级索引（secondary index）与主要数据存储器一起维护的附加数据结构，使您可以高效地搜索与某种条件相匹配的记录。 请参阅第85页上的其他索引结构和第206页上的分区和二级索引。 可序列化（serializable）保证多个并发事务同时执行时，它们的行为与按顺序逐个执行事务相同。 请参阅第251页上的可序列化。 无共享（shared-nothing）与共享内存或共享磁盘架构相比，独立节点（每个节点都有自己的CPU，内存和磁盘）通过传统网络连接。 见第二部分的介绍。 偏斜（skew） 各分区负载不平衡，例如某些分区有大量请求或数据，而其他分区则少得多。也被称为热点。请参阅第205页上的工作负载偏斜和减轻热点和第407页上的处理偏斜。 时间线异常导致事件以不期望的顺序出现。 请参阅第237页上的快照隔离和可重复读取中的关于读取偏斜的讨论，第246页上的写入偏斜和模糊中的写入偏斜以及第291页上的订购事件的时间戳中的时钟偏斜。 脑裂（split brain）两个节点同时认为自己是领导者的情况，这种情况可能违反系统担保。 请参阅第156页的处理节点中断和第300页的真相由多数定义。 存储过程（stored procdure）一种对事务逻辑进行编码的方式，它可以完全在数据库服务器上执行，事务执行期间无需与客户端通信。 请参阅第252页的实际串行执行。 流处理（stream process）持续运行的计算。可以持续接收事件流作为输入，并得出一些输出。 见第11章。 同步（synchronous）异步的反义词。 记录系统（system of record）一个保存主要权威版本数据的系统，也被称为真相的来源。首先在这里写入数据变更，其他数据集可以从记录系统派生。 参见第三部分的介绍。 超时（timeout）检测故障的最简单方法之一，即在一段时间内观察是否缺乏响应。 但是，不可能知道超时是由于远程节点的问题还是网络中的问题造成的。 请参阅第281页上的超时和无限延迟。 全序（total order）一种比较事物的方法（例如时间戳），可以让您总是说出两件事中哪一件更大，哪件更小。 总的来说，有些东西是无法比拟的（不能说哪个更大或更小）的顺序称为偏序。 请参见第341页的因果顺序不是全序。 事务（transaction）为了简化错误处理和并发问题，将几个读写操作分组到一个逻辑单元中。 见第7章。 两阶段提交（2PC, two-phase commit）一种确保多个数据库节点全部提交或全部中止事务的算法。 请参阅第354页上的原子提交和两阶段提交（2PC）。 两阶段锁定（2PL, two-phase locking）一种用于实现可序列化隔离的算法，该算法通过事务获取对其读取或写入的所有数据的锁，直到事务结束。 请参阅第257页上的两阶段锁定（2PL）。 无边界（unbounded）没有任何已知的上限或大小。 反义词是边界（bounded）。 参考设计数据密集型应用 - 中文翻译ddia-references]]></content>
      <categories>
        <category>CS</category>
      </categories>
      <tags>
        <tag>Book</tag>
        <tag>Distrubuted-System</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记小时候一些事情]]></title>
    <url>%2F2018%2F07%2F27%2F%E8%AE%B0%E5%B0%8F%E6%97%B6%E5%80%99%E4%B8%80%E4%BA%9B%E4%BA%8B%E6%83%85%2F</url>
    <content type="text"><![CDATA[我喜欢当别人的聆听者，也喜欢讲故事和观点给别人听，如果很久很久没有这样做，我大概是已经疯掉了吧。以下都是自己的碎碎念，不必在意。 小学的时候，迷上了一款游戏（当然，其实玩过很多游戏），那时候还把两个玩伴带到坑里来。这一起一玩就是5年，后来我偶尔还会上线过，如今上了大学仍然不免感慨，那时候这连串的记忆，对如今的我有多大影响——至少我现在回忆起来总是觉得许多地方感觉非常暖和。不过我今天重点不是记游戏的事情（以后有机会，可以写下），主要是那时候一些生活上的许多事情（可以说是被游戏影响着的），一直藏在心里，从来没有分享给别人。 现在想起来，那时候我也是够搞笑的，我父母管我也特别严格。也不能说是严格，而是颇为费解。我家里4年级的暑假买了电脑，不过后来玩的太过火，被拔网线电源了。喔，那个时候的家长好像管理所谓的网瘾少年，就是使用这种流行的方法。那时候上网还是拨号的，经过几年我才醒悟过来，我应该自己备一个适配的网线电源。不过还是由于玩的太过火被发现了，又被没收了。 后来周末能够玩的时候就非常少了，于是我开始去朋友家玩。我爸妈的工作是晚睡晚起的，那时是早上12点起，凌晨2，3点睡觉的模式（以前我觉得很自然，现在上了大学反而觉得当年父母的生活习惯竟然如此不可思议）。于是我早上6点便起床跑到500米远的朋友家去玩了。那时我们都迷恋同一个网游。那时候对游戏的深度讨论丝毫不亚于如今学计算机的一些讨论，可能我们天生就有这样的专注，只不过需要先找到迷人的事物。 不过，没过多久，我妈就发现了，她其实早上会有起床的习惯。于是，她早上7点便来到朋友家楼下，嘶吼着我的名字。我听到了又害怕又尴尬，我只好下楼，任凭我妈对我的拉扯和训斥，直到我躺到自己的床上才作罢。有趣的不是这件事，而是这件事一直持续了大概一年。每次周末，我早上都是如此度过的。像极了叛逆的孩子（大概就是叛逆的孩子吧），跟父母作对。但我的关注点不在这里，我的脑子里全是游戏，虽然不妨碍我上学期间的专注，但是一到周末，我的心都飞到朋友家。 说起来，天刚亮就偷跑出去，这种事情在我没有电脑前也做过无数次了。有跑到野地看日出过，也有专门观察黎明时刻冷清的街道如何变的热闹的。路边的早餐店，卖鸡蛋饼的女人是如何工作的。当然，做的最多的是去游戏厅。不可思议，那时候的游戏厅居然有6点就开的（虽然小学以后再也没有去过游戏厅了）。我进去就是玩《棒球小子》，等人多了，我去玩《拳皇97》或者《拳皇2000》。 上了初中，玩游戏就规律多了。平时住宿，回家只能周五和周六晚上玩。那时候一天就只能玩4小时了，现在如果要做一件事，4小时实在太少了，以前却可以玩的很开心。上高中之前，没有电脑的我是跟电视过的，我会把每个频道翻过去，找到喜欢看的节目。到了高中，有了手机，开始整体刷qq了。电视已经很少看了，电影也慢慢旧了，之前也修过很多次，最后是半可用状态，不过那时候我已经有养成追几部热血漫的习惯了，所以基本作为电视看了。电视还真的是具有非常特殊的意义——电脑不是，电脑总是在身旁，大概反而不给人怀旧感。]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Life</tag>
        <tag>Childhood</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的五月反思]]></title>
    <url>%2F2018%2F05%2F06%2F2018%E5%B9%B45%E6%9C%88%E5%8F%8D%E6%80%9D%2F</url>
    <content type="text"><![CDATA[懒惰的自己与混沌的事情首先恭喜自己美赛拿了M奖。然后接下来的话都是对这一个多月的反思。 读书之前提到的要看的几本书。 《C++性能优化指南》随便看了几章后就还回去了，暂时用不到，可能需要工作2，3年才能有较深刻的体会，不然现在看都是一些大道理罢了，虽然知道这么做非常多，但是因为没有这些编码经验导致印象非常模糊，无法自发地去利用这些知识。 《Effective Modern C++》这本书一直在看，已经看了半本左右吧。我读的是英文版，而且是乱序读的，对哪个 Item 感兴趣就读哪个。这本书基本是对 C++11 C++14 的新特性总结与避坑。这些新特性有一半我已经接触过了，所以有些 Item 看起来还好，只需要注意几个细节。而有些特性平时使用不到，理解不够深刻，读起来也是懵懂的。 《深入理解JVM》这本书是几乎全部快速扫过一遍了。可惜啊，现在很多记不清了，只有一些感性的认知记忆了。毕竟许多知识就是需要多遍的理解并且实践才能获得真正的深刻。 《Scala函数式编程》，这本难度还是超出了我的预期。它把太多的内容交给了读者自己去编写，而且我觉得它里面的部分内容组织的有一定问题，后面也全然不提 Scala 的那些语法，以至于得自己先去了解那些语法。目前为止我粗略读了半本，但是习题才写到第五章。 《算法新解》，惭愧。后来就几乎没有看了，它偏重于函数式，所以可以说跟本科要求的数据结构已经不怎么搭了。而且这本书大多数内容引用了一个博士论文——专门研究函数式数据结构的论文，可见这本书其实没有作者所说的非常基础。 除了以上几本计划内的书，其实还有好多本计划外的书，比如《七周七语言》，非常好的书，不过难度也是比较大的，我主要是把它当做扩展自己的视野的科普书来看的。之前的 Lua 那篇博客就是整理自该书，遗憾的是后面的那些内容太庞杂了，没有精力继续整理。还有个比较重要的是 《Programming in Scala, Third Edition : A comprehensive step-by-step guide》，这是 Scala 之父的作品。我直接读英文版，已经接近半本了。实话说，我觉得我还是没怎么掌握——我真的编码太少了。这本书真的挺厚的，堪比《C++ Primer》，Scala 本身复杂度也是不亚于 C++ 的。 其实对一些书的总结挺没有意思的，并不能真正看出自己学到多少。而且想看的书一直在增多和变化着，感觉变得越来越乏力。 复习前几天终于开始复习高数了，不过非常不顺利，甚至没有找到学习高数的感觉。讲义内的一些题目非常灵活、综合，连选择题的解析也把各个选项当证明题来做。这时候，基础的知识和定义需要理解的清楚而深刻。不幸，我一直不擅长记忆，一连串的定义要精确的记忆理解压垮了我的脑袋。于是，在图书馆按捺不住，没隔多久就打道回府了。 我发现了我缺少了克服一些客观问题和主观问题的意志力，也缺少把计划铭记于心然后执行的习惯。我计划稍后在纸上写下学习计划表，精确到小时，并且随身携带着。有些坏习惯或好习惯必须得靠主观意志来去去除或者养成。 方向就在刚才，我查了许多关于 PLT 的资料，几乎全是英文的，而且许多大量的前置知识。PLT 是指 Programming Language Thesis，这个国内真的很少。全是非常好的学校才有那么一点点人搞这个方向。基本是偏数学（较强的数理逻辑知识是基础）的方向了，这样看来，我现在几乎是零基础（大概学习了一些函数式编程是我在这方向的唯一的基础了），而国内顶尖高校高年级是有开相关的课程的。其实我也间歇性地搜集过 PLT 相关的资料，涉及面太庞大了，PLT 其实本身非常庞大，纳闷的是为什么国内搞这个方向的人这么少。知乎可能是我目前唯一能搜到许多理想的 PLT 相关资料的地方了。入门可能是最麻烦的了，看那些搞 PLT 人在知乎的答案，总是一头雾水。 我想我很可能就是要往这方向发展了（抛开目前的水平不谈，我个人意向就是想学习跟编程语言本身强相关的方向），所以才打算考研，搞这个方向的学校也真是屈指可数。 以我目前的情况嘛，大概主要时间放在考研复习上了，平时偶尔看看基础的 PLT 知识，主要是深入学习 Haskell 吧， Haskell 深入了，大概许多 PLT 的方向也能慢慢摸索出来了。 态度 最近在生活方面和学习方面都比较消极，没有了大一大二时候那种学习热情和乐趣。书本也堆积成山，可能再也看不完了。 这个星期每天睡觉也做梦，而且是一连串的梦，经常是噩梦。这大概是对我消极的生活状态的警示吧。 除此之外，常常把过多时间投入于思考一些无意义的事物中，等反应过来，总是有种空虚感。 最近记忆力感觉又变差了，难得觉得我的英语听力感觉有些进步了。 周围的诱惑太多了，也觉得自己愈发平凡，觉得我有点想谈恋爱了，因为如今的生活毫无新鲜感。 有时候要借助外力推动自己，我需要找到一些人来对自己的生活起到很好的约束与推动。]]></content>
      <categories>
        <category>Life</category>
        <category>CS</category>
      </categories>
      <tags>
        <tag>Graduate</tag>
        <tag>Book</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[七周七语言2-Lua]]></title>
    <url>%2F2018%2F04%2F20%2F%E4%B8%83%E5%91%A8%E4%B8%83%E8%AF%AD%E8%A8%802-lua%2F</url>
    <content type="text"><![CDATA[LuaLua一览Lua 是一门基于 table 的编程语言，它的核心抽象层简单而强大，你可以用它实现过程式、面向对象、事件驱动等。Lua 的 table 很适合基于原型的面向对象编程。这种编程风格中，类和实例不是割裂的概念。你先创建一个实例，这个实例看起来像你需要的对象。然后你再把这个实例复制多份，对每一份做一些定制化。 初窥Lua 的语法友好，并且易于接近，无须担心分号或者空格写哪里。空格对 Lua 来说不太重要，两个语句之间甚至不需要换行。Lua 使用 – 来注释。 以下示例主要在REPL中运行。 12345&gt; print "hello world"hello world&gt; print"hello world"&gt; print "hello world" print "hello world" Lua 基础Lua 是动态类型的，也就是说代码中的变量不需要声明类型，在运行时才会决定其具体类型。Lua 的基础类型有： 数字 布尔值 字符串 1234567&gt; 3.143.14&gt; truetrue&gt; "hello world"hello world&gt; Lua 没有整数，就跟 JS 一样。字符串也可以使用单引号，也使用反斜杠转义特殊字符或不可见字符。 字符串可以用 .. 来拼接，用 # 来获取长度。 12345&gt; "hello\t" .. "world"hello world&gt; # "hello"5 在 Lua 中，nil 有自己的类型，它表示找不到或者不存在。 12&gt; some_funcnil 表达式Lua 里的数学表达式和其他语言差不多。 123456&gt; 6 + 5 * 4 - 3 / 224.5&gt; 6 + (5 * 4) - (3 / 2)24.5&gt; (6 + 5) * (4 - 3) / 25.5 Lua 的布尔操作是用 and or not 三个关键字。Lua 的逻辑表达式也是会短路的。Lua 里布尔操作跟JS里的类似。== ~= 用来比较任意值相等或者不相等。&lt; &gt; &lt;= &gt;= 只对字符串和数字适用。 12345678910111213141516171819&gt; true or 1true&gt; true and 11&gt; not 1false&gt; not 0false&gt; not falsetrue&gt; "hello" == "world"false&gt; "hello" ~= "world"true&gt; "hello" &lt;= "world"true&gt; "hello" &gt; "world"false 函数Lua 的函数定义看起来和其他常见脚本语言类似，如JS. 123456function triple(num) return 3 * numend-- 函数名字不是必须的(function(num) return 3 * num end)(2) 在 Lua 的世界里，函数是一等公民。可以跟其他值一样用于赋值和传递参数。 灵活的参数调用函数的时候，传入的参数个数不一致时，未传入的参数赋值为nil，多余的参数则被忽略掉。 123456789101112131415&gt; function print_three(a, b, c)&gt;&gt; print(a)&gt;&gt; print(b)&gt;&gt; print(c)&gt;&gt; end&gt; print_three(1, 2)12nil&gt; print_three(1, 2, 3, 4)123 可以把最后一个参数标为 ... 来表示不定参数。 123456789101112131415161718192021&gt; function print_char(a, ...)&gt;&gt; print(a)&gt;&gt; b = &#123;...&#125;&gt;&gt; print(b[1])&gt;&gt; print(b[2])&gt;&gt; end&gt; print_char("hello")hellonilnil&gt; print_char("hello", "world")hellownil&gt; print_char("hello", "world", "你好")helloworld你好 Lua 的函数会做尾递归优化。和参数类似，函数的返回值可以有多个，你可以选择使用所有返回值，或者忽略一些。 123456789101112131415161718192021&gt; function return_two()&gt;&gt; return "hello", "world"&gt;&gt; end&gt;&gt; a = return_two()&gt;&gt; x, y = return_two()&gt; ahello&gt; xhello&gt; yworld&gt; a,b,c = return_two()&gt; ahello&gt; bworld&gt; cnil 具名参数Lua 不像 python 和 ruby 在语法层面上支持具名函数。不过你可以通过把 table 当作参数来获得类似的效果。 123456&gt; function print_prices(table)&gt;&gt; print("The clothes costs " .. table.medium)&gt;&gt; end&gt;&gt; print_prices&#123;small=5.0, medium=7.0&#125;The clothes costs 7.0 控制流程Lua 内建的流程控制有 if 语句，两种 for 循环和 while 循环。 123456789101112131415161718192021222324&gt; score = 7&gt;&gt; if score == 10 then&gt;&gt; print "good"&gt;&gt; elseif score &gt;= 6 then&gt;&gt; print "not bad"&gt;&gt; else&gt;&gt; print "terrible"&gt;&gt; endnot bad&gt; for i = 1, 3 do&gt;&gt; print(i)&gt;&gt; end123&gt;&gt; for i = 1, 5, 2 do&gt;&gt; print(i)&gt;&gt; end135 还可以用 for 循环来遍历集合，之后会讲。最后一种是 while 循环。 1234&gt; while math.random(100) &lt; 50 do&gt;&gt; print("again")&gt;&gt; endagain 变量Lua 的变量默认是全局的，需要使用 local 关键字来给局部变量声明。 1234567891011121314&gt; function doubleMe(a)&gt;&gt; a2 = a * 2&gt;&gt; return a2&gt;&gt; end&gt;&gt; doubleMe(2)4&gt; a24&gt; function doubleMe(a)&gt;&gt; local a2 = a * 2&gt;&gt; return a2&gt;&gt; end table 作为字典和 JS 的 Object， Ruby 的哈希类似，Lua 的 table 是键值对的集合。通过大括号创建， 这种表达式叫做 table 构造器。（最后一个逗号可以省略） 123456789101112131415161718192021-- create&gt; book = &#123;&gt;&gt; title = "My story",&gt;&gt; auther = "yjh",&gt;&gt; pages = 100&gt;&gt; &#125;&gt;&gt; booktable: 0276e1e0-- get&gt; book["title"]My story-- add&gt; book.stars = 5&gt; book[0] = 1&gt; book[0]1-- delete&gt; book.stars = nil&gt; book.starsnil 你可以使用任何类型的数据作为键：布尔，函数，table。 Lua 并没有自带打印 table 的函数，需要自定义。 1234567891011&gt; function print_table(t)&gt;&gt; for k, v in pairs(t) do&gt;&gt; print(k .. ": " .. v)&gt;&gt; end&gt;&gt; end&gt;&gt; print_table(book)auther: yjhtitle: My storypages: 1000: 1 pairs() 是内建函数，它是一个迭代器，可以与循环工作。 穿着数组外衣的字典对于 Lua 来说，数组只是键值对存储结构的一个特例，键是连续的数字。 123456789medals = &#123; "gold", "silver", "bronze"&#125;&gt; medals[1]gold&gt; medals[4] = "lead" Lua 运行时给数组提供了特殊的快车道，只要你连续用数字作为键向字典添加数据，Lua 就能高效地存储和访问数据。 metatables目前为止的table，如果你给定键，Lua给你找到一个值。这个逻辑是 Lua 内建的。有时候这种默认行为不是程序需要的。你可能需要返回一个默认值而不是nil；或者想把读写的历史记录到某个 table。这些可以通过 metatables 实现。 如果你熟悉 JS 的原型或者 Python 的双下划线方法名，你会发现 Lua 的方式也很熟悉。Lua 的每个 table 都有一个 metatable，其中可以包含读写键值对的函数，可以包含用来遍历 table 内容的代码，也可以重载某些操作符。 12345678&gt; greek_numbers = &#123;&gt;&gt; ena = "one",&gt;&gt; dyo = "two",&gt;&gt; tria = "three"&gt;&gt; &#125;&gt;&gt; getmetatable(greek_numbers)nil 1234567function table_to_string(t) local result = &#123;&#125; for k, v in pairs(t) do result[#result + 1] = k .. ": " .. v end return table.concat(result, "\n")end 123456789101112&gt; mt = &#123;&gt;&gt; __tostring = table_to_string&gt;&gt; &#125;&gt;&gt; setmetatable(greek_numbers, mt)dyo: twotria: threeena: one&gt; greek_numbersdyo: twotria: threeena: one 这样我们实现了自定义table输出为字符串的行为。 读和写Lua 的 table 非常宽容，读取不存在的键只会得到一个 nil。如果需要更加严格的 table，读取不存在的键或者覆盖已经存在的键都会导致运行时错误。要做到这件事，只需要： 把你想要自定义的读写逻辑写到两个函数里。 把这两个函数存储到一个 table 中， 分别为 __index 和 __newindex。 把上一步创建的 table 设置为你的数据 metatable。 下面依次进行这三步。 1234567891011121314151617181920212223242526local _private = &#123;&#125;function strict_read(table, key) if _private[key] then return _private[key] else error("Invalid key: " .. key) endendfunction strict_write(table, key, value) if _private[key] then error("Duplicate key: " .. key) else _private[key] = value endendlocal mt = &#123; __index = strict_read, __newindex = strict_write&#125;treasure = &#123;&#125;setmetatable(treasure, mt) 现在你可以使用这个新自定义的字典了。 12345678&gt; treasure.gold = 50&gt; treasure.gold50&gt; treasure.silvernil&gt; treasure.gold = 100&gt; treasure.gold100 除此之外，lua 还有其他特殊的键名，这种语法跟python非常类似。 自制面向对象系统Lua 有自己的面向对象语法。不过得益于 lua 强大的抽象，可以容易地定义另一种面向对象风格，然后可以观察它们的区别。面向对象的核心观念就是对象之间互相发送消息。下面是一个玩家打 Boss 的代码的例子。 123456789101112dietrich = &#123; name = "Dietrich", health = 100, take_hit = function (self) self.health = self.health - 10 end&#125;&gt; dietrich.take_hit(dietrich)&gt; dietrich.health90 Boss 可能不止一个，如果 Boss 共用一份 take_init()，我需要知道让哪个 Boss 掉血，这就是传入 self 参数的目的。 原型123456789101112131415161718192021222324Villain = &#123; health = 100, new = function(self, name) local obj = &#123; name = name, health = self.health &#125; setmetatable(obj, self) self.__index = self return obj end, take_hit = function(self) self.health = self.health - 10 end&#125;&gt; yjh = Villain.new(Villain, "yjh")&gt; yjh.health100&gt; Villain.take_hit(yjh)&gt; yjh.health90 setmetable 把 obj 委托给了 self，这个例子中是 Villain，也是说把 Villain 作为查找的后备（这里原型继承应该跟 JS 类似，是原型链）。 继承如上所说，基于原型的面向对象系统的一个好处是不需要特殊的机制来实现继承，可以像之前那样定制，复制对象就可以了。如果你需要创建一个 Final Boss，你只需要创建一个新的原型，然后复制它就好了。 1234567891011-- 定制新的 Boss 类SuperVillain = Villain.new(Villain)function SuperVillain.take_hit(self) self.health = self.health - 5end-- 实例化&gt; boss = SuperVillain.new(SuperVillain, "Final Boss")&gt; boss.take_hit(boss)&gt; boss.health95 语法糖如果你写的是 table:method() 而不是 table.method(self), Lua 会隐式传入 self 参数。 123456789101112131415161718192021222324-- same as beforeVillain = &#123; health = 100&#125;function Villain:new(name) local obj = &#123; name = name, health = self.health &#125; setmetatable(obj, self) self.__index = self return obj endfunction Villain:take_hit() self.health = self.health - 10 end&gt; yjh = Villain:new("yjh")&gt; yjh:take_hit()&gt; yjh.health90 协程之前的 Lua 代码都是串行执行的。那么 Lua 如何处理多线程？ Lua 不处理多线程。但是 Lua 内建了简单、容易理解的用于多任务处理的基本类型：协程。与线程不同，协程不是抢占式的，你需要显式标注哪里暂停任务，让位给其他任务。它在概念上更简单，可以免去许多并行的问题。 下面定义一个死循环。 12345678910111213141516171819function fibonacii() local m = 1 local n = 1 while true do coroutine.yield(m) m, n = n , m + n endend&gt; gen = coroutine.create(fibonacii)&gt; succeeded, value = coroutine.resume(gen)&gt; value1&gt; succeeded, value = coroutine.resume(gen)&gt; value1&gt; succeeded, value = coroutine.resume(gen)&gt; value2 要运行协程需要先用 coroutine.create 函数创建一个协程，然后调用 couroutine.resume。调用 resume 后就执行该函数直到遇见下一个 yield 调用就跳回调用者处，并且返回一个状态码和 yield 的参数。这种方式适合耗时长的计算或者网络操作，可以把它分解为较小的子任务来执行，维持程序的响应性。 多任务协程虽然简单但功能强大，可以实现类似多线程的行为。操作系统的进程调度器通常需要几千行代码，不过我们可以使用几十行来实现一个。 先定义 scheduler.lua 。注意： 局部函数必须先定义后使用。 1234567891011121314local function sort_by_time(array) table.sort(array, function(e1, e2) return e1.time &lt; e2.time end)endlocal pending = &#123;&#125;local function schedule(time, action) pending [#pending + 1] = &#123; time = time, action = action &#125; sort_by_time(pending)end 想让某件事在未来发生的话，把它丢到 pending 数组里，该数组以时间戳排序（程序启动到现在的秒数）。 1234local function wait(seconds) coroutine.yield(seconds) -- seconds will be sent to coroutine.resume's return.end 协程一个是轻量级的。在开始运行（run）以后后就做相应的处理，做完马上返回或者 yield。所以 wait 函数不应该真的等待，而是向调度器 yield。 123456789101112131415161718192021222324local function remove_first(array) result = array[1] array [1] = array[#array] array[#array] = nil return resultendlocal function run() -- when there is not task, the scheduler will quit. while #pending &gt; 0 do while os.clock() &lt; pending[1].time do end -- busy-wait local item = remove_first(pending) local _, seconds = coroutine.resume(item.action) -- got seconds from coroutine.yield. if seconds then later = os.clock() + seconds -- item's next action time -- `now` plus `schedule time` -- print('later:' .. later) schedule(later, item.action) end endend run 会在没有任务时等待（任务时间还未到达），在有任务时执行任务。如果执行的任务调度 wait, 任务执行所需要消耗的秒数就会被 yield 给我们。我们会用这个秒数决定未来什么时候继续执行当前的任务（因为协程是非抢占的，所以调度以后，可能其他任务花费 wait 给定时间的数倍才让回来，这里需要程序员自己来设计好调度时机）。 当任务执行完毕后，协程不会 yield 任何东西。 这时候 resume 函数会返回 nil，我们也不会给该任务分配任何时间了。 最后我们把这个 API 封装成一个模块。下面代码仍然写到 scheduler.lua里。12345return &#123; schedule = schedule, run = run, wait = wait&#125; 现在可以执行一个新文件了。1234567891011121314151617181920-- main.luascheduler = require 'scheduler'function punch() for i = 1, 5 do print('punch ' .. i) scheduler.wait(0.3) endendfunction block() for i = 1, 3 do print('block ' .. i) scheduler.wait(1.0) endendscheduler.schedule(0, coroutine.create(punch))scheduler.schedule(0, coroutine.create(block))scheduler.run() 我们使用 require 函数来加载模块： 检查模块是否已经加载过。 搜索多个程序库的加载路径（可以配置）。 给局部变量提供安全的命名空间。 123456789$ ./lua53.exe main.luapunch 1block 1punch 2punch 3punch 4block 2punch 5block 3 总结我们实现了一个面向对象系统以及一个类似线程的并行 API，并且这些代码写得可读、紧凑、模块化。让这一切容易实现得益于 Lua 易于组合的数据结构：table 和 协程。table 可以作为数组和字典使用，lua 还给我们提供了切入点来扩展 table 的行为。之后我们学习了协程，这是 Lua 并行的方式。尽管协程 API 不多，但是仍然可以构造复杂强大的多任务系统。]]></content>
      <categories>
        <category>CS</category>
      </categories>
      <tags>
        <tag>Book</tag>
        <tag>Lua</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop Install]]></title>
    <url>%2F2018%2F04%2F20%2FHadoop-Install%2F</url>
    <content type="text"><![CDATA[Hadoop Install我采用的是 linux 下的 hadoop 安装，本安装文档说明前提是已经全局安装了open-jdk1.8(也可以使用 hotspot jdk)。本文档主要参考官方文档。部分英文由自己补充和改写，主要内容翻译为中文。 Required software for Linux include: Java™ must be installed. Recommended Java versions are described at HadoopJavaVersions. ssh must be installed and sshd must be running to use the Hadoop scripts that manage remote Hadoop daemons. Installing Software以下是必需的软件。 12$ sudo apt-get install ssh$ sudo apt-get install rsync DownloadTo get a Hadoop distribution, download a recent stable release from one of the Apache Download Mirrors. For global using, move the hadoop software to ~/software.Before start hadoop, we should check the etc/profile and ~/.bashrc coded with the following code.These are my personal config about Java and basic hadoop config.For hadoop, HADOOP_HOME must to be set firstly. 我使用的是 hadoop2.7.5 版本。到官网下载后，解压，并把主目录移动到了~/software下，并且改名为了hadoop。然后需要配置一些最基本的环境变量，etc/profile，~/.bashrc配置一些全局变量，我这里包括java的基本配置。如果与读者不一致，后续操作需要做相应的修改(官方后续许多跟hadoop相关的操作是在 HADOOP_HOME 下的，我把命令全部放入到PATH中，后续操作更方便)。 etc/profile: 1234export JAVA_HOME=/usr/lib/jdk1.8export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib export PATH=$PATH:$&#123;JAVA_HOME&#125;/bin ~/.bashrc: 123export HADOOP_HOME=~/software/hadoopexport PATH=$PATH:$HADOOP_HOME/binexport PATH=$PATH:$HADOOP_HOME/sbin 想让环境配置立即刷新需要使用下面的命令。 12source /etc/profilesource ~/.bashrc Standalone OperationBy default, Hadoop is configured to run in a non-distributed mode, as a single Java process. This is useful for debugging. The following example copies the unpacked conf directory to use as input and then finds and displays every match of the given regular expression. Output is written to the given output directory. hadoop 默认为单击模式，这对 debugging 比较友好。这里复制一些文件到input，然后跑一个Wordcount程序读取input，输出到output。 1234$ mkdir input$ cp $HADOOP_HOME/etc/hadoop/*.xml input$ hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.5.jar grep input output &apos;dfs[a-z.]+&apos;$ cat output/* Pseudo-Distributed OperationHadoop can also be run on a single-node in a pseudo-distributed mode where each Hadoop daemon runs in a separate Java process. 通过利用多个java进程，hadoop 也可以在单个节点上运行伪分布式模式。 Use the following: 伪分布式需要先配置。 $HADOOP_HOME/etc/hadoop/core-site.xml: 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; $HADOOP_HOME/etc/hadoop/hdfs-site.xml: 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; Setup passphraseless sshNow check that you can ssh to the localhost without a passphrase: 先检测下是否能用密钥访问 localhost。 1$ ssh localhost If you cannot ssh to localhost without a passphrase, execute the following commands: 如果你无法通过密钥来访问 localhost，执行下面的命令。 123$ ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys$ chmod 0600 ~/.ssh/authorized_keys ExecutionThe following instructions are to run a MapReduce job locally. 以下的说明针对的是本地任务。 Format the filesystem: 第一次使用该文件系统需要初始化。 1$ hdfs namenode -format Start NameNode daemon and DataNode daemon: 用该命令启动节点。 1$ start-dfs.sh The hadoop daemon log output is written to the $HADOOP_LOG_DIR directory (defaults to $HADOOP_HOME/logs). 我们这里没有设置$HADOOP_LOG_DIR，使用默认的就好。 Browse the web interface for the NameNode; by default it is available at: 成功启动后，可以访问 Web 界面 http://localhost:50070 查看 NameNode 和 Datanode 信息，还可以在线查看 HDFS 中的文件。通过 50070 这个默认端口用浏览器浏览名字节点。 1NameNode - http://localhost:50070/ Make the HDFS directories required to execute MapReduce jobs: 创建 DFS 文件来提供执行 maoreduce 任务。 12$ hdfs dfs -mkdir /user$ hdfs dfs -mkdir /user/&lt;username&gt; Copy the input files into the distributed filesystem: 把本地需要测试的文件复制到 dfs 里的文件系统中。 1$ hdfs dfs -put $HADOOP_HOME/etc/hadoop input Run some of the examples provided: 运行一个官方实例。可以看到已经运行了WordCount。 1$ hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.5.jar grep input output &apos;dfs[a-z.]+&apos; Examine the output files: Copy the output files from the distributed filesystem to the local filesystem and examine them: 从 hadoop 中的 dfs 的复制 output 文件到本地一份。 12$ hdfs dfs -get output output$ cat output/* When you’re done, stop the daemons with: 当你工作结束后，使用下面命令关闭。 1$ stop-dfs.sh Reference Hadoop: Setting up a Single Node Cluster. ssh：connect to host localhost port 22: Connection refused Hadoop安装教程_单机/伪分布式配置]]></content>
      <categories>
        <category>CS</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018学习和复习]]></title>
    <url>%2F2018%2F03%2F20%2F2018%E5%AD%A6%E4%B9%A0%E5%92%8C%E5%A4%8D%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[2018学习和复习大学做过无数次计划，似乎少有完全达成，这样看来，其实是自己的问题。所以特此再做一次计划，计划不会特别具体，但要让自己严格执行。 学习把学习和复习分开来计划其实是很明智的。复习计划是与考研强相关的，学习计划则是与专业强相关，与考研弱相关的。当然介于考研其实是场持久战，而且如果要考408的话，复习时间其实是很紧的，所以复习期间看考研不相关的东西其实是不太明智的，除非能提早开始准备。当然，我这个人就这样，不喜欢总是做一些课内的或者合时宜的事情，偶尔会在课外捣鼓一些其他东西。这次准备看一些感兴趣的又能对考研的复习有一定作用的书籍。首先是最近已经在看了的，《Scala函数式编程》。因为课内可能需要捣鼓大数据，分布式什么的，这本书可以学习到一些Scala编程，其实有一定帮助。但对考研帮助不大，可能里面涉及到的少部分函数式数据结构，是对复习数据结构有帮助，但是其实应该不明显的。这本书习题蛮多的，后面还特别吃力。所以最近可能会缓一缓，基本上一个月看一章足矣。记得寒假看了点《JS函数式编程》和《Haskell趣学指南》，其实已经学到了不少函数式知识了。Scala这本也基本没超出之前所看的，唯一区别的是这本难度更大，而且有大量习题。第二本是《算法新解》，这本也开始看了，不过没有前者看深刻。这本除了图数据结构以外，基本囊括了数据结构这门课的知识点并且还有许多高级数据结构，除此之外还增加了近一半函数式数据结构和算法。这本应该不会全看完，视情况而定。前期数据结构复习以此为准，毕竟要对自己更高要求嘛。未来几个月打算看的书有《深入理解JVM》，《Effective Modern C++》，《C++性能优化指南》。这几本书只打算在这个学期和暑假的时候看下，3本通读半本内容后，就作为参考了。接下来说说我为什么选这3本。《深入理解JVM》早就想看了，因为以前厌恶Java的原因导致我也不想了解JVM。许多Java实习生，其实应该看这本书，因为对他们面试和工作都有用处。不过对我而已，这本书其实在讲虚拟机的体系结构罢了，它其实就只是在计算机系统结构上又抽象出了一层体系结构。这样说来，了解过JVM的体系结构以后，其实对计算机体系结构甚至操作系统也有比较大的帮助。考虑到考研可能要考组成原理，而组成原理与系统结构其实知识点很相近，所以说这本书对组成原理复习是有一定帮助的。《Effective Modern C++》，这本是在大一学C++的时候就想要读了。然而，那时候功底不够，后来大二的时候接触C++比较少了，而且有对C++一些历史原因和特性的无奈与恐惧，还有英文功底不好等原因，没有看。最近开始拿C++刷题了，又扫过几眼C++的书，觉得对STL掌握更好了，最后是因为Rust，导致我挺想看的。《C++性能优化指南》，这本我其实一点也不了解，单纯看目录，感觉对自己的底层编程有帮助，姑且作为参考吧。 复习身边人都陆陆续续开始复习了，而我课程其实不少，始终只是在记单词和刷点题。当数学复习开始的时候，大概就是正式进入复习状态吧。网上查说数学复习开始时间是3-6月，而专业课复习开始是6-7月。所以我计划5月一定要开始数学复习，而专业目前只看数据结构并且刷题。专业课7月正式开始。其实这样真的蛮紧张的，所以英语复习是每天都不能松懈的，并且争取这次过6级。关于复习资料，目前还没收集全，每周会查阅一些信息来补充复习计划。 关于PAT刷题计划： 3月，前一百题，刷完20分的题。 4月，前一百题，刷完25分的题。 5月，前一百题，刷完30分的题。 暑假，刷完最后的题。 整体上复习大纲就是这样，坚持下来，就会收获。]]></content>
      <categories>
        <category>Life</category>
        <category>CS</category>
      </categories>
      <tags>
        <tag>Graduate</tag>
        <tag>Book</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018新年与考研]]></title>
    <url>%2F2018%2F03%2F07%2F2018%E6%96%B0%E5%B9%B4%E4%B8%8E%E8%80%83%E7%A0%94%2F</url>
    <content type="text"><![CDATA[2018新年与考研新年新的2018年到来了，还没用文章对自己过去一年做一个总结。寒假的时候参加美赛，奈何自己太菜，估计只能获得一个参赛奖吧，好像也没什么好说的。 寒假的后半段基本是自学一些感兴趣的东西，比如Haskell、Rust、Scala，主要是学习函数式编程吧，不过其实也就才刚入门的水平而已。除此之外，还在查阅一些考研相关的资料与信息，因为一直以来都有考研的打算，毕竟GPA不理想，没有机会保研嘛。 现状与打算到了大三下，大部分同学的课表就比较空了，但我可能还是蛮多的，跟大一上的时候差不多吧，不过幸好很多是选修课，不过这些选修课也都挺严格的。而创新实践课老师这个学期给了很多方向，让我们自己选择感兴趣的去学习，只要能真得学到东西就好了。我觉得这样挺不错的，之前的数据可视化我就不太感冒，自己的几何知识真的不太好。虽然现在也是数据可视化相关的，但是细分下来，可以有很多具体的不同方向的知识。比如做数据处理相关的，大数据方向。具体点就是学习分布式处理框架比如hadoop、spark、storm之类。如果硬要说兴趣的话，对这些还是有一点点兴趣的。至于可视化还有一些其他的工作，就交给其他组员做吧。当然我不满足于只是用用这些框架，虽然会应用是很重要，但是我打算学一些分布式系统的知识，这样学习也更深刻。比如CMU和MIT的分布式课，都有公开的，不过好像都是研究生级的课，而且资料不太全，可能很麻烦，这个具体看情况吧。 其实要学的东西还真不少，再加上考研的复习，可能每天都很忙碌。我其实不清楚自己的计划可不可行，但是每天不接触点新知识总是不自在，而复习老知识相对比较枯燥，即使我没有掌握好。 除此之外，在mooc大学平台上，还参加1，2门网络课，跟考研知识比较紧密，但不是为考研而开设的课程，其实也就是CSAPP（我其实基本看完了，课后练习大部分也做了，当然家庭作业都没有做，不过对上个学期操作系统的考试并没有什么帮助。。。）吧，相当于把编译加载，组成原理，操作系统结合的课程，当然不可能讲到那么细，只是学习它们很紧密的地方。这样也能大概巩固操作系统和计组的知识吧。如果我要考浙大的话，就是考408了。需要计网，操作系统，组成原理，数据结构四门核心课的复习。这样能确实有不少的复习作用，而且是从不同角度学习的，因此也有许多新知识，不会枯燥。 数据结构我打算刷PAT，下次考试好像是8月左右，因此时间还蛮充裕的，配套练习有139题，争取全刷了。还有自己准备的一本《算法新解》，比考研难度要高，但是可以作为复习。还有一本《Scala函数式编程》，其实这本跟数据结构和算法关系不大，但是里面少部分还是对数据结构和算法有一定要求和锻炼的。 除了计算机知识，还有英语和数学是这个学期就要开始了，先不考虑政治。数学我打算可能要晚一点再开始，英语现在每天记单词吧，偶尔可能刷刷其他知识点。姑且先这样吧，可能先适应1，2周并且打听打听其他人的学习情况。所以这个月重点在数据结构和英语了吧。 当然还有课内要规划下，重点在分布式系统这块吧。网络编程这门课感觉有点水，打算退了选择云计算，这样也跟创新实践比较紧密些，可以减少负担。还有一门日语课也是重点，虽然跟考研完全无关，也没有帮助。但是既然自己已经选择了，那就好好学习。这门课一周4节，我估计学完了也就N5的水平了吧，听说N3,N4,N5都没什么用。所以路还长吧，N2好像至少要学2年。如果非要说点希望的话，希望自己本科可以把日语学到N3吧，当然英语更不必说了。 姑且先写到这里吧。]]></content>
      <categories>
        <category>Life</category>
        <category>CS</category>
      </categories>
      <tags>
        <tag>Graduate</tag>
        <tag>Book</tag>
        <tag>Life</tag>
        <tag>CS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS函数式编程笔记2]]></title>
    <url>%2F2018%2F02%2F07%2FJS%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B02%2F</url>
    <content type="text"><![CDATA[JS函数式编程笔记里面常常依赖一些库和之前的代码，而且可能存在理论上的代码和未实现的函数，阅读时，需要仔细。 下面部分经常的东西后面经常用到，我先导入。 123456789101112131415161718const curry = require('lodash/curry')const _ = require('lodash')const fp = require('lodash/fp')const moment = require('moment')let compose = function(f, g)&#123; return function(x)&#123; return f(g(x)) &#125;&#125;let toUpperCase = function(str)&#123; return str.toUpperCase()&#125;let toLowerCase = function(str)&#123; return str.toLowerCase()&#125; Hindley-Milner 类型签名初识类型刚接触函数式编程的人很容易深陷类型签名（type signatures）的泥淖。类型（type）是让所有不同背景的人都能高效沟通的元语言。很大程度上，类型签名是以 “Hindley-Milner” 系统写就的。 类型签名在写纯函数时所起的作用非常大，大到英语都不能望其项背。这些签名轻轻诉说着函数最不可告人的秘密。短短一行，就能暴露函数的行为和目的。类型签名还衍生出了 “自由定理（free theorems）” 的概念。因为类型是可以推断的，所以明确的类型签名并不是必要的；不过你完全可以写精确度很高的类型签名，也可以让它们保持通用、抽象。类型签名不但可以用于编译时检测（compile timechecks），还是最好的文档。所以类型签名在函数式编程中扮演着非常重要的角色——重要程度远远超出你的想象。 123456// capitalize :: String -&gt; Stringlet capitalize = function(s)&#123; return toUpperCase(head(s)) + toLowerCase(tail(s))&#125;console.log(capitalize("hello")) 在 Hindley-Milner 系统中，函数都写成类似 a -&gt; b 这个样子，其中 a 和 b是任意类型的变量。因此， capitalize 函数的类型签名可以理解为“一个接受String 返回 String 的函数”。换句话说，它接受一个 String 类型作为输入，并返回一个 String 类型的输出、 PS：Haskell里面HM类型签名作为类型约束语法（type constraints）是很常见的。 1234567891011121314151617181920// strLength :: String -&gt; Numberlet strLength = function(s)&#123; return s.length&#125;// join :: String -&gt; [String] -&gt; Stringlet join = curry(function(what, xs)&#123; xs.join(what)&#125;)// match :: Regex -&gt; String -&gt; [String]let match = curry(function(reg, s)&#123; return s.match(reg)&#125;)// replace :: Regex -&gt; String -&gt; String -&gt; Stringlet replace = curry(function(reg, sub, s)&#123; return s.replace(reg, sub)&#125;)console.log(replace(/world/ig)('JS')('hello world')) strLength 和 capitalize 类似：接受一个 String 然后返回一个Number 。至于其他的，第一眼看起来可能会比较疑惑。不过在还不完全了解细节的情况下，你尽可能把最后一个类型视作返回值。那么 match 函数就可以这么理解：它接受一个 Regex 和一个 String ，返回一个 [String] 。 这里有一个非常有趣的地方，对于 match 函数，我们完全可以把它的类型签名这样分组1234// match :: Regex -&gt; (String -&gt; [String])let match = curry(function(reg, s)&#123; return s.match(reg) &#125;) 是的，把最后两个类型包在括号里就能反映更多的信息了。现在我们可以看出match 这个函数接受一个 Regex 作为参数，返回一个从 String 到 [String] 的函数。因为 curry，造成的结果就是这样：给 match 函数一个Regex ，得到一个新函数，能够处理其 String 参数。当然了，我们并非一定要这么看待这个过程，但这样思考有助于理解为何最后一个类型是返回值。 123// onHoliday :: String -&gt; [String]let onHoliday = match(/holiday/ig)console.log(onHoliday('hello holiday')) 每传一个参数，就会弹出类型签名最前面的那个类型。所以 onHoliday 就是已经有了 Regex 参数的 match 。 PS：我最近刚开始学习Haskell，Haskell里面所有函数都是auto-curried的。配合HM签名效果感觉很强大。 1234// replace :: Regex -&gt; (String -&gt; (String -&gt; String))let replace = curry(function(reg, sub, s)&#123; return s.replace(reg, sub);&#125;) 但是在这段代码中，就像你看到的那样，为 replace 加上这么多括号未免有些多余。所以这里的括号是完全可以省略的，如果我们愿意，可以一次性把所有的参数都传进来；所以，一种更简单的思路是： replace 接受三个参数，分别是 Regex 、 String 和另一个 String ，返回的还是一个 String 。 12345678// id :: a -&gt; alet id = function(x)&#123; return x &#125;// map :: (a -&gt; b) -&gt; [a] -&gt; [b]let map = curry(function(f, xs)&#123; return xs.map(f)&#125;)console.log(map((x) =&gt; x * 2)([1,2,3])) 这里的 id 函数接受任意类型的 a 并返回同一个类型的数据。和普通代码一样，我们也可以在类型签名中使用变量。把变量命名为 a 和 b 只是一种约定俗成的习惯，你可以使用任何你喜欢的名称。对于相同的变量名，其类型也一定相同。这是非常重要的一个原则，所以我们必须重申： a -&gt; b 可以是从任意类型的 a 到任意类型的 b ，但是 a -&gt; a 必须是同一个类型。例如， id 可以是 String -&gt; String ，也可以是 Number -&gt; Number ，但不能是 String -&gt; Bool 。 相似地， map 也使用了变量，只不过这里的 b 可能与 a 类型相同，也可能不相同。我们可以这么理解： map 接受两个参数，第一个是从任意类型 a 到任意类型 b 的函数；第二个是一个数组，元素是任意类型的 a ； map 最后返回的是一个类型 b 的数组。 辨别类型和它们的含义是一项重要的技能，这项技能可以让你在函数式编程的路上走得更远。不仅论文、博客和文档等更易理解，类型签名本身也基本上能够告诉你它的函数性（functionality）。要成为一个能够熟练读懂类型签名的人，你得勤于练习；不过一旦掌握了这项技能，你将会受益无穷，不读手册也能获取大量信息。 123456789101112// head :: [a] -&gt; alet head = function(xs)&#123; return xs[0]&#125;// filter :: (a -&gt; Bool) -&gt; [a] -&gt; [a]let filter = curry(function(f, xs)&#123; return xs.filter(f)&#125;)// reduce :: (b -&gt; a -&gt; b) -&gt; b -&gt; [a] -&gt; blet reduce = curry(function(f, x, xs)&#123; return xs.reduce(f, x)&#125;) 注意看 reduce 的签名，可以看到它的第一个参数是个函数，这个函数接受一个 b 和一个 a 并返回一个 b 。那么这些 a 和 b 是从哪来的呢？很简单，签名中的第二个和第三个参数就是 b 和元素为 a 的数组，所以唯一合理的假设就是这里的 b 和每一个 a 都将传给前面说的函数作为参数。我们还可以看到， reduce 函数最后返回的结果是一个b ，也就是说， reduce 的第一个参数函数的输出就是 reduce 函数的输出。知道了 reduce 的含义，我们才敢说上面关于类型签名的推理是正确的。 12console.log(reduce((b, a) =&gt; a * b)(3)([1, 2, 3]))console.log([1, 2, 3].reduce((b, a) =&gt; a * b, 3)) 缩小可能性范围一旦引入一个类型变量，就会出现一个奇怪的特性叫做 parametricity（http://en.wikipedia.org/wiki/Parametricity ）。这个特性表明，函数将会以一种统一的行为作用于所有的类型。 a 告诉我们它不是一个特定的类型，这意味着它可以是任意类型；那么我们的函数对每一个可能的类型的操作都必须保持统一。这就是 parametricity 的含义。 这种“可能性范围的缩小”（narrowing of possibility）允许我们利用类似 Hoogle 这样的类型签名搜索引擎去搜索我们想要的函数。类型签名所能包含的信息量真的非常大。 PS: Hoogle是Haskell官网中搜索文档的搜索引擎。 自由定理类型签名除了能够帮助我们推断函数可能的实现，还能够给我们带来自由定理（free theorems）。1234// head :: [a] -&gt; acompose(f, head) == compose(head, map(f));// filter :: (a -&gt; Bool) -&gt; [a] -&gt; [a]compose(map(f), filter(compose(p, f))) == compose(filter(p), map(f)); 不用写一行代码你也能理解这些定理，它们直接来自于类型本身。 PS: 这些函数是纯的才能成立。 第一个例子中，等式左边说的是，先获取数组的第一个元素，然后对它调用函数 f ；等式右边说的是，先对数组中的每一个元素调用 f ，然后再取其返回结果的头部 。这两个表达式的作用是相等的，但是前者要快得多。 第二个例子 filter 也是一样。等式左边是说，先组合 f 和 p 检查哪些元素要过滤掉，然后再通过 map 实际调用 f （别忘了 filter 是不会改变数组中元素的，这就保证了 a 将保持不变）；等式右边是说，先用 map 调用 f ，然后再根据 p 过滤元素。这两者也是相等的。 类型约束最后要注意的一点是，签名也可以把类型约束为一个特定的接口（interface）。 1// sort :: Ord a =&gt; [a] -&gt; [a] 胖箭头左边表明的是这样一个事实： a 一定是个 Ord 对象。也就是说， a 必须要实现 Ord 接口。 Ord 到底是什么？它是从哪来的？在一门强类型语言中，它可能就是一个自定义的接口，能够让不同的值排序。通过这种方式，我们不仅能够获取关于 a 的更多信息，了解 sort 函数具体要干什么，而且还能限制函数的作用范围。我们把这种接口声明叫做类型约束（type constraints）。 PS：Haskell里面存在Ord类型类，Rust里面也有类似的东西。 1// assertEqual :: (Eq a, Show a) =&gt; a -&gt; a -&gt; Assertion 总结Hindley-Milner 类型签名在函数式编程中无处不在，它们简单易读，写起来也不复杂。但仅仅凭签名就能理解整个程序还是有一定难度的，要想精通这个技能就更需要花点时间了。 特百惠强大的容器我们已经知道如何书写函数式的程序了，即通过管道把数据在一系列纯函数间传递的程序。我们也知道了，这些程序就是声明式的行为规范。但是，控制流（control flow）、异常处理（error handling）、异步操作（asynchronous actions）和状态（state）呢？还有更棘手的作用（effects）呢？ 首先我们将创建一个容器（container）。这个容器必须能够装载任意类型的值。这个容器将会是一个对象，但我们不会为它添加面向对象观念下的属性和方法。 1234567let Container = function(x) &#123; this.__value = x&#125;Container.of = function(x) &#123; return new Container(x)&#125; 我们将使用 Container.of 作为构造器（constructor），这样就不用到处去写糟糕的 new 关键字了，非常省心。 实际上不能这么简单地看待 of 函数，但暂时先认为它是把值放到容器里的一种方式。 123456Container.of(3)//=&gt; Container(3)Container.of("hotdogs")//=&gt; Container("hotdogs")Container.of(Container.of(&#123;name: "yoda"&#125;))//=&gt; Container(Container(&#123;name: "yoda" &#125;)) 在继续后面的内容之前，先澄清几点：Container 是个只有一个属性的对象。尽管容器可以有不止一个的属性，但大多数容器还是只有一个。我们很随意地把 Container 的这个属性命名为 value。value 不能是某个特定的类型，不然 Container 就对不起它这个名字了。数据一旦存放到 Container ，就会一直待在那儿。我们可以用 .__value 获取到数据，但这样做有悖初衷。 第一个 functor(函子)一旦容器里有了值，不管这个值是什么，我们就需要一种方法来让别的函数能够操作它。 12345678// (a -&gt; b) -&gt; Container a -&gt; Container bContainer.prototype.map = function(f) &#123; return Container.of(f(Container.__value))&#125;// 这个 map 跟数组那个著名的 map 一样，除了前者的参数是 Container a 而后者是 [a] 。它们的使用方式也几乎一致console.log(Container.of(2).map(num =&gt; num + 2))console.log(Container.of('hello').map(str =&gt; str.toUpperCase())) 为什么要使用这样一种方法？因为我们能够在不离开 Container 的情况下操作容器里面的值。这是非常了不起的一件事情。 Container 里的值传递给 map 函数之后，就可以任我们操作；操作结束后，为了防止意外再把它放回它所属的 Container。这样做的结果是，我们能连续地调用 map ，运行任何我们想运行的函数，甚至还可以改变值的类型。 如果我们能一直调用 map ，那它不就是个组合（composition）么！这里边是有什么数学魔法在起作用？是 functor。各位，这个数学魔法就是 functor。 functor 是实现了 map 函数并遵守一些特定规则的容器类型。 没错，functor 就是一个签了合约的接口。functor 是范畴学里的概念。 把值装进一个容器，而且只能使用 map 来处理它，这么做的理由到底是什么呢？如果我们换种方式来问，答案就很明显了：让容器自己去运用函数能给我们带来什么好处？答案是抽象，对于函数运用的抽象。当 map 一个函数的时候，我们请求容器来运行这个函数。不夸张地讲，这是一种十分强大的理念。 薛定谔的 Maybe说实话 Container 挺无聊的，而且通常我们称它为 Identity ，与 id 函数的作用相同（这里也是有数学上的联系的）。除此之外，还有另外一种 functor，那就是实现了 map 函数的类似容器的数据类型，这种 functor 在调用 map 的时候能够提供非常有用的行为。现在让我们来定义一个这样的 functor。 123456789101112131415let Maybe = function(x) &#123; this.__value = x&#125;Maybe.of = function(x) &#123; return new Maybe(x)&#125;Maybe.prototype.isNothing = function() &#123; return (this.__value === null) || (this.__value === undefined)&#125;Maybe.prototype.map = function(f) &#123; return this.isNothing() ? Maybe.of(null) : Maybe.of(f(this.__value))&#125; Maybe 会先检查自己的值是否为空，然后才调用传进来的函数。这样我们在使用 map 的时候就能避免恼人的空值了（这个实现出于教学目的做了简化）。 123console.log(Maybe.of('Malkovich Malkovich').map(match(/a/ig)))console.log(Maybe.of(null).map(match(/a/ig)))console.log(Maybe.of(&#123;name: "Dinah", age: 14&#125;).map(_.property('age'))) 当传给 map 的值是 null 时，代码并没有爆出错误。这是因为每一次 Maybe 要调用函数的时候，都会先检查它自己的值是否为空。 这种点记法（dot notation syntax）已经足够函数式了，但是正如在第 1 部分指出的那样，我们更想保持一种 pointfree 的风格。碰巧的是， map 完全有能力以 curry 函数的方式来“代理”任何 functor。 1234// map :: Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f bmap = curry(function(f, any_functor_at_all) &#123; return any_functor_at_all.map(f)&#125;) 这样我们就可以像平常一样使用组合，同时也能正常使用 map 了，非常振奋人心。ramda 的 map 也是这样。后面的，我们将在点记法更有教育意义的时候使用点记法，在方便使用 pointfree 模式的时候就用 pointfree。你注意到了么？我在类型标签中偷偷引入了一个额外的标记： Functor f =&gt; 。这个标记告诉我们 f 必须是一个 functor。 用例 123456789101112// safeHead :: [a] -&gt; Maybe(a)let safeHead = function(xs) &#123; return Maybe.of(xs[0])&#125;let streetName = compose(compose(map(_.property('street')), safeHead), _.property('addresses'))console.log(streetName(&#123;addresses:[]&#125;))// Maybe(null)console.log(streetName(&#123;addresses: [&#123;street: "Shady Ln.", number: 4201&#125;]&#125;))// Maybe("Shady Ln.") safeHead 与一般的 _.head 类似，但是增加了类型安全保证。引入 Maybe 会发生一件非常有意思的事情，那就是我们被迫要与狡猾的 null 打交道了。 safeHead 函数能够诚实地预告它可能的失败,然后返回一个 Maybe 来通知我们相关信息。实际上不仅仅是通知，因为毕竟我们想要的值深藏在 Maybe 对象中，而且只能通过 map 来操作它。本质上，这是一种由 safeHead 强制执行的空值检查。 类似这样的 API 能够把一个像纸糊起来的、脆弱的应用升级为实实在在的、健壮的应用，这样的 API 保证了更加安全的软件。 有时候函数可以明确返回一个 Maybe(null) 来表明失败 12345678910111213141516171819// withdraw :: Number -&gt; Account -&gt; Maybe(Account)let withdraw = curry(function(amount, account)&#123; return account.balance &gt;= account ? Maybe.of(&#123;account: account.balance - account&#125;) : Maybe.of(null)&#125;)// finishTransaction :: Account -&gt; Stringlet finishTransaction = compose(remainingBalance, updateLedger)// &lt;- 假定这两个函数已经在别处定义好了// getTwenty :: Account -&gt; Maybe(String)let getTwenty = compose(map(finishTransaction), withdraw(20))getTwenty(&#123; balance: 200.00&#125;)// Maybe("Your balance is $180.00")getTwenty(&#123; balance: 10.00&#125;)// Maybe(null) withdraw 也显示出了它的多变性，使得我们后续的操作只能用 map 来进行。这个例子与前面例子不同的地方在于，这里的 null 是有意的。我们不用 Maybe(String) ，而是用 Maybe(null)来发送失败的信号，这样程序在收到信号后就能立刻停止执行。这一点很重要：如果 withdraw 失败了，map 就会切断后续代码的执行，因为它根本就不会运行传递给它的函数，即finishTransaction 。这正是预期的效果：如果取款失败，我们并不想更新或者显示账户余额。 释放容器里的值人们经常忽略的一个事实是：任何事物都有个最终尽头。那些会产生作用的函数，不管它们是发送 JSON 数据，还是在屏幕上打印东西，还是更改文件系统，还是别的什么，都要有一个结束。但是我们无法通过 return 把输出传递到外部世界，必须要运行这样或那样的函数才能传递出去。 应用程序所做的工作就是获取、更改和保存数据直到不再需要它们，对数据做这些操作的函数有可能被 map 调用，这样的话数据就可以不用离开它温暖舒适的容器。讽刺的是，有一种常见的错误就是试图以各种方法删除 Maybe 里的值，好像这个不确定的值是魔鬼，删除它就能让它突然显形，然后一切罪恶都会得到宽恕似的（此处原文应该是源自圣经）。要知道，我们的值没有完成它的使命，很有可能是其他代码分支造成的。我们的代码，就像薛定谔的猫一样，在某个特定的时间点有两种状态，而且应该保持这种状况不变直到最后一个函数为止。这样，哪怕代码有很多逻辑性的分支，也能保证一种线性的工作流。 不过，对容器里的值来说，还是有个逃生口可以出去。也就是说，如果我们想返回一个自定义的值然后还能继续执行后面的代码的话，是可以做到的；要达到这一目的，可以借助一个帮助函数 maybe ： 123456789101112// maybe :: b -&gt; (a -&gt; b) -&gt; Maybe a -&gt; blet maybe = _.curry(function(x, f, m)&#123; return m.isNothing() ? x : f(m.__value)&#125;)// getTwenty :: Account -&gt; Stringlet getTwenty = compose(maybe("You're broke!", finishTransaction), withdraw(20))getTwenty(&#123; balance: 200.00&#125;);// "Your balance is $180.00"getTwenty(&#123; balance: 10.00&#125;);// "You're broke!" 这样就可以要么返回一个静态值（与 finishTransaction 返回值的类型一致），要么继续愉快地在没有 Maybe 的情况下完成交易。 maybe 使我们得以避免普通 map 那种命令式的 if/else 语句： if(x !== null) { return f(x) } 。 引入 Maybe 可能会在初期造成一些不适。Swift 和 Scala 用户知道我在说什么，因为这两门语言的核心库里就有 Maybe 的概念，只不过伪装成 Option(al) 罢了。 PS：Haskell 里面有Maybe，Rust里面也是类似伪装成 Option。 被迫在任何情况下都进行空值检查，的确让大部分人头疼不已。然而随着时间推移，空值检查会成为第二本能，不管怎么说，空值检查大多数时候都能防止在代码逻辑上偷工减料，让我们脱离危险。 Maybe 能够非常有效地帮助我们增加函数的安全性。 有一点我必须要提及，否则就太不负责任了，那就是 Maybe 的“真正”实现会把它分为两种类型：一种是非空值，另一种是空值。这种实现允许我们遵守 map 的 parametricity 特性，因此 null 和 undefined 能够依然被 map 调用，functor 里的值所需的那种普遍性条件也能得到满足。所以你会经常看到 Some(x) / None 或者 Just(x) / Nothing 这样的容器类型在做空值检查，而不是Maybe 。 “纯”错误处理说出来可能会让你震惊， throw/catch 并不十分“纯”。当一个错误抛出的时候，我们没有收到返回值，反而是得到了一个警告！ 有了 Either 这个新朋友，我们就能以一种好得多的方式来处理错误，那就是返回一条非常礼貌的消息作为回应。 12345678910111213141516171819202122232425262728293031323334353637383940let Left = function(x) &#123; this.__value = x&#125;Left.of = function(x) &#123; return new Left(x)&#125;Left.prototype.map = function(f) &#123; return this&#125;let Right = function(x) &#123; this.__value = x&#125;Right.of = function(x) &#123; return new Right(x)&#125;Right.prototype.map = function(f) &#123; return Right.of(f(this.__value))&#125;// Left 和 Right 是我们称之为 Either 的抽象类型的两个子类。let right = Right.of("rain").map(function(str) &#123; return "b" + str&#125;)// Right("brain")let left = Left.of("rain").map(function(str) &#123; return "b" + str&#125;)// Left("rain")console.log(Right.of(&#123;host: 'localhost', port:80&#125;).map(_.property('host')))// Right('localhost')console.log(Left.of("rolls eyes...").map(_.property('host')))// Left('rolls eyes...') Left 就像是青春期少年那样无视我们要 map 它的请求。 Right 的作用就像是一个 Container （也就是 Identity）。这里强大的地方在于， Left 有能力在它内部嵌入一个错误消息。 假设有一个可能会失败的函数，就拿根据生日计算年龄来说好了。的确，我们可以用 Maybe(null) 来表示失败并把程序引向另一个分支，但是这并没有告诉我们太多信息。很有可能我们想知道失败的原因是什么。 123456789101112let getAge = curry(function(now, user) &#123; let birthdate = moment(user.birthdate, 'YYYY-MM-DD') if(!birthdate.isValid()) &#123; return Left.of("Birth date could not be parsed") &#125; return Right.of(now.diff(birthdate, 'years'))&#125;)console.log(getAge(moment(), &#123;birthdate: '1996-12-30'&#125;))// Right(21)console.log(getAge(moment(), &#123;birthdate: '12-30'&#125;))// Left("Birth date could not be parsed") 这么一来，就像 Maybe(null) ，当返回一个 Left 的时候就直接让程序短路。跟 Maybe(null) 不同的是，现在我们对程序为何脱离原先轨道至少有了一点头绪。有一件事要注意，这里返回的是 Either(String, Number) ，意味着我们这个 Either 左边Left的值是 String，右边Right，也就是正确的值，是 Number 。这个类型签名不是很正式，因为我们并没有定义一个真正的 Either父类；但我们还是从这个类型那里了解到不少东西。它告诉我们，我们得到的要么是一条错误消息，要么就是正确的值。 PS：Rust里面也有类似的概念，对应于Result enum里的Ok，Err。 123456789// map 如之前所实现，会调用container的map方法// zoltar :: User -&gt; Either(String, _)let zoltar = compose(map(console.log), getAge(moment()))console.log(zoltar(&#123;birthdate: '1996-12-30'&#125;))// 21// Right(undefined)console.log(zoltar(&#123;birthdate: 'balloons!'&#125;))// Left("Birth date could not be parsed") 如果 birthdate 合法，这个程序就会把它神秘的命运打印在屏幕上让我们见证；如果不合法，我们就会收到一个有着清清楚楚的错误消息的 Left ，尽管这个消息是稳稳当当地待在它的容器里的。这种行为就像，虽然我们在抛错，但是是以一种平静温和的方式抛错，而不是像一个小孩子那样，有什么不对劲就闹脾气大喊大叫。 我们根据 birthdate 的合法性来控制代码的逻辑分支，同时又让代码进行从右到左的直线运动，而不用爬过各种条件语句的大括号。 我们在 Right 分支的类型签名中使用 _ 表示一个应该忽略的值（在有些浏览器中，你必须要 console.log.bind(console) 才能把 console.log 当作一等公民使用）。 这个例子中，尽管 fortune 使用了 Either ，它对每一个 functor 到底要干什么却是毫不知情的。通俗点来讲，一个函数在调用的时候，如果被map 包裹了，那么它就会从一个非 functor 函数转换为一个 functor 函数。我们把这个过程叫做 lift。 一般情况下，普通函数更适合操作普通的数据类型而不是容器类型，在必要的时候再通过 lift 变为合适的容器去操作容器类型。这样做的好处是能得到更简单、重用性更高的函数，它们能够随需求而变，兼容任意 functor。 Either 并不仅仅只对合法性检查这种一般性的错误作用非凡，对一些更严重的、能够中断程序执行的错误比如文件丢失或者 socket 连接断开等， Either 同样效果显著。 它的能耐远不止于此。比如，它表示了逻辑或（也就是||）。再比如，它体现了范畴学里 coproduct 的概念。还比如，它是标准的 sum type（或者叫不交并集，disjoint union of sets），因为它含有的所有可能的值的总数就是它包含的那两种类型的总数。 Either 能做的事情多着呢，但是作为一个 functor，我们就用它处理错误。 就像 Maybe 可以有个 maybe 一样， Either 也可以有一个 either 。两者的用法类似，但 either 接受两个函数（而不是一个）和一个静态值为参数。这两个函数的返回值类型一致： 123456789101112131415let either = curry(function(f, g, e) &#123; switch(e.constructor) &#123; case Left: return f(e.__value) case Right: return g(e.__value) &#125;&#125;)const localStorage = Object.create(null)// getFromStorage :: String -&gt; (_ -&gt; String)let getFromStorage = function(key) &#123; return function() &#123; return localStorage[key] &#125;&#125; 要是我们没把 getFromStorage 包在另一个函数里，它的输出值就是不定的，会随外部环境变化而变化。有了这个结实的包裹函数（wrapper），同一个输入就总能返回同一个输出：一个从 localStorage 里取出某个特定的元素的函数。 然而，这并没有多大的用处。就像是你收藏的全新未拆封的玩偶，不能拿出来玩有什么意思。所以要是能有办法进到这个容器里面，拿到它藏在那儿的东西就好了…办法是有的，请看 IO。 12345678910111213let IO = function(f) &#123; this.__value = f&#125;IO.of = function(x) &#123; return new IO(function() &#123; return x &#125;)&#125;IO.prototype.map = function(f) &#123; return new IO(compose(f, this.__value))&#125; IO 跟之前的 functor 不同的地方在于，它的 __value 总是一个函数。不过我们不把它当作一个函数——实现的细节我们最好先不管。这里发生的事情跟我们在getFromStorage 那里看到的一模一样： IO 把非纯执行动作（impure action）捕获到包裹函数里，目的是延迟执行这个非纯动作。就这一点而言，我们认为 IO 包含的是被包裹的执行动作的返回值，而不是包裹函数本身。这在 of 函数里很明显： IO(function(){ return x }) 仅仅是为了延迟执行，其实我们得到的是 IO(x) 。 12345678910111213let io_window = new IO(function() &#123; return window&#125;)io_window.map(function(win) &#123; return window.innerWidth&#125;)// IO(1430)io_window2 = io_window.map(_.property('location')) .map(_.property('href')) .map(_.split('/'))// IO(["http:", "", "localhost:8000", "blog", "posts"]) 这里， io_window 是一个真正的 IO ，我们可以直接对它使用 map 。我把这里的返回值都写成了概念性的，这样就更加直观；不过实际的返回值是 { __value: [Function] } 。当调用 IO 的 map 的时候，我们把传进来的函数放在了 map 函数里的组合的最末端（也就是最左边），反过来这个函数就成为了新的 IO 的新 value ，并继续下去。传给 map 的函数并没有运行，我们只是把它们压到一个“运行栈”的最末端而已，一个函数紧挨着另一个函数，就像小心摆放的多米诺骨牌一样，让人不敢轻易推倒。这种情形很容易叫人联想起“四人帮”（《设计模式》一书作者）提出的命令模式（command pattern）或者队列（queue）。 IO 的 value 并不是它包含的值，也不是像两个下划线暗示那样是一个私有属性。value 是手榴弹的弹栓，只应该被调用者以最公开的方式拉动。为了提醒用户它的变化无常，我们把它重命名为 unsafePerformIO 看看 12345678910111213let IO = function(f) &#123; this.unsafePerformIO = f&#125;IO.of = function(x) &#123; return new IO(function() &#123; return x &#125;)&#125;IO.prototype.map = function(f) &#123; return new IO(compose(f, this.unsafePerformIO))&#125; 现在调用的代码对于应用程序的用户简直就直白得不能再直白了。 之后我们将学习一种跟 IO 在精神上相似，但是用法上又千差万别的类型。 异步任务处理异步代码，我们有一种更好的方式，它的名字以“F”开头。这种方式的内部机制过于复杂，复杂得哪怕我唾沫横飞也很难讲清楚。所以我们就直接用 Quildreen Motta 的 Folktale 里的 Data.Task。 123456789101112131415161718192021222324252627282930313233const Folktale = require('folktale')const Task = Folktale.concurrency.task// Node readfile example://=======================const fs = require('fs');// readFile :: String -&gt; Task(Error, JSON)let readFile = function(filename) &#123; return new Task(function(reject, result) &#123; fs.readFile(filename, 'utf-8', function(err, data) &#123; err ? reject(err) : result(data); &#125;); &#125;);&#125;;readFile("metamorphosis").map(split('\n')).map(head);// Task("One morning, as Gregor Samsa was waking up from anxious dreams, he discovered that// in bed he had been changed into a monstrous verminous bug.")// jQuery getJSON example://========================// getJSON :: String -&gt; &#123;&#125; -&gt; Task(Error, JSON)let getJSON = curry(function(url, params) &#123; return new Task(function(reject, result) &#123; $.getJSON(url, params, result).fail(reject); &#125;);&#125;);getJSON('/video', &#123;id: 10&#125;).map(_.prop('title'));// Task("Family Matters ep 15")// 传入普通的实际值也没问题Task.of(3).map(function(three)&#123; return three + 1 &#125;);// Task(4) 例子中的 reject 和 result 函数分别是失败和成功的回调。正如你看到的，我们只是简单地调用 Task 的 map 函数，就能操作将来的值，好像这个值就在那儿似的。 如果熟悉 promise 的话，你该能认出来 map 就是 then ， Task 就是一个promise。 与 IO 类似， Task 在我们给它绿灯之前是不会运行的。事实上，正因为它要等我们的命令， IO 实际就被纳入到了 Task 名下，代表所有的异步操作—— readFile 和 getJSON 并不需要一个额外的 IO 容器来变纯。更重要的是，当我们调用它的 map 的时候， Task 工作的方式与 IO 几无差别：都是把对未来的操作的指示放在一个时间胶囊里，就像家务列表（chore chart）那样——真是一种精密的拖延术。 我们必须调用 fork 方法才能运行 Task ，这种机制与 unsafePerformIO 类似。但也有不同，不同之处就像 fork 这个名称表明的那样，它会 fork 一个子进程运行它接收到的参数代码，其他部分的执行不受影响，主线程也不会阻塞。当然这种效果也可以用其他一些技术比如线程实现，但这里的这种方法工作起来就像是一个普通的异步调用，而且 event loop 能够不受影响地继续运转。 12345678910111213141516// Pure application//=====================// blogTemplate :: String// blogPage :: Posts -&gt; HTMLlet blogPage = Handlebars.compile(blogTemplate);// renderPage :: Posts -&gt; HTMLlet renderPage = compose(blogPage, sortBy('date'));// blog :: Params -&gt; Task(Error, HTML)let blog = compose(map(renderPage), getJSON('/posts'));// Impure calling code//=====================blog(&#123;&#125;).fork(function(error)&#123; $("#error").html(error.message); &#125;,function(page)&#123; $("#main").html(page); &#125;);$('#spinner').show(); 调用 fork 之后， Task 就赶紧跑去找一些文章，渲染到页面上。与此同时，我们在页面上展示一个 spinner，因为 fork 不会等收到响应了才执行它后面的代码。最后，我们要么把文章展示在页面上，要么就显示一个出错信息，视getJSON 请求是否成功而定。 我们只需要从下读到上，从右读到左就能理解代码，即便这段程序实际上会在执行过程中到处跳来跳去。这种方式使得阅读和理解应用程序的代码比那种要在各种回调和错误处理代码块之间跳跃的方式容易得多。 一点理论functor 的概念来自于范畴学，并满足一些定律。 1234// identitymap(id) === id;// compositioncompose(map(f), map(g)) === map(compose(f, g)); 同一律很简单，但是也很重要。因为这些定律都是可运行的代码，所以我们完全可以在我们自己的 functor 上试验它们，验证它们是否成立。 在范畴学中，functor 接受一个范畴的对象和态射（morphism），然后把它们映射（map）到另一个范畴里去。根据定义，这个新范畴一定会有一个单位元（identity），也一定能够组合态射；我们无须验证这一点，前面提到的定律保证这些东西会在映射后得到保留。 可以把范畴想象成一个有着多个对象的网络，对象之间靠态射连接。那么 functor 可以把一个范畴映射到另外一个，而且不会破坏原有的网络。如果一个对象 a 属于源范畴 C ，那么通过 functor F 把 a 映射到目标范畴 D 上之后，就可以使用 F a 来指代 a 对象。 比如， Maybe 就把类型和函数的范畴映射到这样一个范畴：即每个对象都有可能不存在，每个态射都有空值检查的范畴。这个结果在代码中的实现方式是用 map 包裹每一个函数，用 functor 包裹每一个类型。这样就能保证每个普通的类型和函数都能在新环境下继续使用组合。 从技术上讲，代码中的 functor 实际上是把范畴映射到了一个包含类型和函数的子范畴（sub category）上，使得这些 functor 成为了一种新的特殊的 endofunctor。可以用一张图来表示这种态射及其对象的映射。 这张图除了能表示态射借助 functor F 完成从一个范畴到另一个范畴的映射之外，我们发现它还符合交换律，也就是说，顺着箭头的方向往前，形成的每一个路径都指向同一个结果。不同的路径意味着不同的行为，但最终都会得到同一个数据类型。这种形式化给了我们原则性的方式去思考代码——无须分析和评估每一个单独的场景，只管可以大胆地应用公式即可。 123456789101112131415161718192021// topRoute :: String -&gt; Maybe(String)let topRoute = compose(Maybe.of, safeHead);// bottomRoute :: String -&gt; Maybe(String) let bottomRoute = compose(map(safeHead), Maybe.of);console.log(topRoute('hi'));// Maybe('h')console.log(bottomRoute('hi'));// Maybe('h')// topRoute :: String -&gt; Maybe(String)let topRoute = compose(Maybe.of, reverse);// bottomRoute :: String -&gt; Maybe(String)let bottomRoute = compose(map(reverse), Maybe.of);topRoute("hi");// Maybe("ih")bottomRoute("hi");// Maybe("ih") 根据所有 functor 都有的特性，我们可以立即理解代码，重构代码。 functor 也能嵌套使用 12345let nested = Task.of([Right.of("pillows"), Left.of("no sleep for you")]);map(map(map(toUpperCase)), nested);Task([Right("PILLOWS"), Left("no sleep for you")]) nested 是一个将来的数组，数组的元素有可能是程序抛出的错误。我们使用map 剥开每一层的嵌套，然后对数组的元素调用传递进去的函数。可以看到，这中间没有回调、 if/else 语句和 for 循环，只有一个明确的上下文。的确，我们必须要 map(map(map(f))) 才能最终运行函数。不想这么做的话，可以组合 functor。 1234567let Compose = function(f_g_x) &#123; this.getCompose = f_g_x;&#125;Compose.prototype.map = function(f) &#123; return new Compose(map(map(f), this.getCompose));&#125; functor 组合是符合结合律的，而且之前我们定义的 Container 实际上是一个叫 Identity 的 functor。identity 和可结合的组合也能产生一个范畴，这个特殊的范畴的对象是其他范畴，态射是 functor。 总结我们已经认识了几个不同的 functor，但它们的数量其实是无限的。有一些值得注意的可迭代数据类型（iterable data structure）我们没有介绍，像 tree、list、map 和 pair 等，以及所有你能说出来的。eventstream 和 observable 也都是 functor。其他的 functor 可能就是拿来做封装或者仅仅是模拟类型。我们身边到处都有 functor的身影。 用多个 functor 参数调用一个函数怎么样呢？处理一个由不纯的或者异步的操作组成的有序序列怎么样呢？要应对这个什么都装在盒子里的世界，目前我们工具箱里的工具还不全。下一章，我们将直奔 monad 而去。 自我总结PS里面是自己的话。 这部分的其实还有挺多的东西没有搞懂，书中许多代码其实是无法运行的，或者依赖很多编写成本很大。书里面还有一些题目，我也没有花时间进去。个人觉得这本书比一般的函数式书要深入，当然估计在Haskell书里，这些知识都很常规。其他的东西大部分都是书中的内容，我自己进行了少量地修改和较多地删减。 最近在学习Rust和Haskell，打算寒假精力主要放在学习这2门语言特有的特性，说白了，就是安全性和函数式。但是这2门语言门槛其实都不低，2018年平时可能会投入一些精力去学习。寒假如果顺利，希望能用这些语言来复习数据结构的知识。下个学期特意选了一些偏实战专业课，打算把新学到的语言、特性和思想用起来。 这本书最后还有2章，如果有机会，想在寒假看完。]]></content>
      <categories>
        <category>CS</category>
      </categories>
      <tags>
        <tag>Book</tag>
        <tag>JS</tag>
        <tag>FP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IEEE802.11概述]]></title>
    <url>%2F2017%2F12%2F26%2FIEEE802.11%E6%97%A0%E7%BA%BFLANs%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[IEEE 802.11无线 LANs 概述无线链路和网络特征802.11协议簇是国际电工电子工程学会（IEEE）为无线局域网络制定的标准。虽然WI-FI使用了802.11的媒体访问数据链路层（DLL）和物理层（PHY），但是两者并不完全一致。 这里主要概述了 802.11无线LAN的特征。不过首先应该关注的是无线网络与有线网络的区别。 当我们寻找有线和无线网络的重要区别时，应该重点关注链路层，我们确实可以发现有线链路和无线链路有许多重要区别： 递减的信号强度。信号强度受到发送方和接收方距离的增加而递减。 来自其他源的干扰。在同一个频段发送信号的电波源会互相干扰。 多径传播（multipath propagation）。当电磁波受到反射，会在发送方和接收方之间走过多条不同长度的路径，此时出现多径传播，这使得接受的信号变得模糊。 IEEE 802.11 标准常见的关于LAN的802.11标准包括 802.11b、802.11a、802.11g等。它们使用相同的媒体访问协议CSMA/CA，并且使用相同的链路层帧格式。 802.11 体系结构802.11 体系结构的基本构建模块是基本服务集（Basic Service Set BSS），一个BBS包含若干个无线站点和一个被称为接入点（AP）的中央基站。与以太网设备类似，每个802.11无线站点都具有6字节的MAC地址。每个AP的无线接口也具有一个MAC地址。这些MAC地址也由IEEE管理，理论上全球唯一。 信道与关联在802.11中，每个无线站点在发送或者接受网络层数据前，必须与一个AP相关联。而安装一个AP时，需要为其分配单字或者双字的服务集标识（SSID）。除了SSID，还需要分配一个信道号。 信道： 802.11定义了11个部分重叠的信道，当且仅当2个信道由4个或更多的信道隔开时，它们才不重叠。比如 1、6、11是唯一的3个非重叠信道集合。这样就可以在同一个物理网络安装3个AP。 关联： 为了获得互联网接入，你的无线站点需要加入其中一个子网并需要与其中一个AP相关联（associate）。关联意味着该无线站点在自身与该AP之间创建了一个虚拟线路。 然而，首先得回答：你的无线站点如何与某个特定的AP相关联？或者你的无线站点如何知道当前位置哪个AP可以使用？ 802.11标准要求每个AP周期性地发送信标帧（beacon frame），每个信标帧包括该AP的SSID和MAC地址。你的无线站点为了得知正在发送信标帧的AP，扫描11个信道，找到可能来着可能位于该区域的AP所发出的信标帧。 不过，802.11标准没有指定选择哪个可用的AP进行关联的算法，这个由802.11固件和无线主机的软件设计者来设计。比如，主机选择接收到的具有最高信号强度的信标帧，但可能过载。 这里扫描分两种： 被动扫描（passive scanning），自AP发送信标帧，无线站点扫描信道和监听信标帧，然后向选择的AP发送关联请求帧，最后AP响应关联响应帧 主动扫描（active scanning），无线站点广播探测请求帧，AP发送探测响应，然后无线站点选择AP发送关联请求帧，最后AP响应关联响应帧 鉴别与接入为了与特定的AP创建关联，某无线站点可能需要向该AP鉴别它自身。 802.11 提供了几种不同的鉴别和接入方法 基于站点的MAC地址允许其接入网络 基于用户名和口令 上述两种情况下，可能使用的RADIUS（基于UDP）和DIAMETER（最初作为RADIUS的改良）这样的协议。]]></content>
      <categories>
        <category>CS</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>IEEE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS函数式编程笔记1]]></title>
    <url>%2F2017%2F12%2F11%2FJS%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B01%2F</url>
    <content type="text"><![CDATA[JS函数式编程我们在做什么 “we both know what happens when you assume”，源自一句名言“When you assume you make an ASS of U and ME”，意思是“让两人都难堪”）。但我猜想你在使用可变状态（mutable state）、无限制副作用（unrestricted side effects）和无原则设计（unprincipled design）的过程中已经遇到过一些麻烦。 现在已经有一些通用的编程原则了，各种缩写词带领我们在编程的黑暗隧道里前行：DRY（不要重复自己，don’t repeat yourself），高内聚低耦合（loosecoupling high cohesion），YAGNI （你不会用到它的，ya ain’t gonna need it），最小意外原则（Principle of least surprise），单一责任（single responsibility）等等。这些原则同样适用于函数式编程(FP)。 先看基本的例子 1234567891011let add = function (x, y) &#123; return x + y&#125;;let multiply = function (x, y) &#123; return x * y&#125;;let a = 4;let b = 2;let c = 0;let result = add(multiply(b, add(a, c)), multiply(a, b));//=&gt;16 你会发现我们不过是在运用古人早已获得的知识 1234567891011121314// 结合律（assosiative）add(add(x, y), z) == add(x, add(y, z));// 交换律（commutative）add(x, y) == add(y, x);// 同一律（identity）add(x, 0) == x;// 分配律（distributive）multiply(x, add(y, z)) == add(multiply(x, y), multiply(x, z));// 原有代码add(multiply(b, add(a, c)), multiply(a, b));// 应用同一律，去掉多余的加法操作（add(a, c) == a）add(multiply(b, a), multiply(a, b));// 再应用分配律multiply(b, add(a, a)); 当然这里我们定义 add 和 multiply 是为了代码完整性，实际上并不必要——在调用之前它们肯定已经在某个类库里定义好了。 我们希望去践行每一部分都能完美接合的理论，希望能以一种通用的、可组合的组件来表示我们的特定问题，然后利用这些组件的特性来解决这些问题。相比命令式编程的那种“某某去做某事”的方式，函数式编程将会有更多的约束，不过你会震惊于这种强约束、数学性的“框架”所带来的回报。 一等公民的函数先看看常见的JS风格的一个例子 1234567891011121314151617181920function ajaxCall(func) &#123; //...&#125;// 用一个函数把另一个函数包起来，目的仅仅是延迟执行，真的是非常糟糕的编程习惯let getServerStuff = function (callback) &#123; return ajaxCall(function (json) &#123; return callback(json); &#125;);&#125;;// 这行// return ajaxCall(function (json) &#123;// return callback(json);// &#125;);// 等价于这行// return ajaxCall(callback);// 应该改成这样let getServerStuff = ajaxCall; 再看另一个例子 12345678910111213141516171819202122232425262728293031323334353637383940414243class Views &#123; //...&#125;class Db &#123; //...&#125;// 这样做除了徒增代码量，提高维护和检索代码的成本外，没有任何用处let BlogController = (function () &#123; let index = function (posts) &#123; return Views.index(posts); &#125;; let show = function (post) &#123; return Views.show(post); &#125;; let create = function (attrs) &#123; return Db.create(attrs); &#125;; let update = function (post, attrs) &#123; return Db.update(post, attrs); &#125;; let destroy = function (post) &#123; return Db.destroy(post); &#125;; return &#123; index, show, create, update, destroy &#125;;&#125;)();// 我们可以把它重写成这样：let BlogController = &#123; index: Views.index, show: Views.show, create: Db.create, update: Db.update, destroy: Db.destroy&#125;;// 或者直接全部删掉，因为它的作用仅仅就是把视图（Views）和数据库（Db）打包在一起而已 另外，如果一个函数被不必要地包裹起来了，而且发生了改动，那么包裹它的那个函数也要做相应的变更。 12345678910111213httpGet('/post/2', function(json)&#123; return renderPost(json);&#125;);// 如果 httpGet 要改成可以抛出一个可能出现的 err 异常，那我们还要回过头去把“胶水”函数也改了// 把整个应用里的所有 httpGet 调用都改成这样，可以传递 err 参数。httpGet('/post/2', function(json, err)&#123; return renderPost(json, err);&#125;);// 写成一等公民函数的形式，要做的改动将会少得多httpGet('/post/2', renderPost); // renderPost 将会在 httpGet 中调用，想要多少参数都行 除了删除不必要的函数，正确地为参数命名也必不可少。当然命名不是什么大问题，但还是有可能存在一些不当的命名，尤其随着代码量的增长以及需求的变更，这种可能性也会增加。 如果一个底层函数使用了 this，而且是以一等公民的方式被调用的，那你就等着 JS 这个蹩脚的抽象概念发怒吧 12345const fs = require('fs');// 太可怕了fs.readFile('freaky_friday.txt', Db.save);// 好一点点fs.readFile('freaky_friday.txt', Db.save.bind(Db)); 纯函数的好处 纯函数是这样一种函数，即相同的输入，永远会得到相同的输出，而且没有任何可观察的副作用。 比如 slice 和 splice。我们说 slice 符合纯函数的定义是因为对相同的输入它保证能返回相同的输出。而 splice 却会嚼烂调用它的那个数组，然后再吐出来；这就会产生可观察到的副作用，即这个数组永久地改变了。 123456789101112131415let xs = [1,2,3,4,5];// 纯的xs.slice(0,3);//=&gt; [1,2,3]xs.slice(0,3);//=&gt; [1,2,3]xs.slice(0,3);//=&gt; [1,2,3]// 不纯的xs.splice(0,3);//=&gt; [1,2,3]xs.splice(0,3);//=&gt; [4,5]xs.splice(0,3);//=&gt; [] 下一个例子 1234567891011121314151617// 不纯的let minimum = 21;let checkAge = function (age) &#123; return age &gt;= minimum;&#125;;// 纯的let checkAge = function (age) &#123; let minimum = 21; return age &gt;= minimum;&#125;;// 纯的const minimum = 21;let checkAge = function (age) &#123; return age &gt;= minimum;&#125;; 在不纯的版本中， checkAge 的结果将取决于 minimum 这个可变变量的值。换句话说，它取决于系统状态（system state）；这一点令人沮丧，因为它引入了外部的环境，从而增加了认知负荷（cognitive load）。 另一方面，使用纯函数的形式，函数就能做到自给自足。我们也可以让 minimum成为一个不可变（immutable）对象，这样就能保留纯粹性，因为状态不会有变化。要实现这个效果，必须得创建一个对象，然后调用 Object.freeze 方法 123let immutableState = Object.freeze(&#123; minimun: 21&#125;); 副作用是在计算结果的过程中，系统状态的一种变化，或者与外部世界进行的可观察的交互。 副作用可能包含，但不限于： 更改文件系统 往数据库插入记录 发送一个 http 请求 可变数据 打印/log 获取用户输入 DOM 查询 访问系统状态 这个列表还可以继续写下去。概括来讲，只要是跟函数外部环境发生的交互就都是副作用——这一点可能会让你怀疑无副作用编程的可行性。函数式编程的哲学就是假定副作用是造成不正当行为的主要原因。 这并不是说，要禁止使用一切副作用，而是说，要让它们在可控的范围内发生。后面讲到 functor 和 monad 的时候我们会学习如何控制它们。 追求“纯”的理由 可缓存性（Cacheable） 可移植性／自文档化（Portable / SelfDocumenting） 可测试性（Testable） 合理性（Reasonable） 并行性(Parallelism) 可缓存性（Cacheable）首先，纯函数总能够根据输入来做缓存。实现缓存的一种典型方式是 memoize 技术 123456789let squareNumber = memoize(function()&#123;return x * x&#125;);squareNumber(4);//=&gt; 16squareNumber(4); // 从缓存中读取输入值为 4 的结果//=&gt; 16squareNumber(5);//=&gt; 25squareNumber(5); // 从缓存中读取输入值为 5 的结果//=&gt; 25 下面的代码是memoize一个简单的实现，尽管它不太健壮 123456789function momoize(f) &#123; let cache = &#123;&#125; return function () &#123; let arg_str = JSON.stringify(arguments) // 有cache则返回cache里的，总是保存到cache里 cache[arg_str] = cache[arg_str] || f(...arguments) return cache[arg_str] &#125;&#125; 值得注意的一点是，可以通过延迟执行的方式把不纯的函数转换为纯函数 123let pureHttpCall = memoize(function(url, params)&#123; return function() &#123; return $.getJSON(url, params); &#125;&#125;); 这里有趣的地方在于我们并没有真正发送 http 请求——只是返回了一个函数，当调用它的时候才会发请求。这个函数之所以有资格成为纯函数，是因为它总是会根据相同的输入返回相同的输出：给定了 url 和 params 之后，它就只会返回同一个发送 http 请求的函数 我们的 memoize 函数工作起来没有任何问题，虽然它缓存的并不是 http 请求所返回的结果，而是生成的函数。 重点是我们可以缓存任意一个函数，不管它们看起来多么具有破坏性。 可移植性／自文档化（Portable / SelfDocumenting）纯函数是完全自给自足的，它需要的所有东西都能轻易获得。仔细思考思考这一点…这种自给自足的好处是什么呢？首先，纯函数的依赖很明确，因此更易于观察和理解。 12345678910111213141516171819202122232425262728// 不纯的let signUp = function (attrs) &#123; let user = saveUser(attrs); welcomeUser(user);&#125;;let saveUser = function (attrs) &#123; let user = Db.save(attrs); //...&#125;;let welcomeUser = function (user) &#123; // Email(user, ...); //...&#125;;// 纯的let signUp = function (Db, Email, attrs) &#123; return function () &#123; // 参数绑定 let user = saveUser(Db, attrs); welcomeUser(Email, user); &#125;;&#125;;let saveUser = function (Db, attrs) &#123; //...&#125;;let welcomeUser = function (Email, user) &#123; //...&#125;; 这个例子表明，纯函数对于其依赖必须要诚实，这样我们就能知道它的目的。仅从纯函数版本的 signUp 的签名就可以看出，它将要用到 Db、Email 和 attrs ，这在最小程度上给了我们足够多的信息。 其次，通过强迫“注入”依赖，或者把它们当作参数传递，我们的应用也更加灵活；因为数据库或者邮件客户端等等都参数化了。 命令式编程中“典型”的方法和过程都深深地根植于它们所在的环境中，通过状态、依赖和有效作用（available effects）达成；纯函数与此相反，它与环境无关，只要我们愿意，可以在任何地方运行它。 可测试性（Testable）第三点，纯函数让测试更加容易。 我们不需要伪造一个“真实的”支付网关，或者每一次测试之前都要配置、之后都要断言状态（assert the state）。只需简单地给函数一个输入，然后断言输出就好了。 合理性（Reasonable）很多人相信使用纯函数最大的好处是引用透明性（referential transparency）。如果一段代码可以替换成它执行所得的结果，而且是在不改变整个程序行为的前提下替换的，那么我们就说这段代码是引用透明的。 由于纯函数总是能够根据相同的输入返回相同的输出，所以它们就能够保证总是返回同一个结果，这也就保证了引用透明性。我们来看一个例子。 12345678910111213141516171819202122let Immutable = require('immutable');let decrementHP = function(player) &#123; return player.set("hp", player.hp-1);&#125;;let isSameTeam = function(player1, player2) &#123; return player1.team === player2.team;&#125;;let punch = function(player, target) &#123; if(isSameTeam(player, target)) &#123; return target; &#125; else &#123; return decrementHP(target); &#125;&#125;;let jobe = Immutable.Map(&#123;name:"Jobe", hp:20, team: "red"&#125;);let michael = Immutable.Map(&#123;name:"Michael", hp:20, team: "green"&#125;);punch(jobe, michael);//=&gt; Immutable.Map(&#123;name:"Michael", hp:19, team: "green"&#125;) decrementHP 、 isSameTeam 和 punch 都是纯函数，所以是引用透明的。我们可以使用一种叫做“等式推导”（equational reasoning）的技术来分析代码。所谓“等式推导”就是“一对一”替换，有点像在不考虑程序性执行的怪异行为（quirks of programmatic evaluation）的情况下，手动执行相关代码。我们借助引用透明性来剖析一下这段代码。 1234567let punch = function(player, target) &#123; if(player.team === target.team) &#123; return target; &#125; else &#123; return decrementHP(target); &#125;&#125;; 因为是不可变数据，我们可以直接把 team 替换为实际值： 1234567let punch = function(player, target) &#123; if("red" === "green") &#123; return target; &#125; else &#123; return decrementHP(target); &#125;&#125;; if 语句执行结果为 false ，所以可以把整个 if 语句都删掉： 123let punch = function(player, target) &#123; return decrementHP(target);&#125;; 如果再内联 decrementHP ，我们会发现这种情况下， punch 变成了一个让 hp 的值减 1 的调用： 123let punch = function(player, target) &#123; return target.set("hp", target.hp-1);&#125;; 总之，等式推导带来的分析代码的能力对重构和理解代码非常重要。事实上，我们重构海鸥程序使用的正是这项技术：利用加和乘的特性。 并行性(Parallelism)最后一点，也是决定性的一点：我们可以并行运行任意纯函数。因为纯函数根本不需要访问共享的内存，而且根据其定义，纯函数也不会因副作用而进入竞争态（race condition）。 并行代码在服务端 js 环境以及使用了 web worker 的浏览器那里是非常容易实现的，因为它们使用了线程（thread）。 柯里化（curry）不可或缺的 curry 我父亲以前跟我说过，有些事物在你得到之前是无足轻重的，得到之后就不可或缺了。微波炉是这样，智能手机是这样，互联网也是这样——老人们在没有互联网的时候过得也很充实。对我来说，函数的柯里化（curry）也是这样。 curry 的概念很简单：只传递给函数一部分参数来调用它，让它返回一个函数去处理剩下的参数。 你可以一次性地调用 curry 函数，也可以每次只传一个参数分多次调用。 12345678910let add = function(x) &#123; return function(y) &#123; return x + y &#125;&#125;let inc = add(1)inc(10)// 11 这里我们定义了一个 add 函数，它接受一个参数并返回一个新的函数。调用add 之后，返回的函数就通过闭包的方式记住了 add 的第一个参数。一次性地调用它实在是有点繁琐，好在我们可以使用一个特殊的 curry 帮助函数（helperfunction）使这类函数的定义和调用更加容易。 123456789101112131415161718const curry = require('lodash/curry')let match = curry(function(what, str)&#123; return str.match(what)&#125;)let replace = curry(function(what, replacement, str)&#123; return str.replace(what, replacement)&#125;)let filter = curry(function(f, ary)&#123; return ary.filter(f)&#125;)let map = curry(function(f, ary)&#123; return ary.map(f)&#125;) 我在上面的代码中遵循的是一种简单，同时也非常重要的模式。即策略性地把要操作的数据（String， Array）放到最后一个参数里 12345678910111213141516171819202122match(/\s+/g, "hello world")// [ ' ' ]match(/\s+/g)("hello world")// [ ' ' ]let hasSpaces = match(/\s+/g)// function(x) &#123; return x.match(/\s+/g) &#125;hasSpaces("hello world")// [ ' ' ]hasSpaces("spaceless")// nullfilter(hasSpaces, ["tori_spelling", "tori amos"])// ["tori amos"]let findSpaces = filter(hasSpaces)// function(xs) &#123; return xs.filter(function(x) &#123; return x.match(/\s+/g) &#125;) &#125;findSpaces(["tori_spelling", "tori amos"])// ["tori amos"]let noVowels = replace(/[aeiou]/ig);// function(replacement, x) &#123; return x.replace(/[aeiou]/ig, replacement) &#125;let censored = noVowels("*");// function(x) &#123; return x.replace(/[aeiou]/ig, "*") &#125;censored("Chocolate Rain");// 'Ch*c*l*t* R**n' 这里表明的是一种“预加载”函数的能力，通过传递一到两个参数调用函数，就能得到一个记住了这些参数的新函数。 不仅仅是双关语／咖喱用 map 简单地把参数是单个元素的函数包裹一下，就能把它转换成参数为数组的函数。 12345let getLength = function(x)&#123; return x.length&#125;let getAllLength = map(getLength) 只传给函数一部分参数通常也叫做局部调用（partial application），能够大量减少样板文件代码（boilerplate code）。 通常我们不定义直接操作数组的函数，因为只需内联调用 map(getChildren) 就能达到目的。这一点同样适用于 sort 、 filter 以及其他的高阶函数（higherorder function）（高阶函数：参数或返回值为函数的函数）。 当我们谈论纯函数的时候，我们说它们接受一个输入返回一个输出。curry 函数所做的正是这样：每传递一个参数调用函数，就返回一个新函数处理剩余的参数。这就是一个输入对应一个输出啊。哪怕输出是另一个函数，它也是纯函数。当然 curry 函数也允许一次传递多个参数，但这只是出于减少 () 的方便。 curry 函数用起来非常得心应手，通过简单地传递几个参数，就能动态创建实用的新函数；而且还能带来一个额外好处，那就是保留了数学的函数定义，尽管参数不止一个。 代码组合（compose）函数饲养12345let compose = function(f, g)&#123; return function(x)&#123; return f(g(x)) &#125;&#125; f 和 g 都是函数， x 是在它们之间通过“管道”传输的值在 compose 的定义中， g 将先于 f 执行，因此就创建了一个从右到左的数据流。 组合看起来像是在饲养函数让它们结合，产下一个崭新的函数。组合的用法如下： 1234567891011121314151617let toUpperCase = function(str)&#123; return str.toUpperCase()&#125;let toLowerCase = function(str)&#123; return str.toLowerCase()&#125;let exclaim = function(str)&#123; return str + '!'&#125;let reverse = function(str)&#123; return str.split('').reverse().join('')&#125;let angry = compose(exclaim, toUpperCase)let shout = compose(exclaim, toUpperCase)console.log(exclaim(toUpperCase("hello")))console.log(shout("hello")) 这个组合中函数的执行顺序应该是显而易见的。尽管我们可以定义一个从左向右的版本，但是从右向左执行更加能够反映数学上的含义——是的，组合的概念直接来自于数学课本。 现在是时候去看看所有的组合都有的一个特性了。 1234567// 结合律（associativity）// let associative = compose(f, compose(g, h)) == compose(compose(f, g), h) // true// 结合律的一大好处是任何一个函数分组都可以被拆开来，然后再以它们自己的组合方式打包在一起console.log(compose(reverse, compose(toUpperCase, exclaim))("world"))console.log(compose(compose(reverse, toUpperCase), exclaim)("world")) 符合结合律意味着不管你是把 g 和 h 分到一组，还是把 f 和 g 分到一组都不重要。 结合律的一大好处是任何一个函数分组都可以被拆开来，然后再以它们自己的组合方式打包在一起。 pointfreepointfree 模式指的是，永远不必说出你的数据。它的意思是说，函数无须提及将要操作的数据是什么样的。一等公民的函数、柯里化（curry）以及组合协作起来非常有助于实现这种模式。 1234567// 非 pointfree，因为提到了数据：wordlet snakeCase = function (word) &#123; return word.toLowerCase().replace(/\s+/ig, '_');&#125;// pointfreesnakeCase = compose(replace(/\s+/ig, '_'), toUpperCase) 这里所做的事情就是通过管道把数据在接受单个参数的函数间传递。利用 curry，我们能够做到让每个函数都先接收数据，然后操作数据，最后再把数据传递到下一个函数那里去。 另外，pointfree 模式能够帮助我们减少不必要的命名，让代码保持简洁和通用。对函数式代码来说，pointfree 是非常好的石蕊试验，因为它能告诉我们一个函数是否是接受输入返回输出的小函数。比如，while 循环是不能组合的。不过你也要警惕，pointfree 就像是一把双刃剑，有时候也能混淆视听。并非所有的函数式代码都是 pointfree 的，不过这没关系。可以使用它的时候就使用，不能使用的时候就用普通函数。 debug组合的一个常见错误是，在没有局部调用之前，就组合类似 map 这样接受两个参数的函数。 12345678// 错误做法：我们传给了 `angry` 一个数组，根本不知道最后传给 `map` 的是什么东西。let latin = compose(map, angry, reverse)latin(["frog", "eyes"])// error// 正确做法：每个函数都接受一个实际参数。let latin = compose(map(angry), reverse)latin(["frog", "eyes"])// ["EYES!", "FROG!"]) 如果在 debug 组合的时候遇到了困难，那么可以使用下面这个实用的，但是不纯的trace 函数来追踪代码的执行情况。 123456789101112let trace = curry(function(tag, x)&#123; console.log(tag, x) return x&#125;)let dasherize = compose(join('-'), toLower, split(' '), replace(/\s&#123;2,&#125;/ig, ' '))dasherize('The world is a vampire')// TypeError: Cannot read property 'apply' of undefinedlet dasherize = compose(join('-'), toLower, trace("after split"), split(' '), replace(/\s&#123;2,&#125;/ig, ' '));// after split [ 'The', 'world', 'is', 'a', 'vampire' ] trace 函数允许我们在某个特定的点观察数据以便 debug。像 haskell 和 purescript 之类的语言出于开发的方便，也都提供了类似的函数。 组合将成为我们构造程序的工具，而且幸运的是，它背后是有一个强大的理论做支撑的。 范畴学（category theory）范畴学（category theory）是数学中的一个抽象分支，能够形式化诸如集合论（settheory）、类型论（type theory）、群论（group theory）以及逻辑学（logic）等数学分支中的一些概念。范畴学主要处理对象（object）、态射（morphism）和变化式（transformation），而这些概念跟编程的联系非常紧密。下图是一些相同的概念分别在不同理论下的形式： 在范畴学中，有一个概念叫做…范畴。有着以下这些组件（component）的搜集（collection）就构成了一个范畴： 对象的搜集 态射的搜集 态射的组合 identity 这个独特的态射 范畴学抽象到足以模拟任何事物，不过目前我们最关心的还是类型和函数，所以让我们把范畴学运用到它们身上看看。 对象的搜集 对象就是数据类型，例如 String 、 Boolean 、 Number 和 Object 等等。通常我们把数据类型视作所有可能的值的一个集合（set）。像 Boolean 就可以看作是 [true, false] 的集合， Number 可以是所有实数的一个集合。把类型当作集合对待是有好处的，因为我们可以利用集合论（set theory）处理类型。 态射的搜集 态射是标准的、普通的纯函数。 态射的组合 这就是本章介绍的新玩意儿—— 组合 。我们已经讨论过compose 函数是符合结合律的，这并非巧合，结合律是在范畴学中对任何组合都适用的一个特性。 123let g = function(x)&#123; return x.length &#125;let f = function(x)&#123; return x === 4; &#125;let isFourLetterWord = compose(f, g) identity 这个独特的态射 让我们介绍一个名为 id 的实用函数。这个函数接受随便什么输入然后原封不动地返回它： 1let id = function(x) &#123;return x&#125; id 函数跟组合一起使用简直完美。下面这个特性对所有的一元函数（unary function）（一元函数：只接受一个参数的函数） f 都成立： 123// identitycompose(id, f) == compose(f, id) == f// true 这就是实数的单位元（identity property）嘛！慢慢理解它的无用性，很快我们就会到处使用 id 了，不过暂时我们还是把它当作一个替代给定值的函数。这对写 pointfree 的代码非常有用。 除了类型和函数，还有什么范畴呢？还有很多，比如我们可以定义一个有向图（directed graph），以节点为对象，以边为态射，以路径连接为组合。还可以定义一个实数类型（Number），以所有的实数为对象，以 &gt;= 为态射（实际上任何偏序（partial order）或全序（total order）都可以成为一个范畴）。范畴的总数是无限的，但我们只需要关心上面定义的范畴就好了。 总结组合像一系列管道那样把不同的函数联系在一起，数据就可以也必须在其中流动——毕竟纯函数就是输入对输出，所以打破这个链条就是不尊重输出，就会让我们的应用一无是处。我们认为组合是高于其他所有原则的设计原则，这是因为组合让我们的代码简单而富有可读性。另外范畴学将在应用架构、模拟副作用和保证正确性方面扮演重要角色。 示例应用声明式代码我们要开始转变观念了，从本章开始，我们将不再指示计算机如何工作，而是指出我们明确希望得到的结果。 与命令式不同，声明式意味着我们要写表达式，而不是一步一步的指示。 以 SQL 为例，它就没有“先做这个，再做那个”的命令，有的只是一个指明我们想要从数据库取什么数据的表达式。至于如何取数据则是由它自己决定的。以后数据库升级也好，SQL 引擎优化也好，根本不需要更改查询语句。这是因为，有多种方式解析一个表达式并得到相同的结果。 1234567// 命令式let makes = []for (i = 0; i &lt; cars.length; i++) &#123; makes.push(cars[i].make)&#125;// 声明式let makes = cars.map(function(car)&#123; return car.make &#125;) 命令式的循环要求你必须先实例化一个数组，而且执行完这个实例化语句之后，解释器才继续执行后面的代码。然后再直接迭代 cars 列表，手动增加计数器，把各种零零散散的东西都展示出来…实在是直白得有些露骨。 使用 map 的版本是一个表达式，它对执行顺序没有要求。而且， map 函数如何进行迭代，返回的数组如何收集，都有很大的自由度。它指明的是 做什么 ，不是怎么做。因此，它是正儿八经的声明式代码。 再看一个例子。 12345678let authenticate = function(form)&#123; let user = toUser(form) return logIn(user) // return logIn(toUser(form))&#125;let authenticate = compose(logIn, toUser) 虽然命令式的版本并不一定就是错的，但还是硬编码了那种一步接一步的执行方式。而 compose 表达式只是简单地指出了这样一个事实：用户验证是 toUser和 logIn 两个行为的组合。这再次说明，声明式为潜在的代码更新提供了支持，使得我们的应用代码成为了一种高级规范（high level specification）。 因为声明式代码不指定执行顺序，所以它天然地适合进行并行运算。它与纯函数一起解释了为何函数式编程是未来并行计算的一个不错选择——我们真的不需要做什么就能实现一个并行／并发系统。]]></content>
      <categories>
        <category>CS</category>
      </categories>
      <tags>
        <tag>Book</tag>
        <tag>JS</tag>
        <tag>FP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DNS知识点]]></title>
    <url>%2F2017%2F11%2F11%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[DNSDNS 是域名系统（Domain Name System）的缩写，它是一种用于 TCP/IP 应用程序的分布式数据库，它提供主机名字和 I P 地址之间的转换及有关电子邮件的选路信息。所谓“分布式”是指在 Internet 上的单个站点不能拥有所有的信息。每个站点（如大学中的系、校园、公司或公司中的部门）保留它自己的信息数据库，并运行一个服务器程序供 Internet上的其他系统（客户程序）查询。 DNS 命名方式中，采用了分散和分层的机制来实现域名空间的委派授权，以及域名与地址相转换的授权。通过使用 DNS 的命名方式来为遍布全球的网络设备分配域名，而这则是由分散在世界各地的服务器实现的。 DNS 工作流程域名服务分为客户端和服务器端，客户端提出请求，询问一个 Domain Name 的 IP 地址，服务器端必须回答客户端的请求。本地 DNS 首先查询自己的数据库，如果自己的数据库中没有对应的 IP 地址，则向本地 DNS 上所设的上一级 DNS 询问，得到结果之后，将收到的结果保存在高速缓冲区，并回答给客户端。标识字段由客户程序设置并由服务器返回结果。客户程序通过它来确定响应与查询是否匹配。16 bit 的标志字段被划分为若干子字段 DNS 协议标志中每一位的含义如下： QR：是 1 bit 字段，0 表示查询报文，1 表示响应报文。 Opcode：报文类型，是一个 4 bit 字段，通常值为 0（标准查询），其他值为 1（反向查询）和 2（服务器状态请求）。 AA：是 1 bit 字段，表示“授权回答（authoritative answer）”，如果此位为 1，表示服务器对问题部分的回答是权威性的。 TC：是 1 bit 字段，表示“可截断的（truncated）”。使用 UDP 时，它表示当应答的总长度超过 512 字节时，只返回前 512 个字节。 RD：是 1 bit 字段，表示“期望递归（recursion desired）”。该比特能在一个查询中设置，并在响应中返回。这个标志告诉名字服务器必须处理这个查询，也称为一个递归查询。如果该位为 0，且被请求的名字服务器没有一个授权回答，它就返回一个能解答该查询的其他名字服务器列表，这称为迭代查询。 RA：是 1 bit 字段，表示“可用递归”。如果名字服务器支持递归查询，则在响应中将该比特设置为 1。 Zero：随后的 3 bit 字段必须为 0。 Rcode：是一个 4 bit 的返回码字段。通常的值为 0（没有差错）和 3（名字差错）。名字差错只有从一个授权 DNS 服务器上返回，它表示在查询中制定的域名不存在。随后的 4 个 16 bit 字段说明最后 4 个变长字段中包含的条目数。对于查询报文，问题（question）数通常是 1，而其他 3 项则均为 0。类似地，对于应答报文，回答数至少是 1，剩下的两项可以是 0 或非 0。 DNS 抓包这里DNS报文首部前16bit格式如下： DNS服务器通过Transaction ID来分辨同台主机的不同请求。 我打开浏览器访问百度 我请求百度dns的字段如下： 这里的前16bit是0x0100 而dns的响应如下： 如果请求的域名不存在，则如下：]]></content>
      <categories>
        <category>CS</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>DNS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017-11-5小结]]></title>
    <url>%2F2017%2F11%2F05%2F2017-11-5%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[10月10月就这样过去了。当我觉得这样的10月是多么繁忙的时候，11月时候的我告诉自己，其实大部分时间是自己折腾自己罢了，而且毫无意义。 10月的作业债11月总是要还的。现在回想起来，好像是各个方面都在学习，但如蜗牛一般，于是乎，忘记自己走到了哪里。唯一欣喜的是，我开始接受其他书籍的洗礼了。之前从同学那儿借来了东野圭吾的《解忧杂货店》，每天熄灯后一小时都在咀嚼此书。跟计算机书籍比起来，这样的书确实容易读很多，不费太多的时间就能通读一遍，而且相当的有收获。当然，如果一本书没有从思想层面影响你，或者没有改变你的某个观念，或者没有改变你的一个习惯，或者没有引起你一点儿的思考，这样的书大概是不值得去读的。如某些读者所述，这本书确实在某方面治愈了我，让我感觉到人间一些温情，人与人之间不可思议的联系，很多无意义之事其实蕴含意义，不论好坏。其实我极力追求做一个足够纯粹的人，而我理解的纯粹是指一个的品质而不是性格。这本书里的人有些已经达到了我想追求的纯粹了。而我对纯粹的一些理解就是看淡大部分事情，对人的追求做到坦实真诚，对事的追求做到实事求是，对物的追求做到己所不欲。当然这里的追求比较广义。可能思想更接近于道家，当然我完全不懂这些思想家的大道理。讽刺的是，我一直没有做好，不然可能也不会写这些了。 刚刚提到走着走着忘记走到了哪里。其实就是没有把握本心的缘故吧。原因无外乎就是意志力的不坚定，间接性情绪调节不良。10月份做了什么自己满意的事情？有把该完成的都及时正确地完成吗？除了课内，课外又扩展了多少？写到这里，能回答上的也就是10月份开始的跑步吧，频率还是挺高的，1周能跑4，5次。作业能提早完成的后来质量都出了问题……没有提早完成的基本拖到deadline。满意的事情，还真没有，我这个人不容易对事情感到满足，因为很容易就松懈下来，但实际上大部分时候我松懈的原因是因为失去了耐性或者热情，在这点上，还不如中学时代的自己，中学的我虽然在各方面的都很平庸（虽然现在依然），但做什么事情都饱含热情，是真的喜欢生活，喜欢与人交流。 11月11月的第一周，基本宣告了学期过半。有什么收获自己心里清楚，没有完成什么自己心里清楚，还想做什么自己心里也清楚。人啊，总是会把生活中大部分事情当作理所当然，所以才不容易改变。我极力改变这样的思维，如何看待周围是一件非常严肃，非常需要思考，却被人忽略的事情啊。许多时候不能总保持着“我现在的这样状态真的很好啊”的想法，于是乎还想证明给其他人看。可能还有一种思考角度，我尝试往一个可能更好的习惯学习，如果反而没有帮助，再退回来。前者是以不变应万变，后者是以变应万变。最近开始尝试读一些文学作品，也是从此角度出发。一直读专业相关的作品或资料，当然对学习有很大帮助，但是人很难再脱离自己给自己建造的围墙了，它们既是梯子也是围墙啊。再谈到待人之事，摈弃过去看人优缺点的角度，而尝试以对方的角度看待对方自己，才能做到了解他人吧。 然后是想反思一下一直以来的一些学习缺陷。看比较费理解的书，还是容易倾向于不去深思，原因嘛，就是怕麻烦，觉得这是一场时间黑洞的无意义之旅，然后是有点急于求成，过分看轻一些基本之理。不过还是要承认下自己的记忆力不行，对经历的事情还算有正常的记忆力，但对文字和视频的记忆力特别差，目前感觉原因尚不明朗。 昨天把从同学那里借到了《独唱团》读了半本，今天才查了一下，原来是只出过一期，这个期刊本身还颇有一些故事。当然本身的内容也是不错，作者大多是80后70后吧？聚焦于上个世纪的中国，讲述各种经历，每篇都能透入一些观点，或批判或赞扬。 而我已经好久没有看过这样的散文、诗句、问答、半小说半叙事的文章了。看来我还是不排斥文学作品的，高中每周也会去买杂志跟期刊，看看感兴趣的内容，喜欢看里面待人处事的观点。我敢说我现在的思想跟性格被高中时代接触的内容严重影响了，即使现在我已经几乎想不起那些内容。只记得，他们在谈论生活，他们在谈论文化，透露着思想…… 里面的图片也总是能触动许多，有时候能盯着图片一分钟，思考着什么我早已经忘记了，大概在思考着他们所思考的事情。我想这本书，我应该也会尝试读完吧，感觉把过去的习惯捡起来，就好像在跟老朋友拥抱一般，非常的踏实。 发现写到这里，已经过去整整一小时了。大概我的脑子里又本能的计算着花一小时做这些事情的利弊了。看过的许多作品，我发现我都不能很好地把他们表达出来，或者借用一下，我想有机会写写探讨某些作品本身的博客，目前来说可能还不合适，阅历可能还不够。 已经九点半了，面对着书架上近百本的书，其实心里滋味很难描述。]]></content>
      <categories>
        <category>Life</category>
        <category>CS</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Install MySQL5.7]]></title>
    <url>%2F2017%2F10%2F26%2FInstall-MySQL5-7%2F</url>
    <content type="text"><![CDATA[大二数据库原理的作业，发现还存在电脑里，于是稍微整理下放到博客里。 windows下mysql安装我下载的是zip压缩包，假设解压到D:\software目录下，官网下载的只有服务端和简单的shell，可以去搜索Navicat客户端，支持多种数据库管理系统，下载MySQL客户端，也支持MariaDB使用。 注意：Navicat是收费的，免费使用14天。你还可以使用其他客户端，比如phpAdminMySQL。 mysql的简单配置mysql的目录结构如下 环境变量需要配置bin目录，mysql的程序都在bin目录下，所以环境变量是在PATH变量上新增D:\software\mysql\bin。 各个版本目录可能不同，data目录存放系统数据，一开始应该没有，需要自己新建。 另一个重要的是my.ini文件(或my-default.ini)，需要进行一些简单的配置才能使用。 123456789101112131415161718[mysql]# 设置mysql客户端默认字符集default-character-set = utf8[mysqld]#skip_grant_tables 这个先不要，这是忽略权限。#设置3306端口port = 3306# 设置mysql的安装目录basedir = "D:/software/mysql-5.7.16-winx64"# 设置mysql数据库的数据的存放目录datadir = "D:/software/mysql-5.7.16-winx64/data"# 允许最大连接数max_connections = 200# 服务端使用的字符集默认为8比特编码的latin1字符集，我们改为通用的utf8character-set-server = utf8# 创建新表时将使用的默认存储引擎default-storage-engine = INNODB 常用的2个程序是mysql和mysqld，需要给服务端设置一些配置，如上，语法跟作用一目了然。 windows下运行mysql需要使用cmd管理员权限，使用net start mysql 启动mysql服务，net stop mysql 关闭服务。 一开始需要使用root帐号登录，不需要密码，在命令行下使用mysql -u root -p登录，进入shell后可以使用help命令查看简单的用法。 具体其他使用方法请查看文档手册或者搜索引擎。 Navicat 客户端简单使用按照安装包的引导安装即可。 注意使用期限是14天。 Navicat界面比较简洁，也只提供基本的功能，如需要使用强大的功能可能需要购买或使用其他客户端。 Navicat窗口帮助下可以打开本地中文文档，可以查看各种数据库的使用。 在文件下建立数据库连接写好帐号密码即可连接（不要忘记先在cmd管理员权限下启动mysql服务）。 然后可以简单的开始操作DBMS了。 linux ubuntu 命令行下mysql安装我选择最简单的安装方式 如上，用sudo apt install mysql-server-5.7下载 当然你可以先用图形界面找到需要的版本的包或者路径，用wget等工具下载，这里不详细说明，总体思路跟windows下是一致的。 用 whereis mysql 命令查看mysql等命令已经添加到环境变量中。 如图，用sudo service mysql start 启动，即使远程连接断开也在后台运行，mysql -u root -p登录root用户连接mysql，可以用ps -ef | grep mysql 来查看。 登录以后，在mysql下用show variables like &#39;character% 可以查看一些字符相关的环境配置，发现很多是latin1编码，我们需要改为utf8编码 我们来到/etc/mysql目录下，这里存放许多mysql的配置。我们修改一些配置sudo vim mysql.conf.d/mysqld.cnf，在[mysqld]下配置character-set-server = utf8，当然在这里你还可以修改一些其他配置。 继续修改另外一个配置sudo vim conf.d/mysql.cnf，在[mysql]下配置default-character-set = utf8，这样重启mysql后字符编码就改变了如下： 以上，最基本的配置就完成了，大概作为学习用途已经足够了，再复杂的配置得参考相应的资料了。]]></content>
      <categories>
        <category>CS</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>Database</tag>
        <tag>Configure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VAET阅读稿]]></title>
    <url>%2F2017%2F10%2F11%2FVAET%E9%98%85%E8%AF%BB%E7%A8%BF%2F</url>
    <content type="text"><![CDATA[VAET: A Visual Analytics Approach for E-transactions Time-Series 电子交易时间序列的一种可视化分析方法 Cong Xie, Wei Chen, Member, IEEE, Xinxin Huang, Yueqi Hu, Scott Barlowe, and Jing Yang 创新实践的论文阅读作业，翻译了主要内容，加上部分自己的理解并修改部分内容，保留了部分原文，有时候阅读原文更好理解。全文按照论文组织的方式编写，主要是为了理清楚论文的主要思想，并可以简单地给其他人做宏观上的解释，达到基本的教学目的。此文将作为ppt的前稿。 INTRODUCTION 介绍The E-transaction time-series contains transactions among multiple users in a time range. Each record contains a time stamp, the IDs of the seller and buyer, and the associated attributes of the commodities. Each record is an atomic element representing an online transaction among a seller and a buyer. 电子交易时间序列包含时间范围内的多个用户之间的交易。 每个记录都包含时间戳，卖方和买方的ID以及相关联的商品的属性。每个卖方和买方之间的网上交易记录都是一个原子元素。 在时间上下文中分析电子交易时间序列至关重要了解交易行为，学习用户偏好和发现时间趋势。 通过面试分析师发现以下一些问题常常很难回答： 卖方的多个交易之间的时间和上下文关系是什么？比如短时间内有个卖家有大量的交易，而且可能来自某个买家，分析师需要发现这样的交易的各种属性。 最常见的交易模式是什么？比如平时工作日交易比较稀疏，但是圣诞节交易比较频繁。 如何识别有趣的交易？比如买家勾结来加速影响卖家的信用，一旦这样的模式定义好了，分析师需要在相关的大量数据集中找到特定属性值来挖掘这样的交易。 如何在上下文检测某个交易？比如小额购买巨量的商品可能是个假的交易，利用它来提升卖家排名。为了确认交易是假的，分析师需要将交易与买方和卖方的信息关联。 我们认为，自动数据挖掘过程在回答上述问题时并没有足够的灵活性和准确性。因此迫切需要视觉分析方法，使分析人员能够通过集成计算能力，人类知觉能力和领域知识，通过即时视觉反馈灵活地形成和测试假设。 目前还没有适用于上述情况的可视化方法。多变量时间序列可视化的现有研究工作主要集中在总结多维度的全局和/或时间趋势或发现个体维度的模式，如Sparklines。 因此提出了一种新颖的视觉分析系统，称为电子交易时间序列的视觉分析（VAET），旨在探索电子交易时间序列，以便在时间上下文中分析多个用户之间的交易模式。VAET有如下2个主要的可视化分析组件： Overview: This component helps the analysts effectively identify salient transactions from a large dataset. 该组件帮助分析师在大量数据集中快速找到突出的交易。VAET使用概率决策树学习器首先计算每个交易的显着性值，以揭示其与分析目标的相关性（例如，作为假交易的可能性）。然后，显着值显示在一个称为显着时间映射（TOS）的像素方向的显示中。该映射提供了一个工作空间来探索和选择不同时间粒度的潜在有趣的交易; Detail view：This component allows the analysts to conduct detailed examination on interesting transactions for insights. 该组件允许分析人员对感兴趣的交易进行仔细检查，以获取见解。特别地，从概述中选择的交易使用称为KnotLines的新的视觉隐喻来显示。协调TOS映射和KnotLines，以便分析人员可以快速识别来自大型数据集的有趣事务。 A case study and a user study with a real online transaction dataset demonstrated that VAET was effective in supporting a variety of analysis tasks.真实在线交易数据集的案例研究和用户研究表明，VAET有效支持各种分析任务。 VAET的主要贡献包括： 视觉分析系统，允许分析人员在时间上有效地分析大型电子交易时间序列; 从大型数据集中检测和可视化突出事务的方法; 一种新颖的视觉隐喻，用于紧凑地放置和编码特征属性以及多用户事务的时间和上下文相关性 RELATED WORK 相关工作Visual Analysis of E-transaction Data 电子交易数据的可视化分析以下是之前一些人做的过相关研究（寻找合适的可视化方法来展示交易数据） The transaction data contains various types of attributes, such as numerical, temporal and categorical. The Sparklines [23] can be used to visualize multiple trends in financial data. Liu et al. [11] proposed a visualization system called SellTrend for analyzing airline travel purchase requests. WireVis [3] was proposed to search on predefined patterns in large wire transaction datasets. Visual analytics approaches have been proposed to explore web clickstreams of online transactions [26]. Our approach is among the earliest visual analytics approaches for the exploration of temporal and contextual connections in multiuser transactions. Transaction data often have multi-dimensional attributes. Analyzing them often requires the integration of well-designed data mining models. Probabilistic models are employed to model user behavior [12], resulting in user clusters. This scheme has been successfully applied to classify E-transaction data into different types [2]. Association analysis is another widely used model for transaction data. Hao et al. [7] proposed the DAV system to visualize the relationships of associated products. Visual Analysis of User Behavior Time-Series 用户行为时间序列的可视化分析以前有很多关于用户行为时间序列分析和可视化的作品。这里我们只总结最相关的一些，并将它们分类为分析个人行为，用户交互和组行为的技术。 Temporal Individual Behavior Patterns Many visualization approaches designed to analyze user behavior data are focused on exploring the temporal behavior patterns of individuals.旨在分析用户行为数据的许多可视化方法都集中在探索个人的时间行为模式。TimeSearcher [8] allows users to select interesting time-series using a rectangular query region. LifeLines [16] visualizes health-related incidents of patients along a timeline. Most previous works utilize high-dimensional visual exploration tools such as parallel coordinates [4] to explore extracted patterns. Density-based display techniques [6], [10] are capable of showing large time-series datasets for real-time monitoring. Additional visual exploration techniques include time trajectory [21] and [13]. User Interaction Patterns conventional solutions consider the user network as a social network and analyze its global structure常规解决方案将用户网络视为社会网络，并分析其全局结构。Sallaberry et al. [20] provide an overview of dynamic network evolution over time. Other approaches emphasize the user interaction characteristics such as email connections [25] and instant messages [27]. However, these methods are focused on the structural changes rather than the temporal variations of the interactions. Other approaches aim to reveal the relationships among multiple users in a temporal context. For instance, Storyline [22] shows the narrative threads that form a plot or a subplot in works of fiction. The history flow approach successfully reveals author collaboration patterns [24]. Code Swarm [15] visualizes the animated histories of software project evolution. VAET reveals both the temporal patterns of multi-user behavior and their atomic level correlations. It improves the above approaches by allowing the analysts to explore a large number of transactions at different granularities. Problem definition 问题定义 Multi-user transaction data is a special type of user behavior data with a focus on characterizing raw, detailed, and subtle inter-user transactions. An E-transaction time-series dataset contains information about each E-transaction, including information about transaction time, the buyer, and the seller. Each E-transaction records a transaction between a buyer and a seller. 多用户事务数据是一种特殊类型的用户行为数据，重点是描述原始，详细和微妙的用户间交易。电子交易时间序列数据集包含每个电子交易的信息，包括有关交易时间，买方和卖方的信息。 每个电子交易记录买方和卖方之间的交易。 一般来说，一个电子交易包含以下属性： User information includes the IDs and other information about the buyer and the seller who make the transaction, e.g., their age group, gender, and location. Transaction information includes the time stamp and other information about the commodities, e.g., the payment amount, the number, and the sales category of the commodity. The above attributes can be numerical, ordinal, categorical, textual, or temporal.上述属性可以是数字，序数，分类，文本或时间。 分析师通常通过一系列低级别任务进行复杂的任务。 这些任务通常关注卖方的行为，例如： 识别感兴趣的时段和/或销售类别。 识别具有特定属性的有趣模式的交易（例如，支付金额≥500）并检查其详细信息。 识别具有有趣交易模式的卖家，例如卖家以小额付款金额进行频繁交易。 检查特定卖家的交易模式 The analysts usually conduct a complex task through a set of low level tasks. These tasks typically focus on the behavior of the seller, such as: T1 Identifying time periods and/or sales categories of interest. T2 Identifying transactions with interesting patterns in specific attributes(e.g., payment amount ≥ 500) and examining their detailed information. T3 Identifying sellers with interesting transaction patterns, such as a seller making frequent transactions with small payment amounts. T4 Examining the transaction patterns of a specific seller 我们使用术语显着定量地描述交易与分析师定义的目标的相关程度。根据调查，识别和审查突出交易是电子交易时间序列探索中至关重要但具有挑战性的任务。通常，分析师需要通过迭代查询数据集并检查检索到的事务之间的属性值和关系来手动识别突出事务。此外，分析师经常需要检查突出交易以及用户的历史数据等信息，以证明其行为或揭示有趣的模式。这个过程通常是费力和乏味的。VAET旨在简化此过程，提高整体运行效率。（VAET is designed to ease this process and improve the overall operation efficiency.） APPROACH OVERVIEW 方法概览The goal of VAET is to identify and explore interesting transactions by selecting those with high saliency and studying them. This is accomplished by integrating the capabilities of both data mining and visualization techniques within the following iterative visual exploration pipeline. VAET的目标是通过选择具有高度显着性并研究它们来识别和探索有趣的交易。这是通过将数据挖掘和可视化技术的功能集成在以下迭代视觉探索流程中来实现的。 步骤1，使用决策树的显着计算：从每个事务中提取一组特征。分析人员将某些交易的功能手动标记为训练数据。使用这些特征，在训练数据上构建概率决策树学习器。然后用它来产生每个未标记交易的显着值（图2（b））。 步骤2，使用TOS映射进行浏览和选择：所有事务的显着性值映射到紧凑的基于密度的生存时间（TOS）映射。在此映射中，交易按时间和类别排序，并以颜色对应于显着值的像素表示。分析师可以交互地探索映射，调查全球分布和地域格局，并选择根据这个观点的显着性值，有趣的交易。（图2（c））。 步骤3，使用KnotLines进行详细分析：分析师选择的交易通过一种新颖的视觉隐喻KnotLines可视化，允许研究多个属性和上下文连接（图2（d））。分析人员确定为突出事务的交易可以被标记并反馈到步骤1以继续迭代过程（图2（e））。 Step 1 Saliency computation with decision tree: A set of features are extracted from each transaction. The analysts manually label the features of some transactions as the training data. Using these features, a probabilistic decision tree learner is constructed upon the training data. It is then employed to produce the saliency values for each unlabeled transaction (Figure 2 (b)). Step 2 Browsing and selection using the TOS map: The saliencyvalues of all transactions are mapped to a compact, density-based Time-Of-Saliency (TOS) map. In this map, transactions are ordered by time and categories and represented by pixels whose colors correspond to saliency values. The analysts can interactively explore the map, investigate the global distribution and local patterns, and select interesting transactions according to the saliency values from this view. (Figure 2 (c)). Step 3 Detailed analysis using KnotLines: The analyst-selected transactions are visualized with a novel visual metaphor, KnotLines, that allows the study of multiple attributes and contextual connections (Figure 2 (d)). The transactions identified as salient by the analysts can be labeled and fed back into Step 1 to continue the iterative process (Figure 2 (e)). 分析人员可以通过调整标记的数据集，导航映射和探索有趣的交易来迭代地循环上述步骤。 TOS映射和KnotLines可视化提供可扩展的探索，如时间间隔选择和详细审查。 SALIENCY COMPUTATION WITH DECISION TREE 决策树的显著性计算计算显着性值本质上是上下文感知和任务定位的。 对于许多任务，显着性值不能直接从事务属性导出。 例如，当分析师搜索异常交易时，往往需要考虑卖方的交易频率。 让分析人员手动指定每个交易的显着性值也是不切实际的。 因此，我们建议通过定义和计算一组交易的特征来计算每个记录的显着性值。特别地，我们的方法通过概率决策树计算显着值作为概率估计问题。 我们选择决策树，因为它可以处理连续和分类的属性，很容易解释。 决策树最初由一组分析师确定的训练数据的特征构建。 将决策树应用于每个未标记事务的特征，产生的概率范围为0到1，用作底层事务的显着值。 分析师手动标记为交易的交易可以在随后的分析中添加到训练数据集中（图2（e））。 Feature Extraction 特征提取VAET计算一组分析师指定的每个事务的时间和上下文特征作为一组特征。 一般来说，定义了三种类型的特征： 基本特征 确定交易是否有趣的一个直接方法是使用指定属性的值作为基本特征，例如商品的支付金额。另外，分析人员可以定义新的属性。例如，如果卖家在分析师给出的有趣的列表中，则他或她被视为显着的卖家，如图3所示。这些属性的集合构成一组基本功能。 文本功能 交易可以包含文本信息，例如商品的评论。 VAET检查文本信息是否包含分析师指定列表中的敏感词。分析人员保留一个字典，用于从过去几个月手动收集敏感的词汇和短语。例如，在一种欺诈交易中，买家希望尽快回收现金。 “现金回馈”是一个敏感的短语。敏感词在不同的情况下有所不同，可以视为文字特征。 时间特征 交易序列的时间模式对于识别数据集中的有趣模式至关重要。例如，卖方在时间间隔内的交易金额表示他或她的受欢迎程度。然而，以传统的决策树方法难以发现面向时间的关系。为了解决这个问题，VAET使用卖方在每个时间间隔的交易频率作为衡量时间趋势。时间间隔的大小取决于数据收集配置。 Basic Features One straightforward way to determine whether a transaction is interesting is to use the values of specified attributes as basic features, such as the payment amount of a commodity. In addition, the analysts can define new attributes. For example, if a seller is in the interesting list given by the analyst, he or she is considered as a salient seller, as shown in Figure 3. The collection of these attributes constructs a set of basic features. Textual Features A transaction may contain textual information, such as the comment of a commodity. VAET examines whether the textual information contains sensitive words in a analyst-specified list. The analysts keep a dictionary for sensitive words and phrases collected manually from the past several months. For example, in a kind of fraud transactions, the buyers want their cash back as soon as possible. “cash back” is a sensitive phrase here. Sensitive words vary in different situations, and can be regarded as textual features. Temporal Features Temporal patterns of a sequence of transactions are essential for identifying interesting patterns in the datasets. For example, the transaction amount of a seller in a time interval indicates his or her popularity. However, time-oriented relations are difficult to discover with conventional decision tree approaches. To address this problem, VAET uses the transaction frequency of the seller in every time interval as a measure of the temporal trend. The size of the time interval depends on the data collection configuration. Estimating saliency using Probabilistic Decision Tree 使用概率决策树估计显着性决策树最初使用训练数据集构建，该数据集由分析师标记的交易的提取功能组成。 如图2（e）所示，可以通过添加分析员标识的事务，在可视化探索过程中手动更新训练数据集。 在我们的方法中，使用完善的C4.5算法从训练数据中自动构建决策树，其根据特征将训练集递归地分解为子集。 在决策树中（参见图4的示例），叶子节点表示类（显着或非显着2个类别），内部节点对应于特征。 在每个内部节点处，C4.5根据产生最高归一化信息增益（highest normalized information gain）的特征将样本分解为子集，并将特征分配给该节点 概率根据决策树叶上的交易进行估计。 我们将FP表示为叶上的假阳数，T P表示真阳数（见图5中的混淆矩阵）。 叶上的概率分布估计由下式给出: P(y|x) = TP / (TP + FP) TIME-OF-SALIENCY MAP: BROWSING A LARGE SET OF TRANSACTIONS 显著性时间映射：浏览大量交易集 Generation of Time-Of-Saliency Map 生成显著性时间映射TOS映射是基于2D密度的展示，沿着水平轴的时间和由销售类别（例如，“电子配件”和“衣服”）组织的垂直轴TOS映射均匀分割成行，每个行代表一个销售类别。在图（a）中，由蓝色框突出显示的颜色矩形提供了垂直轴上类别的视觉索引。此外，每行按照时间间隔水平分割。根据其时间戳和销售类别每个事务被投射到相应的单元。投射到同一个单元格的所有事务的显着值相加，并将总和映射到单元格的颜色。色彩映射可以使用默认颜色标度或分析器指定的色标。所得到的TOS映射可视化地为分析任务编码事务的相关性。黑暗区域意味着一组潜在有趣的交易。特别地，连续的暗带表示在一段时间内相应的销售类别中的高度突出的交易（参见图1（a）中的TOS映射中的所选区域）。 Time-Of-Saliency Exploration 探索显著性时间映射TOS映射视图中提供了以下交互，可用于完成第一个任务（即识别感兴趣的时段和/或销售类别。） 时间窗口 TOS映射以分析人员可调节的时间间隔显示交易。可以使用额外的时间窗口小部件来定位视图的特定区域以进行进一步和详细的研究。分析师可以在时间选择栏上单击并拖动以设置TOS映射的时间窗口，如图5（a）中TOS映射顶部的突出显示。图5（b）显示了分析师设置时间窗口后的TOS映射。 感兴趣的区域 分析师可以点击类别索引（图5（a）中的蓝框）来选择同一类别的交易。 也可以使用套索工具来选择有趣的区域。 当选择一个区域时，将出现一个浮动文本框以显示有关该区域的信息。 所选交易的详细信息可以在第7节中描述的KnotLines视图中进一步可视化和探索。此外，还提供了一个条形图视图（图1（c））以显示所选数据中的类别的销售量。 Time Windowing The TOS map shows the transactions in an analyst-adjustable time interval. An additional time windowing widget can be used to locate a specific region of the view for further and detailed study. Analysts can click and drag on the time selection bar to set the time window of the TOS map, as highlighted over the top of the TOS map in Figure 5 (a). Figure 5 (b) shows the TOS map after the analyst sets the time window. Region-Of-Interest Selection The analysts can click on the category index (the blue box in Figure 5 (a)) to choose the transactions of the same category. A lasso tool can also be used to select interesting regions. When a region is selected, a floating text box will appear to show the information about the region. The detailed information of selected transactions can be further visualized and explored in the KnotLines view described in Section 7. In addition, a bar chart view (Figure 1 (c)) is provided to show the sales volume of categories in the selected data. KNOTLINES: EXAMINING TRANSACTIONS IN DETAIL 详细审查交易KnotLines允许分析师对从TOS映射中选择的显着交易进行详细分析。它旨在解决任务2到任务4。KnotLines可视化显示两种类型的信息：属性和交易的时间趋势。 Data Organization and Visual Layout 数据组织和视觉布局为了研究交易之间的属性相似性和时间相关性，所选择的交易集被组织成一个三级分层树（图6）。 首先，我们使用矩阵表来可视化事务的组织，如图7（a）所示。 一级 整个选定的交易集根据不同的卖家分为N组（1级）。图7（a）中的每一行代表一个组。一组包含卖家的所有交易。这些组沿垂直轴从上到下列出。 二级 一组中的交易根据其时间戳进一步分为子组（2级）。图7（a）中的水平轴表示时间。每行分别对应于M个时间间隔的时间轴上的M个正方形。所有间隔的长度相同，可以调整以探索不同粒度的数据。属于相同时间间隔的卖方的交易形成一个子组（2级）。 三级 根据销售类别（例如，“图书”），将一个子组进一步分为（级别3）。在图7（a）中，每个正方形被分割成K个细胞，每个细胞代表一个部分。同一部分的交易由同一卖方作出，在同一时间间隔内进行，属于同一销售类别。 PS：即三级严格递增，一级是属于同一个卖家，二级属于同一个卖家某段时间，三级属于同一个卖家某段时间的某类销售类别。分别按行，按列，按细胞划分。这个是交易集逻辑上关系，而图7是一种为了方便分析而设计表示该关系的可视化方法。 Level One The whole selected transaction set is divided into N groups (level 1) according to different sellers. Each row in Figure 7 (a) represents a group. A group contains all transactions of a seller. The groups are listed from top to bottom along the vertical axis. Level Two The transactions in a group are further divided into subgroups (level 2) according to their time stamps. The horizontal axisin Figure 7 (a) represents the time. Each row is split into M squaresalong the time axis which correspond to M time intervals. The lengthsof all intervals are the same and can be adjusted to explore the data at different granularities. Transactions of a seller which fall into the same time interval form a sub-group (level 2). Level Three A subgroup is further divided into sections (level 3)according to the sales categories (e.g., “Books”). In Figure 7 (a), each square is segmented into K cells, each of which represents a section. Transactions in the same section are made by the same seller, take place in the same time interval, and belong to the same sales category. 因为大部分卖家的交易量在一天的时间内可能很高，因此图7（a）所示矩阵中的交易密度可能非常稀疏。 另外，组N的数量可能很大（例如，100万）。 为了使探索更加有效，矩阵式布局应重新设计为更紧凑。 VAET采用一个简单的两步启发式方案，对每个组进行操作。 在第一步中，删除第一个非空子组之前和最后一个非空子组之后的空子组。 这个步骤导致许多组仅覆盖水平空间的一小部分，因为它们中的大多数具有短的时间跨度。 为了增加空间效率，在第二步中启发式优化组的布局。 迭代布局策略用于满足以下原则： 整洁：组不应重叠; 紧凑型：空间利用率高; 代表：重要群体应优先展示 VAET employs a simple two-step heuristic scheme that operates on each group. In the first step, empty sub-groups before the first nonempty sub-group and after the last non-empty sub-group are removed. This step results in many groups that only cover a small portion of the horizontal space, because most of them have a short time span. To increase the space efficiency, the placement of the groups is heuristically optimized in the second step. An iterative layout strategy is used to satisfy the following principles: Uncluttered: groups should not overlap; Compact: space utilization should be high; Representative: important groups should have a display priority 分析人士指出， 这种设计（即图7） 有几个主要缺点： 由于部分可能包含数百个交易，可视化严重凌乱。 分析师建议在同一部分内汇总交易; 分析师认为，紧凑的布局是空间效率高的必需品。 然而，他们很难从这个视图来识别同一卖家所做的交易。需要额外的视觉属性来强调这一重要关系; 在此视图中没有提供有关交易的重要信息，如付款金额，交易是否缺少值，以及是否经常发生相同的交易。 The analysts pointed out that there were several major drawbacks in this design: (1) The visualization was seriously cluttered since a section may contain hundreds of transactions. The analysts suggested aggregating transactions within the same section; (2) The analysts agreed that the compact layout was necessary for high space efficiency. However, it was difficult for them to identify transactions made by the same seller from this view. Additional visual attributes were desired to emphasize this important relationship; (3) Important information about the transactions such as payment amount, whether a transaction had missing values, and if identical transactions occurred frequently, was not presented in this view. KnotLines （结线）为了解决上述问题，我们设计了一个增强的视觉隐喻调用KnotLines。它受到音乐符号的启发，这可以被看作是一个改进的散点图，它沿着时间轴放置不同类型的点（音符）。它是时间序列（例如节拍和节奏）及其连接的复杂视觉表示。 这里就不详细说明了，具体见表2。 表2 是Knotlines的可视化各个部分的含义| 图形部分 | 含义||—-|—-|| 视觉编码 | 交易数据|| 一个knotline | 同一卖家在不同时间的交易（一组）|| 一个knotbunch | 同一卖家在一段时间间隔内进行交易（一个子组）|| 茎长 | 在一段时间内同一卖家的交易总支付金额|| 一个结 | 在一段时间内，同一卖家与同名销售类别的交易（一节）|| 结的颜色 | 该结点的销售类别|| 结的大小 | 该结点的商品数量|| 一个未填充的结点 | 与卖方或买方位置异常的交易| Visual Exploration 可视化探索除了布局模式的规范和详细结点的调查之外，KnotLines还提供了一套用于分析多个结线的交互。 显着调制 KnotLines视图中显示的每个事务都包含一个显着值。 分析师可以显示从2D TOS映射中选择的所有交易，或仅显示显着值大于给定阈值（例如，0.8）的所选交易。 当视图中显示许多节线时，此过滤操作非常有用，因此它支持T2和T3。 图9显示了显着性调制的影响。 查看导航 KnotLines视图可以水平放大，以清楚说明，这对T2和T3有帮助。 时间间隔的长度将相应调整。 分析师可以垂直或水平滚动查看更多的knotlines。 兴趣选择结 我们可以通过单击或使用套索工具拖动来选择一组结。 指定结时，会以黄色的圆圈突出显示。 由同一买家制造的相关结也用灰色的圆环突出显示分析师的注意力（图1（b））。 将出现一个浮动文本框，显示所选结的详细信息，如卖方的位置，支付金额和销售类别。 分析员可以在详细视图（图1（e））中的所选结（部分）中检查交易的信息（例如，买方和卖方的位置，子类别和商品编号），其中 是为T2设计的。 对T4有帮助的统计视图（图1（f））用于提供所选结的统计信息，如交易频率和卖方支付金额的趋势。 标签 当特定交易被分析师识别为突出显示时，可以将其添加到标记数据中进行迭代视觉分析和探索。 Saliency Modulation Each transaction shown in the KnotLines view contains a saliency value. The analysts can show either all transactions selected from the 2D TOS map or only the selected transactions whose saliency values are larger than a given threshold (e.g., 0.8).This filtering operation is useful when there are many knotlines shown in the view, so it supports T2 and T3. Figure 9 demonstrates the effect of saliency modulation. View navigation The KnotLines view can be zoomed horizontally for clear illustration, a function helpful for T2 and T3. The lengths of the time intervals will be adjusted accordingly. Analysts can scroll vertically or horizontally to see more knotlines. Knots of Interest Selection We can select a set of knots by clicking, or dragging using a lasso tool. When a knot is specified, it is highlighted with a yellow ring. Related knots which are made by the same buyer are also highlighted with grey rings to draw the analysts’ attention (Figure 1 (b)). A floating text box will appear displaying the detailed information of the selected knot such as the location of the seller, the payment amount, and the sales category. Analysts can check the information of the transactions (e.g., the location of the buyer and the seller, the sub-category, and the commodity number) in the selected knot (section) in the detail view (Figure 1 (e)), which is designed for T2. A statistic view (Figure 1 (f)), which is helpful for T4, is used to present statistical information for the selected knot, such as the trend of the transaction frequency and the payment amount of the seller. Labeling When a specific transaction is identified as salient by analysts,it can be added to the labeled data for iterative visual analysis and exploration. CASE STUDY 案例研究PS: 这部分应该是论文作者讲述一次真实分析过程，里面有分析的思路与流程，还有该系统的使用特点。 来自我们的客户 - 客户（C2C）零售业务的数据部门的分析师参与了这项研究。 该公司提供了一个数据集，其中包含2600万个在线电子交易，从中他们想要检测假交易。 约有930万卖家和买家参与了数据集。 他有兴趣通过与合作伙伴买家建立假交易来识别卖家何时积累信用。 异常交易行为的一些指标可能是异常大量的商品，支付金额的大幅变化，特定卖方和买方之间的频繁交易，以及价值超出其正常范围的属性。 Construction of Decision Tree 决策树构建计算每个时间间隔内的卖方的交易频率并将其用作时间特征。 根据由分析者提供并用作文本特征的敏感字典，提取了评论中的关键词和短语（例如“信用”）。我们标注了大约300笔交易，这些交易是使用分层抽样从每个类别中选出的。 我们用标记的数据训练了决策树。 将要分析的交易的提取特征描述作为决策树的输入，并为每个输入产生显着性值。 我们使用精确率p和召回率r来评估决策树的效率：p = TP /（TP + FP）= 0.89，r = TP /（T P + FN）= 0.92，其中TP，TN，FP和FN 从训练数据的预测结果中计数（见表1）。PS: 这几个概念一般用于评估机器学习算法的性能等指标，也是广泛用于信息检索和统计学分类领域的两个度量值，用来评价结果的质量。 决策树算法在这里的作用只是用来找到显著性值。 Abnormal Frequency and Locations of Transactions 异常频率和交易地点分析师在简短的培训课程之后开始在TOS映射中进行勘探。他注意到一个具有高度显着性的长长的地区（图10）。随后，分析师指定了时间窗口，并放大到所需的区域。为了进一步研究交易行为，分析师选择了这个地区，发现许多交易在9月19日上午10点被分类为“图书”。分析师注意到连续的红色结在图1（b）的连线上连接。他告诉我们，这种模式表明卖家在选定的时间间隔内频繁交易。经查核详细资料后，分析师发现这些交易属于“充值卡”类别，并由不同的买家组成。他评论说，这可能是一个促销活动，因为他在这些交易中没有发现异常信息。分析师将显着阈值增加到0.8，显着调制滑块（Saliency Modulation）。这使得分析师能够有效地过滤出许多不太显着的交易。分析师立即找到多个未填充的结，指示具有缺失值的交易。他评论说，使用未填充的结来呈现缺失值的交易有效地吸引了他的注意。为了进一步调查这些记录是否表示促销或假交易，分析师进行了进一步分析。当他点击这条结线中的一个未填充的结时，同一个结线中的许多其他结被突出显示（图1（b）），表明这些结的大部分交易是由同一买方和卖方进行的。分析师注意到，详细信息视图中，买方的交货地址为空，通过查看此结线中的结的详细信息（图1（e））。他评论说这是可疑的，因为如果买方不填写他的地址，买家就不会买到这个商品。通过查看结的交易历史（图1（f）），分析师发现，卖方的销售额在一段时间内急剧增加。 分析师认为这些交易可能与赚取信用有关。 该结论由数据提供商的商业智能部门的分析师进行了验证，他们检查了与交易相关的其他信息，如卖方和买方的IP地址。 他们解释说，这些交易是由一组帮助卖家增加信用的买家进行的，卖家没有真正交付产品。 分析师将这一结线中的交易标示为显着，并将它们添加到训练数据集中。 Abnormal Attribute Values of Transactions 交易的异常属性值分析师在TOS映射中选择了另一个时间窗口。通过查看条形图视图中的销售类别信息（图11（a）），分析师发现，“电子配件”类别中销售的商品总数远远大于其他销售类别。分析师认为，由于商品数量庞大，这将是促销活动。他在knotline视图中调查了这个假设，但没有发现任何包含频繁连续的“电子配件”结。 分析师在TOS映射中选择了此类别，以过滤掉不相关的类别。通过仔细检查剩余的结，分析师发现一个具有短茎的极大的结（图11（b）），表明一个较大的商品数量，总支付金额却较低。分析师告诉我们，“通过这种knotline，我第一眼就注意到支付和商品数量之间的异常关系”。结的详细视图（图11（c））显示，本节包含单笔交易，支付金额仅为10美分，商品却为22万件。通过进一步调查卖方的交易历史（图11（d）），分析师消除了销售促销的概率，因为卖方在一段时间内交易次数很少，表明销售商品很少。分析师认为这是一个事件，卖方试图根据已经出售的商品数量增加他们的互联网搜索排名。分析师还将该交易标注为显着，并将其添加到训练数据集。 USER STUDY 用户研究我们进行了用户研究，以评估VAET支持低级别分析任务的能力，即第3节中讨论的T1-T4。所使用的数据集是第8节（即上一节）中探讨的交易数据集。 Design 设计10名参与者（6名男性和4名女性）年龄介于21岁至35岁之间进行了用户研究。 其中两人是分析师，其他人则是研究生。 所有参与者都具有使用在线商务的经验，并且对电脑熟悉。 学生的专业包括计算机科学，设计，数学和生物学。 以前他们都没有使用过VAET。 参与者一个接一个地参加研究。 对于每个参与者，在测试部分之前进行了短暂的训练。 在培训部分，教练首先向参与者介绍了VAET的25分钟演示。 在演示过程中，教练解释了VAET的视觉设计和功能。 在演示之后，参与者在教练的帮助下练习了VAET提供的互动5分钟。 在测试部分，参与者被要求完成9次练习，与实际分析中遇到的练习类似，无需教练帮助。 然后，他们被要求通过回答问卷并提供主观反馈来评估系统。 这9个练习是针对三种不同的具体情况而设计的，其中两项是在第8节中描述的。每个低级任务都通过一项或多项练习进行评估。 他们评估的练习和任务如下（任务显示在括号中）： E1 “使用TOS映射，从9月21日上午9点至10点选择具有最高显着性的销售类别。”目标：确定TOS映射（T1）中感兴趣的时间段和销售类别。 E2和E3来自第8.3节所述的情况。 E2 “选择商品数量最大的销售类别”。目标：解释条形图并确定感兴趣的销售类别（T1）。 E3 “查找KnotLines中商品数量最多的交易。”目标：解释一个结的视觉编码，并识别具有特定属性（T2）的有趣模式的交易。 要完成E4-E6，参加者将被要求在9月19日晚上18点至19点之间设置时间，并选择“充值卡”。 E4 “从KnotLines视图中找到具有最高交易频率的卖家（knotline）”。目标：解释knotline的视觉编码，并识别有趣的交易模式（T3）的卖家。 E5 “在KnotLines视图中，哪些卖方的交易模式不会发生？ （a）大量商品的单笔交易，但支付金额很小。 （b）低频连续交易。 （c）持续交易频率高，支付金额小。 （d）我不知道。“目标：解释这个knotlines和识别有趣的卖家交易模式（T3）。 E6 “E4中卖方交易历史的一个特征是什么？ （a）持续，频繁的交易。 （b）偶尔交易。 （c）突然，频繁的交易。“目标：以统计视角检查卖方的行为（T4）。 E7 - E9与8.2节描述的情况相同。 E7 “在KnotLines视图中查找未填充的结，并报告买方城市。”目标：使用详细信息视图（T2）检查事务的属性值。 E8 “E7中确定的卖方交易历史的主要特点是什么？ （a）持续，频繁的交易。 （b）偶尔交易。 （c）突然，频繁的交易。“目标：使用统计视图检查卖方的行为（T4）。 E9 “E7中确定的卖方的交易行为是什么？ （a）具有大量商品编号的单一交易。 （b）付款金额低的频繁交易。 （c）与异常购买城市频繁交易。“目标：解释和检查KnotLines（T4）的卖方交易模式。 E1 “Choose the sales category with the highest saliency value from 9 am to 10 am on September 21 using the TOS map.” Objective: Identify time periods and sales categories of interest in the TOS map (T1). E2 and E3 were from the case described in Section 8.3. E2 “Choose the sales category with the largest commodity number.” Objective: Interpret the bar chart and identify sales categories of interest (T1). E3 “Find the transaction with the largest number of commodities in KnotLines.” Objective: Interpret the visual encoding of a knot and identify transactions with interesting patterns in specific attributes (T2). To finish E4 - E6, participants were asked to set the time between 18 pm and 19 pm on September 19, and choose “Top-up Card”. E4 “Find the seller (knotline) with the highest transaction frequency from the KnotLines view.” Objective: Interpret the visual encoding of a knotline and identify sellers with interesting transaction patterns (T3). E5 “In the KnotLines view, which transaction pattern of the seller does not occur? (a) Single transaction with a large number of commodities but a small payment amount. (b) Continuous transactions with low frequency. (c) Continuous transactions with high frequency and a small payment amount. (d) I don’t know.” Objective: Interpret the knotlines and identify interesting seller transaction patterns (T3). E6 “Which is a feature of the seller transaction history of the knot in E4? (a) Continuous, frequent transactions. (b) Occasional transactions. (c) Sudden, frequent transactions.” Objective: Examine the seller’s behavior in the statistic view (T4). E7 - E9 were the same case described in Section 8.2. E7 “Find unfilled knots in the KnotLines view and report the buyer cities of them.” Objective: Examine the attribute values of the transactions using the detailed information view (T2). E8 “What is the main feature of the seller’s transaction history of the knot identified in E7? (a) Continuous, frequent transactions. (b) Occasional transactions. (c) Sudden ,frequent transactions.” Objective: Examine the seller’s behavior using the statistic view (T4). E9 “What is the transaction behavior of the seller identified in E7? (a) A single transaction with a large commodity number. (b) Frequent transactions with low payment amounts. (c) Frequent transactions with abnormal buyer cities.” Objective: Interpret and examine the seller’s transaction patterns from KnotLines (T4). 练习结束后，参加者完成了由6个问题组成的调查问卷（Q1〜Q6）。 要求用1到5评估等级（1=非常简单或高效，5=非常困难或低效），学习该VAET系统的难度和VAET的效率。 这些问题也收集到主观反馈。 六个问题如下： 容易或难以学习TOS映射？ 使用TOS映射探索显着数据是否有效？ 单个结的视觉编码是否容易或难以解释？ KnotLines的视觉编码和布局是否容易或难以解释？ 使用KnotLines分析用户交易模式是否有效？ 使用VAET整体分析多用户行为是否容易或困难？ Is it easy or hard to learn the TOS map? Is it efficient or not to explore salient data with the TOS map? Is it easy or hard to interpret the visual encoding of a single knot? Is it easy or hard to interpret the visual encoding and layout of KnotLines? Is it efficient or not to analyze the user transaction patterns with KnotLines? Is it easy or hard to analyze multi-user behavior with VAET as a whole? Results 结果收集9次练习的准确性和时间进行评估 总体来说，参加者完成了练习，在90次全部练习中产生5次错误（精准度为94.4％）。分析师正确回答了所有问题。对于学生参与者，其中两人在E5上出错，E8上有两个错，E9上有一个错。 我们采访了E5的错误参与者。他们都说他们“只注意到knotlines的主要模式，忽略了发生较少的模式”。但是，他们没有解释用户行为的问题。回答E8的参与者错误地提到他们忘记检查统计视图中显示的卖家历史信息（图1（f）），而是根据KnotLines视图回答问题。在E9错误的参与者认为，零买家城市（null buyer cities）对于不需要交货地址的电子书等虚拟商品的交易是正常的。事实上，“图书”类别中的所有商品都是实物。虽然一些参与者有T3和T4的问题，但总体准确性表明VAET可以很好地支持任务。 Overall, the participants completed the exercises, yielding 5 mistakes out of the 90 total exercises (94.4% accuracy). The analysts answered all questions correctly. As for the student participants, two of them erred on E5, two erred on E8 and one erred on E9. We interviewed the participants who erred on E5. They both said that they “only noticed the main patterns of the knotlines and ignored the patterns with fewer occurrence”. However, they had no problem interpreting the user behavior from them. The participants who answered E8 incorrectly mentioned that they forgot to check the seller history information shown in the statistic view (Figure 1(f)) and answered the question based on the KnotLines view instead. The participant who erred on E9 thought that null buyer cities are normal for transactions of virtual commodities such as E-books, which do not need delivery addresses. In fact, all commodities in the “Books” category are real items. Although some participants had problems with T3 and T4, the overall accuracy indicates that VAET supports the tasks well. 图12显示了每次锻炼的完成时间的平均值和标准偏差。 E3，E4和E7的时间比其他问题的时间长。 这三个练习要求参与者搜索具有特定特征的knotlines，这可能需要更多的时间仔细检查视图。 参与者能够快速完成其他6个问题。 完成时间为5.14s至44.22s。 分析师花费的时间远远少于学生参与者。 总体而言，VAET允许大多数参与者在90秒内完成复杂的练习和任务（如E4和T3）。 在考虑交易的各种属性以及它们之间的关系时，这是相当快的。 分析师的反馈与讨论 在用户研究结束时，我们采访了分析师和其他参与者。 两位分析师都对分类结果感到满意。 他们评论说，在分析不同类型的交易时，特征提取和决策树的集成是高效和灵活的。 我们要求他们将决策树与之前使用的逻辑回归进行比较。 他们认为我们的模型有几个优点： 决策树对于分析师来说很简单易懂。 在处理属性的缺失值时，决策树比逻辑回归更强大。 使用决策树的分类比逻辑回归更快。 他认为我们可以通过使用更多的信息来改进我们的方法，比如今后买家和卖家的IP地址。 他们评论说，TOS映射是直观的，勘探方便。一位分析师表示：“KnotLines的多用户行为的可视化是创造性和生动的。”虽然一位分析师认为该系统很难学习，但很少参与者难以解释可视化，大多数人能够在简短的培训会议后找到有趣的交易。两位分析师都提到，VAET能够帮助他们探索未被发现的交易模式。这是至关重要的，因为“欺骗人总是不断改变伎俩”。他们认为，VAET有能力找到新兴的交易模式，并帮助他们改进数据模型。分析师们渴望将VAET用于实际的多用户应用程序，在这些应用程序中，他们发现许多高维度事务之间的上下文和时间相关性。有趣的是，一些参与者最初认为使用我们的系统需要基本的音乐知识。其他与会者提到，他们对音乐笔记的了解影响了对设计的理解。例如，茎长度固定在音符中，但在我们的设计中是可变的。此外，有些人认为与其他knotbunches（类似于四分之一音符）没有连接的单个knotbunch持续比连接的knotbunch短（类似于第八个音符）。事实是，交易没有持续时间，因为它们都是立即在线发生的。与会者告诉我们，在他们学会如何解读设计之后，这种差异并没有妨碍他们的分析。 CONCLUSION 总结本文提出了一种用于识别基本交易数据和研究大片碎片记录的时间或集体行为的新型视觉探索方案。 在对KnotLines进行选择交易的详细探索和推理之前，执行决策树算法和TOS映射的过滤过程，以从大量记录中选择潜在有趣的交易。 案例研究和用户研究证实，VAET可以有效地支持大部分任务。 根据结果，一些任务如T3需要更好地解决，因为交易模式可以是动态的和多层次的。 为了使VAET更容易学习和使用，我们希望使TOS映射和KnotLines的设计更加直观。 对于未来的工作，我们也希望对更多的数据集进行扩展。 This paper presents a novel visual exploration scheme for identifying elementary transaction data and studying the temporal or collective behavior from large pieces of fragmented records. Prior to detailed exploration and reasoning of the chosen transactions with KnotLines, a decision tree algorithm and a filtering process by TOS map are performed to choose potentially interesting transactions from a huge amount of records. The case study and the user study verify that VAET can effectively support most of the tasks. According to the result, some tasks such as T3 need to be better addressed as the patterns of transactions can be dynamic and multi-level. To make VAET easier to learn and use, we would like to make the design of TOS map and KnotLines more intuitive. For future work, we also would like to extend our approach for more datasets.]]></content>
      <categories>
        <category>CS</category>
      </categories>
      <tags>
        <tag>Data-Visualization</tag>
        <tag>Paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机系统结构]]></title>
    <url>%2F2017%2F09%2F28%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[系统结构的部分课后作业，因为需要写成电子版，故稍微整理下也放入到博客中，方便你我他。答案是自己组织的，不保证一定正确。书是张晨曦的《计算机系统结构（第2版）》 第一章1翻译技术是先把 L+1 级程序全部变换成 L 级程序后，再去执行新产生的 L 级程序，在执行过程中 L+1 级程序不再被访问。 解释技术是每当一条 L+1 级指令被译码后，就直接去执行一串等效的 L 级指令，然后再去取下一条 L+1 级指令，以此重复进行。 计算机系统结构是程序员所看到的计算机属性，即概念性结构与功能特性。 Amdahl定律：加快某部件执行速度所能获得的系统性能加速比，受限于该部件的执行时间占系统中总执行时间的百分比。 程序的局部性原理是指程序执行时所访问的存储器地址不是随机的，而是相对簇聚。局部性原理又表现为：时间局部性和空间局部性。时间局部性是指如果程序中的某条指令一旦执行，则不久之后该指令可能再次被执行；如果某数据被访问，则不久之后该数据可能再次被访问。空间局部性是指一旦程序访问了某个存储单元，则不久之后。其附近的存储单元也将被访问。 CPI(Cycles Per Instruction，每天指令的周期数) CPI = 执行程序所需要的时钟时间 / 执行指令个数。 模拟是指用软件的方法在一台现有的计算机（宿主机host）上实现另一台计算机（虚拟机VM）的指令集。 仿真是指用一台计算机（宿主机host）上的微程序去解释实现另一台计算机（目标机）的指令集。 3Flynn分类法是按照指令流和数据流的多倍性进行分类的。 Flynn分类法把计算机系统结构分为以下4类： 单指令流单数据流（SISD） 单指令流多数据流（SIMD） 多指令流单数据流（MISD） 多指令流多数据流（MIMD） 4计算机系统设计经常使用的4个定量原理 以经常性事件为重点，按照对经常性情况采用优化方法的原则，能得到更多整体上的改进。 Amdahl定律，加快某部件执行速度所能获得的系统性能加速比，受限于该部件的执行时间占系统中总执行时间的百分比。 CPU性能公式，CPU时间 = 时钟周期时间 x CPI x IC，只要改进任何一个参数都能提高CPU性能 程序的局部性原理，程序执行时所访问的存储器地址不是随机的，而是相对簇聚。 6执行时间 = 0.000575(s) CPI =(45000 + 750000 2 + 8000 4+ 1500 * 2) / （45000 + 75000 + 8000 + 1500）~= 1.776(s) MIPS = 400Mhz / (CPI * 10^6) ~= 225.217 执行时间 = 总时钟周期时间数 * 平均周期时间 = 0.000575(s) 8 加速比= 1/((0.9-x)+0.3/30+0.3/20+x/10)=10 x=0.361 加速比=1/(0.2+0.3/30+0.2/20+0.2/10)=1/0.245 比例=0.2 * 加速比 ~= 0.816 第二章1CISC Complex Instruction Set Computer 复杂指令集计算机 复杂指令集计算机的设计策略是使用大量的指令，包括复杂指令。与其他设计相比，在CISC中进行程序设计要比在其他设计中容易，因为每一项简单或复杂的任务都有一条对应的指令。程序设计者不需要写一大堆指令去完成一项复杂的任务。 但指令集的复杂性使得CPU和控制单元的电路非常复杂。 RISC Reduced Instruction Set Computer 精简指令集计算机 精简指令集计算机是一种执行较少类型计算机指令的微处理器。它能够以更快的速度执行操作（每秒执行更多百万条指令，即MIPS）。因为计算机执行每个指令类型都需要额外的晶体管和电路元件，计算机指令集越大就会使微处理器更复杂，执行操作也会更慢。 数据表示(data representation) 是指计算机硬件能够直接识别和指令集可以直接调用的数据类型。 2区分指令集结构的主要因素：寻址方式，数据表示，指令格式。根据该因素可将指令集结构分为以下三类： 寄存器-寄存器类型(R-R) 寄存器-存储器类型(R-M） 存储器-存储器类型(M-M) 4指令集应满足的基本要求：完备性，规整性，高效性，兼容性 5指令集结构设计所涉及的内容有指令功能设计，数据表示，寻址方式，指令格式 9表示寻址方式的主要方法有2种： 把寻址方式编码于操作码中，操作码在描述指令功能的同时也描述了相应的寻址方式，这缩短了指令长度，译码快，但增加了指令的条数和多样性，也增加了CPU对指令译码的难度。 把寻址方式跟操作码分离，为每个操作数设置一个地址描述符，由地址描述符表示相应操作数的寻址方式，这扩大了寻址的范围，但增大了指令长度。这种方式译码较慢，但操作码和寻址互相独立，易于扩展。 10体系结构中常用的指令格式有变长编码格式，定长编码格式，混合编码格式。分别是考虑目标代码的大小，性能，和兼顾两者。 第三章1流水线：把一个重复的过程分解为若干个子过程，每个子过程由专门的功能部件实现。把多个处理过程在时间上错开，依次通过各功能段，这样每个子过程及其功能部件可以与其他子过程并行进行。 流水线的特点： 把大的处理功能分解为多个独立的功能部件，依靠他们的并行工作来缩短程序的执行时间 流水线中的各段的时间尽可能相等，否则会引起流水线的阻塞、断流，因为时间长的段会成为流水线的瓶颈 流水线每个功能部件后面都要有一个锁存器，作用是在相邻两段传送数据，保证后面要用到的数据，并把各段处理的工作隔离 适用于大量重复的时序过程，只有在输入不断地提供任务，才能充分发挥效率 流水线需要通过时间跟排空时间。 流水线的分类： 单功能流水线，多功能流水线 静态流水线，动态流水线 部件级流水线，处理机级流水线，处理机间流水线 线性流水线，非线性流水线 顺序流水线，乱序流水线 吞吐率ThroughPut是指单位时间内流水线所完成的任务数量或输出结果的数量。 流水线加速比Speedup是指完成同一批任务，不使用流水线与使用流水线所用时间之比。 流水线的效率Efficiency是指流水线中的设备实际使用时间与整个运行时间比值，即流水线设备的利用率。 名相关是指某两条指令使用相同的名，但是它们之间没有数据流动。 定向技术：在某条指令产生计算结果之前，其他指令并不真正理解需要该计算结果，如果能够将该计算结果从其产生的地方直接送到其他指令需要它的地方，那么就可以避免停顿。 链接技术是指具有先写后读相关的两条指令，在不出现功能部件冲突 和源向量冲突的情况下，可以把功能部件链接起来进行流水处理，以达到加快执行的目的。 分段开采技术：当向量长度大于向量寄存器长度时，必须把长向量分成长度固定的段，然后循环分段处理 ，每次循环只处理一个向量段。 3解决流水线瓶颈问题通常有两种方法：细分瓶颈段，重复设置瓶颈段 4减少分支延迟的静态方法：预测分支失败；预测分支成功；延迟分支。 73种向量处理方式： 横向处理方式：向量长度为N时，则相当于N次循环，使用流水线时，在每次循环会出现数据相关和功能转换的问题，所以不适合流水线处理。 纵向处理方式：将整个向量按相同的运算符处理完后，再去进行别的运算。无论N多大，相同运算都用一条向量指令完成。因此需要采用存储器-存储器结构流水线。 纵横处理方式：结合以上两种，它把向量分成若干组，组内用纵向处理方式，依次处理各组。可以设置能快速访问的向量寄存器，用于存放源向量、目的向量、中间结果，构成了寄存器-寄存器结构流水线。 91)TP = 10 / (50 + 50 + (10-1) * 200) = 1/220 E = (10 (50 + 50 + 100 + 200)) / ((50 + 50 + 200 + (10 – 1) 200) * 4) = 5/11 2)该流水线的瓶颈在第四段。分别采用细分瓶颈段和重复设置瓶颈段。两种的吞吐率和效率相同 TP = 10 / (50 + 50 + 100 + 100 + 100 + (10 – 1) * 100) = 1/130 E = 10 400 / (1300 5) = 8/13 11TP = 8 / ((1+2+1+1) + 23 + (1+1+1) + 1 3) = 8 / 17 S = 4 (1+2+1+1) + 4 (1+1+1) / 17 = 32/17 E = S/TP = 1/4]]></content>
      <categories>
        <category>CS</category>
      </categories>
      <tags>
        <tag>Arch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人工智能发展报告]]></title>
    <url>%2F2017%2F09%2F24%2F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8F%91%E5%B1%95%E6%8A%A5%E5%91%8A%2F</url>
    <content type="text"><![CDATA[人工智能的发展数据挖掘课程的作业报告，放在这里分享一下，都是一些蠢话。 符号主义（逻辑主义）推理期二十世纪五十年代到七十年代初，人工智能的研究处于“推理期”，那时候人们普遍认为机器获得智能的方法是赋予机器逻辑推理的能力。该阶段比较有代表的工作比如在1955年12月，赫伯特·西蒙（Herbert Simon）和艾伦·纽厄尔（Allen Newell）开发出逻辑理论家，这是世界上第一个人工智能程序，有能力证明罗素和怀特海《数学原理》第二章52个定理中的38个定理，甚至在后来证明了全部52个定理。这两位也因此在1975年获得了图灵奖。 知识期随着后来的研究进展，人们意识到仅仅具有逻辑推理能力是远远无法实现人工智能的。从二十世纪七十年代中期开始，人工智能的研究进入知识期。专家系统被大量开发出来，E.A. Feigenbaum 作为“知识工程”之父在1994年获得图灵奖。不过后来人们又意识到专家系统的“知识工程瓶颈”，把人类总结的知识教授给计算机是相当困难的。 机器学习八十年代左右，“从样例中学习”的一大主流还是符号主义的思想，其代表如决策树（Decision tree）和基于逻辑的学习。决策树以信息论为基础，而基于逻辑的学习是归纳逻辑程序设计，可看作是机器学习与逻辑程序设计的交叉。 神经网络（联结主义）九十年代中期之前，“从样例中学习”的另一个主流是基于神经网络的联结主义。1986年，D.E. Rumelhart 等人重新发明了BP算法，产生了深远影响，如今的深度学习最基本的概念便是BP算法。不过联结主义产生的是“黑箱”模型，从知识获取角度看有明显的弱点。然而，由于BP算法，深度学习算法在实际中非常有用，在2006年开始第三次神经网络高潮以深度学习之名重新爆发而来，在2012年之后成为人工智能的主流算法。当然，在九十年代统计学习兴起时，而又因为当时的局限性，曾经落入低潮。 统计学习九十年代中期，“统计学习”迅速兴起并成为主流，如今依然是主流的机器学习算法，典型代表是SVM。早在九十年代之前，统计学习的许多基础理论已经出现，但因为联结主义的神经网络在九十年代具有局限性而没落后，统计学习被人瞄准目光而迅速流行起来。 谈谈自己的理解前面的一些概括是我认为比较重要的历史的整理。如今看来，人工智能的历史虽然不漫长，却可以说的上丰富与多变。现如今，从学术、商业、工业角度来审视的人工智能，占据主流的是传统机器学习跟深度学习，然后才是强化学习跟规则学习（个人看法）。当然，如今的机器学习算法或多或少都用上了概率统计的知识。 深度学习如今越来越火热，在我自己开始留意机器学习的内容开始，深度学习相关的文章跟新闻就狂轰滥炸地映入我的眼里，以至于我没法不正视它。后来我也简单的接触深度学习，才发现它确实不可思议。在训练深度学习时，它就是在特征空间里不断逼近然后拟合到数据特征的“万能函数”，怪不得说它是万能近似图灵机。把它应用到许多领域感觉也就不奇怪了，当然它不一定比传统的机器学习和其他人工智能算法要更有效。 不过可能是因为深度学习的万用性导致它的黑箱性，大部分人在使用它来解决问题的时候，没有获得很好的解释，无论是从深度学习结构模型的理论角度还是问题本身特性的角度。只是在设计网络架构时，粗略地分析问题的特性，然后改良别人成功的架构跟“合理”的解释来解决问题。在问题较满意的解决后，没有合理的可解释性或者干脆从他人理解的来解释。 当然，从实用性跟工作角度来说，我觉得这样没错。深度学习跟你的剪刀和锤子一样，只是解决问题跟生产的工具，并不需要在意内部机理。这样的比喻来解释可能非常不妥当，不过我想表达的是，如今主流的人工智能方法：深度学习，虽然在构造时有比较好的理论基础，但在优化模型，优化架构，并且在解释优化可能性与优化思路上，缺乏理论。（对深度学习接触才几周，造成这样可能有偏差的认知，如有错误，欢迎指出） 相比于深度学习，传统偏向于统计的机器学习，可解释性就比较强了，而且在很多时候，从各种角度上与深度学习比较，丝毫不逊色。而且现实的问题非常复杂，蕴含大量不确定跟随机的事件，而概率论与数理统计就是对现实世界建立这样的一些模型跟假设，这方面的理论也算比较完善了，所以传统机器学习更能在数学理论上解释一些模型的行为。 而最近神经网络之父 Geoffrey Hinton 也表明对BP非常怀疑，应该抛弃它。BP如今是深度学习最常用的算法了，如果丢弃它，深度学习大概会大变样吧。如此看来，深度学习没有那么“完美”，仍需要大量基础研究甚至真的可能在未来被更好的算法替代。 展望未来的人工智能如果想到三十年以后，大街小巷到处有序而不拥挤地行驶着无人车，载着乘客去景点；天空上时不时飞过一架架无人机，拍摄城市风景与监控城市安全；新闻报道是程序根据许多视频、图片。文本而撰写的；家里布满了传感器跟智能家具，许多繁琐事情可以通过简单对话跟指令来处理。想到如果真能如此，不由对如今的人工智能算法寄予厚望并抱有乐观的想法。 但在我看来，上面的美好描述可能还是过于乐观了。 之前曾在哪里看到一个观点，20年后人工智能将会代替80%的工作。我觉得这个也太乐观了。当今现实生活中，从事脑力活动的人已经多余从事体力活动的人了，未来这个趋势应该更加明显。而以目前人工智能的能力，很多脑力活动不能代替（或者说，实现这些能代替脑力活动的程序需要更多脑力的脑力活动）。若人工智能的算法更加成熟稳定，许多人应该会从事人工智能相关的工作，建设基础设施（其实现在就有很多人往机器学习这个方向转业），普及人工智能，而只有少部分人推动人工智能的发展。 其实我一直不敢想像未来，这对于我太难了。十年前我对未来的展望似乎跟如今的现实大相径庭了。自己觉得可能出现的东西往往没有到来，反而出现一些超出以前认知的意外事物。说不定，以后深度学习也不再是实现人工智能的主力了，出现一些特定算法可以实现以前难以实现的智能，但却无法较好完成如今研究的方向。最坏的情况就是深度学习仍然是主流，而其理论仍然不明朗，调参也没有完备的方法论，人工智能发展停滞了几十年，直到我们这一代人死去，这后面的事情我也不想展望了。]]></content>
      <categories>
        <category>CS</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[暑假机器学习总结]]></title>
    <url>%2F2017%2F09%2F13%2F%E6%9A%91%E5%81%87%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[机器学习学习总结经过两个月的学习，从对机器学习一点点懵懂认知，到现在对机器学习的基础知识跟体系有一定的认知。如今学习暂告一段落，总结如今学习过的重点知识，可以起到很好的复习作业，也是对两个月以来的交代。以下，我将按照每周学习进度来总结回顾机器学习的知识。 第一周我们先学习了python编程基础，之后的学习是基于python各种库来实验的。我之前已经比较熟悉python，所以很快就完成这段学习；之后接触numpy，这是C语言为python编写的底层矩阵库，我之前也接触过，但比较浅，不过numpy封装的很好，用起来门槛也很低，很快就上手了；numpy之后就是pandas,它是数据分析常用的库，基于numpy，非常全面，我之前也用过，但基本需要重新学习，pandas比较难用，尤其是IO部分，有细粒度的操作，文档看起来也比较麻烦，没有示例，所以学习的过程中，是遇到问题再去查找方法，后面的学习pandas其实用到的也比较少。 以上内容大概花了两天，算是对机器学习的预备知识的准备。当然期间也学习了简单的使用anaconda，jupyter等工具，不再一一总结了。 之后开始学习基本的图像知识： 颜色直方图,它所描述的是不同色彩在整幅图像中所占的比例，而并不关心每种色彩所处的空间位置。之后有几次作业中要编写颜色直方图的处理，一开始还是挺棘手的。不过跟后来的图像特征提取就是小巫见大巫了 HOG特征，主要思想：在一副图像中，局部目标的表象和形状（appearance and shape）能够被梯度或边缘的方向密度分布很好地描述。（本质：梯度的统计信息，而梯度主要存在于边缘的地方）。这也是之后的需要实现的任务，学习过程中，只有这个博客资料可以参考，其他资料要么可能是全英文晦涩难懂，要么还不如这个。这个博客写的太精炼了，初学的时候，实现起来非常困难，以至于有些同学对根据这个博客写出算法的可能性产生怀疑。我这周的作业也卡在这里了，学习机器学习的时候反而是因为图像处理知识不过关。后来经过讲解对这些特征有更新的认识了，不过可能实现起来对于现在我的依然有些困难。 LBP特征、Haar特征也是这周的基础知识，我稍微学习了下LBP，发现比HOG要好懂，Haar并没有怎么看，最后这2个内容没有出现在作业里，我对这些也只有粗浅的认识。LBP（Local Binary Pattern，局部二值模式）是一种用来描述图像局部纹理特征的算子；它具有旋转不变性和灰度不变性等显著的优点。而Haar特征值反映了图像的灰度变化情况。 图像的部分内容学习后，开始了最基础的机器学习的内容： Regression，先从最简单的线性回归开始，线性回归可能是踏入机器学习世界的第一步吧，它教你如何做最简单的预测和机器学习比较本质的思维。前几周学习的知识大多是看李宏毅的视频，前几周感觉还是不错的。从线性回归开始，学到了基本的梯度下降思想和度量性能的代价函数。 Error，第二节就是深入理解各种模型评估的知识，讲授了误差,偏差,方差的区别与联系。 Gradient Descent，最后是深入学习梯度下降，学习推导基本的梯度下降，然后提出随机梯度下降(SGD)，从大量数据中随机选择一定量数据来训练，提高学习效率。除此之外，讲解了梯度下降的问题：学习率的选择。然后基于此讲解了一个算法Adagrad来控制学习率。 最后还有一些比较杂的知识，了解了K折交叉验证的思想与作用，把数据集分割为训练集，验证集，测试集的思想与作用。各种距离度量，可以衡量样本近似度。最后完成两个作业 图像知识可能在第一周学习，有点不知其有何用的感觉，就算想在思想上重视它，但没有实际用起来，还是难以深刻理解它的重要性吧。谈点个人感受，我其实挺不擅长也不太喜欢处理图像的，大一自己有简单接触过图像处理（跟现在的学的不太一样，而是常规的图像处理，不跟特征，知识等内容关联），虽然不比这些难，但也很吃力。 第二周第二周主要学习的是分类的基础算法，分别学习K近邻,决策树,逻辑斯蒂回归： KNN的思想就是把某个样本跟其他所有样本进行距离度量并总和，该样本离哪个类别’最近’，就标记为该类别。KNN是懒惰学习的典型算法，即到需要分类的时候才使用上训练集。在学习KNN的时候还了解到矢量化编程的重要性，减少不必要的python for 循环可以利于底层numpy优化为并行代码，在我这次实验里速度提升了近百倍。 Logistic Regression 该算法跟线性回归（跟感知器也类似）基本类似，不过它加入了sigmoid函数来进行分类而不是回归。后来学习深度学习才知道这里有神经网络的最基本的思想，或者说可能是最简单的神经网络了。 Decision Tree决策树可能是到这周为止最难的算法了，写起来会特别长。思想其实很简单，就是树的思想 + 人类决策过程。常用的决策树有ID3,C4.5,C5.0,CART等，不过我只编写了最简单的ID3，其他决策树进行了简单的了解。学习过程中认识到如果生成完整的决策树，那会变得非常耗时，后面学习到剪枝知识（我在作业里面编写预剪枝了，不过效果很差）。学习决策树里面知道了一些信息论的知识，如信息熵,信息增益,纯度等知识，决策的依据便是依据这些数值来找到最佳决策特征。 最后根据学到的知识完成一个井字棋胜负预测，不过我的模型很一般。这周的知识量不是很大，更多侧重机器学习基础编程，但是感觉学到东西很多的一周。 第三周这周学习的机器学习非常强大和实用，是现在也很常用的模型： SVM非常理论，学到这里，我感觉我的高数白学了。其实现在我也不是很了解SVM，只对基本概念有了解。基本思想应该是把低维空间的非线性问题映射到高维空间线性问题来解决。然后里面概念非常多：支持向量的概念，距离度量，核函数，核方法，对偶问题，KKT等。 集成学习非常实用且广用。许多一般模型集成后都可以大幅度提升性能。这周接触了许多集成学习算法： bagging，非常简单的集成学习算法，我后来实现了bagging决策树，性能提升了许多。基本思想是自助采样一些样本后，分别训练n个模型，然后进行投票决策。这样可以大大减少过拟合而提升性能。 Random Forest，bagging算法的变体，基于决策树实现的。它与bagging的区别在于特化了决策树，在节点决策时，加入属性扰动（即只从一部分特征里选择最优特征），而bagging只有样本扰动（随机采集样本）。它的性能一般来说比bagging要好，我猜大概是加入了新的扰动后，更能避免决策树容易过拟合的缺点吧。 其他如 boosting的adboost和xgboost，进行了简单了解，xgboost在kaggle里面很热门，因为性能特别好。不过这几个算法难度更大，我了解的也比较少。adboost的基本思想是让之前训练错误的部分对应的权重变大，让模型认识到这点。 最后是一点点图像特征的知识bag of words model，对此进行一些了解 第四周这周学习无监督学习，主要是聚类跟降维，不过主要是侧重分析并运用这些技术： 聚类可以按结构特性分为原型聚类，层次聚类，密度聚类。K-means是最基础的聚类算法，主要思想是通过多次迭代来把刻画原型，来使误差最小。K-means之后是了解了高斯混合聚类，也是原型聚类。 降维这部分主要接触了PCA，不过这部分理论特别麻烦，我只搞懂了基本思想，并学会基本运用。总之，它变换了基底，并把比重最小的维度去掉来降维，这样能最大程度的保存原始样本的信息。 这周机器学习的知识就进入进阶难度了，说实话，我掌握的不太会好，不过算是对机器学习的体系更加了解了。除了以上还有一些其他知识，不过比较零碎，我就不一一总结了。 第五六七八周这里把深度学习一起总结了吧，这几周感觉学习的不太好。 第六周开始，陆陆续续有许多高大上的报告可以听讲了，了解了许多前沿的应用，思想，算法。从第五周开始也步入了深度学习的大门。 这几周主要学习全连接网络跟卷积神经网络跟循环神经网络，理解了很久反向传播，卷积运算。其实参考过很多资料，这里就不一一列出了。目前对反向传播也有稍微清晰的了解了，对于卷积，了解了其思想，但对其运算还是只有抽象的认识，反向传播可以说是目前神经网络的基础。全连接网络在理论上是近似的图灵机，不过实用性很差，一般是配合其他网络而使用。而卷积神经网络是针对图像而发明的，在图像处理跟机器视觉应用广泛。而循环神经网络更适用于序列数据，如文本，这在自然语言处理很常用，而由于一般的RNN有局限性，所以有一个LSTM的变体，这个模型我不是很清楚，但它的表现更好（最近好像又出现一个比较厉害的新变体SRU）。 由于神经网络的代码非常难写，写出来也基本不可以重用，我们学习了keras这个基于tensorflow的深度学习框架来实现一些经典的模型。后来应用keras解决了一些基本的图像分类问题。 第七八周陆陆续续的讲座也应该提一提，感觉大大开阔了我的视野。原来我以为深度学习基本都是在对图像上进行工作，好像这样也没什么意思。不过后来发现图像几乎是实现人工智能最基本的办法。而老师们的方向五花八门，问题的复杂度也远超过我的脑容量，深度学习反而变成了实现人工智能的基本工具而已，更多在于对于问题的深入研究和对特征的深入探究。 这几周的学习积极性变得比较差劲了，不过收获还是很多。至少认识到机器学习跟深度学习的基本思想，我觉得未来的程序员都或多或少需要了解这些知识，因为他们可能会用到相应的算法来部署一些人工智能应用到各种设备、各种网站、各种系统中去，而懂得这些知识的人显然能在工作中更胜一筹。 总结以上大概就是我的总结了，很粗略，也没有讲到具体的数学知识，更多是讲到自己的小小收获跟感受。我不清楚以后是否会从事机器学习相关的工作，但我以后肯定会抱着好奇心继续完善我对这些知识认识，了解里面的新思想，跟上人工智能的潮流，做一个终身学习的人吧。]]></content>
      <categories>
        <category>CS</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>DIP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从《线性代数应该这样学》到《Redis的设计与实现》]]></title>
    <url>%2F2017%2F09%2F08%2F%E4%BB%8E%E3%80%8A%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%BA%94%E8%AF%A5%E8%BF%99%E6%A0%B7%E5%AD%A6%E3%80%8B%E5%88%B0%E3%80%8ARedis%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E3%80%8B%2F</url>
    <content type="text"><![CDATA[从《线性代数应该这样学》到《Redis的设计与实现》《线性代数应该这样学》 起因这本是好早之前买的，当时想着是时候复习一下线性代数了，因为以后要用到（那时候想到可能需要接触一下机器学习了）。买来之后发现太数学了，怕是数学系的许多人也吃不消看这种书。 收获与总结这本书可以作为第一本或者第二本线性代数的学习材料。但这本书远比本科工科的要求要高出许多，以致于作者在前言里说，这本每页应当至少投入一小时来学习。匆匆看完前两章我便放下了，书里充斥着大量证明，从最基本的向量空间的定义引出基本的性质，然后用证明的方式来引出各种向量空间的特点跟性质。而这是第一章的内容，读起来还是挺熟悉的感觉的，只是换了一种表述各种概念的方式和术语，但深入思考便与之前学习的向量没有多少差别。这里与我大一学的过程就差别大了，大一是直接从矩阵着手，最后讲到向量，但那时候觉得向量是很突兀的东西，甚至跟高中学习的向量完全不一样，没有从几何的角度稍加解释向量的一些特点。而这本书从标量开始，比较了标量跟向量的异同，开阔了一下从标量到向量的思路，本质上它们比自己想像的还要相似。 第二章接着从有限维度引申到无限维度，在讨论了无限维度的一些异同后，继续深入了向量空间的各种特性，譬如张成空间，直和这些陌生的概念。而这些与普通向量之间的联系是很紧密的，但读到后面愈发抽象，我其实也忘记了七七八八了，惭愧。 后面几章暂时就没有看了，基本应该是从向量空间引申到线性映射，然后引出矩阵及其性质。在我看来矩阵是向量空间的更一般化，所以以后继续讨论的大量性质，也可能用于向量。最后从矩阵的一般运算到各种算子，最后才提出行列式相关内容，大概就是这样（痛苦）的过程吧。 《TCP/IP网络编程》感获这本书是一本网络编程的入门书，整体难度跟CSAPP的第三部分差不多，各有侧重吧，内容有比较多重叠，但内容稍微多于CSAPP。用的是C语言的socket接口来讲解基本原理。用C语言讲有个好处，就是能直接从操作系统级别来思考网络编程，而且能使用全部的socket模式、所有并发模型、IO模型。缺点是这些socket api相当底层，以至于一个api只做一个最基本的逻辑操作。学习起来特别费劲，而且高度依赖一部分操作系统的知识。因为之前接触过golang和nodejs，分别也简单用过相应的socket接口。这两个语言的并发模型比较受限，nodejs目前只能异步跟简单多进程（将来应该可以用上简单多线程），golang倒是用goroutine（线程调度），通道（或者叫信道），多进程。而它们的socket接口便是跟这些模型有一定的耦合，而且高度精简，有更好的语义，从底层上面减去一些麻烦。当然，nodejs因为更单一，比golang还精简。 言归正传，这本书其实重点不在于讲解socket api，而是从使用api来达到理解socket编程思维，socket的各种核心功能，tcp/udp基本原理，并发模型的基本原理，可能还有较多的跨平台编程思路（因为C语言的socket各个系统有差异）。 一点思考因为听说是隔壁网络工程的网络编程课的教材（说实话，这本书可能稍微浅了点，除非隔壁把这本书全部内容都讲授，我觉得才比较合理），我也无意中看到并且有打算看看。不过这本书实际比较狭窄，基本专注于tcp和并发这两个内容了，虽然充分且不错地解释了部分tcp知识，部分操作系统知识，对应用层太忽略了（提了下dns跟http），我个人认为把传输层跟应用层稍加紧密地联系起来，更能激发学习兴趣，也使这本书更加实用。 其实大概就匆匆看了半本，而且忽略了windows下的实现，自己也就琢磨了下源码，基本没有自己去写过（坏习惯啊），算是预热计算机网络跟操作系统的知识了。 《Redis的设计与实现》这是图书馆借的到好书，其实早想看了，之前拿起来抱怨看不懂，这次再拿起来，已经不是那么可怕了。 收获不过现在我看的比较少，数据结构篇还没有看完，也算是温习一些数据结构的知识了。这本书把redis相关的数据结构的内存模型很漂亮的表述清楚了，特别是hash和跳表，结合了许多图片来理解。也讲清楚了实现该数据结构的动机，复杂度，和一些特性。但大部分的数据结构基础操作忽略了，有些地方比较好奇，却没有讲到。这是我目前读到第六章的感受。但快开学了，这些书不太可能全看完，接下来怎么看还是得看心情咯。 最近的读书总结其实还看了不少书，比如《深度学习》、《机器学习》、《统计学习方法》、《流畅的python》等，这些应该是一两个月前开始看的，都是需要长期学习的知识，在暑假机器学习集训班学习时来补充机器学习相关知识的。之后会写机器学习相关的总结，所以没有在这里总结一些知识跟感想。还有部分书是跟下学期的课比较紧密的，提前看看也算是预习，本身也挺感兴趣的，也不一一列出了，读的篇幅不多，列出意义也不大。]]></content>
      <categories>
        <category>Math</category>
        <category>CS</category>
      </categories>
      <tags>
        <tag>Book</tag>
        <tag>Linear-algebra</tag>
        <tag>Redis</tag>
        <tag>TCP/IP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最近一周总结]]></title>
    <url>%2F2017%2F08%2F20%2F%E6%9C%80%E8%BF%91%E4%B8%80%E5%91%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[总结近况杂项发现已经漏了好几天没有写博客，也不知道自己能否坚持下去，最近可能也比较懈怠。C++的模版接触了下，发现使用还是容易掌握的，但后来就没有学下去了。靠这样学是永远学不完的，甚至学了一部分忘记七七八八了。打算下学期如果适合C++编程的任务，尽量使用C++来完成，这样有任务的使用C++应该能学的更快，更扎实，C++学好了，很多东西也都能学好，果然不是空穴来风。 兴趣方向最近学习深度学习的基础，主要是全连接神经网络(FNN)和卷积神经网络(CNN)，其实正式学习是下周，所以这周在机器学习方面懈怠很多。其他方面呢，编译原理，《编译器设计》看了些内容，感觉很有收获，虽然不是直接关于编译原理的收获，但对计算机的全貌有了更进一步的了解。期间，查了一些关于链接的知识，也看了一点点关于linux kernel的知识，总的来说，收获了一点点学习这些知识的方法论。 英文资料这两天买了两本英文书，打算常常读一读，一本是go圣经，一本是《编译原理与实践》。两本对于我来说应该难度不大。go圣经我看过半本中文版，虽然还掌握的不好，但有了前置知识，看英文只是时间问题。《编译原理与实践》比《龙书》跟《编译器设计》要简单些，所以买了英文版，打算下学期的编译原理课配合着读。这么说来了，在编译原理相关的书里，我已经接触了6，7本了，但其实没有精读过任何一本。下学期重点看这三本吧，以我现在半吊子的水平，希望本科基本能看完3，4本。毕竟最想从事的就是编译器相关工作，也说不清为什么偏爱这个。 不足与改正最近越来越意识到自己在编程方面的缺陷，不论编写代码量小还是较大，自己思路一直不清晰。说来，很多我编写的代码，是根据别人的来编写的，自己完全独立从头开始写的几乎没有。上大学以后，在学业方面的作业情况也有类似，就是没有自己的思路，得照着例题来“复现”思路，但这终归不是自己学到的知识。这个问题从大学开始的，已经遗留比较久了，严重影响了自己的思维。趁着下学期的许多有趣课程跟高难度的高强度的课表，改着这个坏毛病，尽量做到把书本知识深刻映入大脑，然后根据自己的思路来完成作业而不依赖参考答案。]]></content>
      <categories>
        <category>Life</category>
        <category>CS</category>
      </categories>
      <tags>
        <tag>Book</tag>
        <tag>OS</tag>
        <tag>Compiler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于未来方向学习的思考]]></title>
    <url>%2F2017%2F08%2F11%2F%E5%85%B3%E4%BA%8E%E6%9C%AA%E6%9D%A5%E6%96%B9%E5%90%91%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[总结现状如今今天是周五，等会就周六了。虽然平时经常忙活在电脑前，但好像没在做什么特别有意义的事情，看到github一个好项目就想着去看它的源码，然后呢，迫于水平不够，一堆工具也掌握的不利索，怎么也没法开始学。其实大概是自己内心不知道以后到底想去做什么吧。其实2，3w行的代码如果搞懂它的流程，而且又熟悉相关语言并懂得一些相关方向的知识，应该是可以开始看了，起码不应该想到看就害怕而退缩，这连第一步都没有走呢。 想法比较坚定的想法呢是以后做编译相关工作，这个工作岗位很少，要求也很高，许多大神高中就开始接触了。我呢，现在还半吊子，最近才开始理解了最基本的知识，将来如果要考研，这应该是计算机体系结构这个方向的研究生吧。而这2个月一直在学机器学习，但我只是对它的原理好奇，又想着机器学习方向能跟数学打交道，又能锻炼算法能力，便很欣然地参加了这次暑假集训。最近这个月有些忙坏了，但又觉得自己学到知识很少，可能是太贪心了吧，自己对计算机许多方向都感兴趣，但未曾深入其中一个。我还经常跟同学说那句话：不要总想着以后要干嘛，你现在在干嘛，以后很可能就在干嘛，所以要干嘛就赶紧现在就开始干。其实我自己也没有做到我所说，虽然我非常希望自己能做到。 兴趣最近查阅了一些资料，其实都是自己以前都看过，只是很多不少都忘记了或者没有重视起来。记得大一时候对C++兴趣特别浓厚，买了C++primer和STL源码剖析，还很认真看过（但没怎么敲代码），后来接触了的其他编程语言，C++也基本没有使用过了。现在要是还记得大一C++要求的内容就不错咯。 但我知识，我想以后从事的方向很可能离不开C++（至少10年是这样）。不论是机器学习或者是编译原理，还是网络编程，在高性能的场合，永远需要它。 目前我比较感兴趣的是编译原理，机器学习，网络编程吧。这三个都可以学很深，无底洞。编译原理简单的话，写个编译器前端，嗯，可能还不如自动化工具强，学的深入了，就得学后端了，代码优化，无底洞，国内好像没几家公司需要这种人才（大概更不需要应届生吧）。不过作为“永不失业的职业”，我对编译原理里面的设计成分更感兴趣，即自制语言。而如果作为一个应用开发者的话，约束应该大很多了，很多时候公式让你做的跟你的兴趣点完全无关，再考虑国内公司的尿性，不让你干缺德的就不错了。 扯远了，关于“设计”这种概念，对于机器学习跟网络编程，也同样适用吧。机器学习重在解决问题，而设计算法是重要环节，这里设计部分可能要发挥脑力，而不是跟风地调用算法，我觉得这里也是很有意思的部分，但如果是解决商业问题，或者做项目的话，自由度感觉不大，倒是留在实验室研究什么的更有意思（虽然最近就是留实验室学习，有点小枯燥）。而网络编程，重点是在网络层以上的开发，打交道是传输层跟应用层，设计协议是很有意思的一环。怎么样的协议更有语义，更高效，更安全，扩展性强，我觉得这部分自制性很强，也是很有研究的感觉。 隐隐约约觉得以上三个方向深入研究最后都要跟并发或性能打交道。毫无疑问，编译原理，优化部分，可能需要把代码隐式转换为并发代码，或者编译成适合并发的机器代码。机器学习虽然是高阶算法，但依托于大数据平台，而底层需要高并发，分布式的架构。有时候算法策略本身可能需要考虑容易并发，而如果设计一个机器学习平台或者系统，底层模型肯定到处是并发，也明显需要分布式。网络编程呢，则是为了更好的进行数据交换，也是并发的策略之一，更是分布式计算的基石。 以上三个方向，在底层方面最需要的便是C++了。众所周知C++是比较难掌握的，虽然现在情况在变好，但我其实还蛮担心自己毕业前连基本的STL都用不好。 基本规划好像写了这么多，还是不清楚接下来应该如何去学习知识，在这最后的2年时间。其实下个学期课挺多的，而且都是比较难的课，虽然大部分我都挺感兴趣的，但精力一旦不足，我容易对计算机感到一种疲惫，然后几天不碰。而下个学期如此多的课的情况，在学好课内之余如何把课外感兴趣的知识补上呢（C++，机器学习，网络编程深入学习）。可能就只能在周末多抽出时间学习了吧。算上准备考研，也该多用用C++了，这样数据结构也算复习了。而下学期部分课需要一些数学知识，正好稍微注重一下复习数学，也算事半功倍了。网络编程这部分，即使只是简单地学习，对计算机网络这门课帮助也不少啊，感觉咬咬牙，下学期真的能学到很多知识吧，只希望自己不要自暴自弃，要善于总结学到的知识。 如此一想，感觉也想通了一些，毕竟学习编译原理，计算机网络，操作系统的时候是完全可以用C++来实现相应的功能的，这也锻炼了我C++的水平，但可能目前完全达不到吧，所以打算暑假稍微学习一下C++。 嗯，决定了，暑假还剩一个月，我打算学习以下内容： 机器学习跟深度学习，这部分跟实验室进度来，尽量不落下吧 C++，看些简单的源码，学习一下STL，把忘记的捡起来 简单的编译原理学习，实现几个简单的玩具编译器（已经跟着博客抄了一个，感觉还需要再写一个），如果可能的话，用C++来实现 网络编程，这一块，估计很麻烦，我基本没有实战过，倒是有基本的理论知识，理想情况下应该是golang或者nodejs来写（它们封装的很好，写起来难度容易接受），C++写的话，估计写一个月也写不出什么。所以尽量暑假尝试写个简单协议（突然想起来之前写redgo，有机会完善下） 其实对C++一向是又爱又恨，爱是觉得这门语言太重要了，感觉如果掌握，用其他语言也就游刃有余了，而且有些语法感觉真的很厉害。恨是觉得这门语言太复杂了，我掌握很吃力，而且有些语法根本就是坑，基础设施STL也是非常复杂。写项目的话，我连相应的流程，工具，环境都不了解。 现在已经周六了，大概就写到这里了，其实我也是编写边梳理自己的思路。写出来呢，可以经常看，这样自己就不会忘记对自己的严格要求了。每次面对电脑，2小时也不做点有意义的事情的时候，你就会觉得自己真是非常没用，那句话怎么说来着：回首过去，尽是些可耻往事。 大概不论爱好什么，最简单证明自己的热情，是不留余力地享受自己的爱好吧]]></content>
      <categories>
        <category>Life</category>
        <category>CS</category>
      </categories>
      <tags>
        <tag>Future</tag>
        <tag>Job</tag>
        <tag>Ideal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习里的一些小概念]]></title>
    <url>%2F2017%2F08%2F10%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%87%8C%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[机器学习里的一些小概念轮廓系数轮廓系数（Silhouette Coefficient），是聚类效果好坏的一种评价方式。最早由 Peter J. Rousseeuw 在 1986 提出。它结合内聚度和分离度两种因素。可以用来在相同原始数据的基础上用来评价不同算法、或者算法不同运行方式对聚类结果所产生的影响。 计算过程假设我们已经通过一定算法，将待分类数据进行了聚类。常用的比如使用K-means，将待分类数据分为了k个簇。对于簇中的每个向量。分别计算它们的轮廓系数。对于其中的一个点 i 来说： 计算 a(i) = average(i向量到所有它属于的簇中其它点的距离) 计算 b(i) = min(i向量到所有其他簇的点的平均距离) 那么i向量轮廓系数就为： $$ S(i) = \frac{b(i) - a(i)}{max\{a(i), b(i)\}} $$ 判断可见轮廓系数的值是介于 [-1,1] ，越趋近于1代表内聚度和分离度都相对较优。将所有点的轮廓系数求平均，就是该聚类结果总的轮廓系数。 a(i) ：i向量到同一簇内其他点不相似程度的平均值 b(i) ：i向量到其他簇的平均不相似程度的最小值 S(i)接近1，则说明样本i聚类合理； S(i)接近-1，则说明样本i更应该分类到另外的簇； 若S(i)近似为0，则说明样本i在两个簇的边界上。 未完待续]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k-means]]></title>
    <url>%2F2017%2F08%2F08%2Fk-means%2F</url>
    <content type="text"><![CDATA[公式排版目前没法解决唉，博客写个数学公式怎么这么揪心，好不容易解决了公式显示问题，但排版又很难控制 k-means基本原理这个星期学无监督学习(unsuperviser-learning), 最基本的是k-means算法.k-means属于原型聚类:假设聚类结构能通过一组原型刻画. 而聚类本身是根据数据相似度来划分的,即”距离”.我这里不展开讲距离计算, 姑且用欧几里德距离来理解，即平时最常见的公式. 我们先写出k-means的最小化平方误差: 给定样本集合 $D = {x_1, x_2, \dots, x_m}$, k-means对聚类所得的簇划分 $C = {C_1, C_2, \dots, C_k}$ 的最小平方误差是 $E = \sum_{i=1}^k \sum_{x \in C_i} ||x - \mu_i||_2^2 $, 其中 $\mu_i = \frac{1}{|C_i|} \sum_{x \in C_i}x$是簇$ C_i $ 的均值向量 可以看出来, 求该式子的最优解并不容易, 因为需要考察每个样本可能的划分情况(NP难问题),所以k-means 采用贪心策略, 通过迭代优化求近似解. 感觉学完以上内容, 脑子里好像有了这个算法的轮廓了, 但仔细一想, 还有很多地方需要斟酌.k-means是划分为k个类别, 那一开始怎么划分呢?我们想到一开始需要选k个样本当作中心点来计算距离, 根据这k个位置就求得每个样本离哪个位置最接近. 这样,第一次聚类就完成了, 但是由于这次聚类造成每个类别的中心点发生了偏移, 我们需要重新计算这时候的中心点,而由于发生了偏移,我们又需要重新计算所有样本最接近的… 如何反复,直到中心点(即均值向量)不变, 便完成了聚类. 如此说来, 这个算法还是蛮朴素的, 但是还有需要斟酌的地方.如何选初始点呢？万一2个点靠得近，或者是离群点, 岂不是可能会出现很极端的聚类(比如一个人一个类别).个人觉得这有时候很难避免, 但可以通过多次随机选取初始点并评估各个聚类的效果(最小平方误差),来减少这种情况. 嗯,好像对算法的轮廓又清晰了, 但可能又有疑问, 到底如何选择k值呢?这里应该要具体问题具体分析了.如果只是想得到更好的性能, 把应该把k值从小到大递增来比较选择最优, 但这不一定给你带来实际上的最大收益.比如你是服装商, 把消费者划分了不同尺码的类别来针对客户进行推荐商品, 那到底多细才最优呢,反正我是不知道, 这得根据实际情况和专家经验来决策,而k-means在这里不充当决策者, 更像是打下手的. 扯远了,总之对于k-means, 知道k值我觉得往往很关键. 算法伪代码这样子算法应该比较清晰了,我下来列出(抄上)伪代码, 参考自周志华的西瓜书: 输入: 样本集 $D = \{x_1, x_2, ..., x_m\}$, 聚类簇数 k 过程: 从D中随机选择k个样本作为初始均值向量 $\{\mu_1, \mu_2, ..., \mu_k\}$ repeat $C_i = \emptyset$ for j = 1, 2, ..., m do 计算$x_j$与各均值向量 \mu_i(1 \leq i \leq k)的距离: $d_ji = ||x_i - \mu_i||_2$; 根据距离最近的均值向量确定x_j的簇标记: $\lambda_j = argmin_{i \in \{1,2,...,k\}}d_ij$; 将样本$x_j$划入相应的簇: $C_{\lambda_j} = C_{\lambda_j} \bigcup\{x_j\}$ end for for i = 1, 2, ..., k do 计算新均值向量: $\mu_i^` = \frac{1}{|C_i|}\sum_{x \in C_i|}x$; if $\mu_i^ \ne\mu_i$ then 更新 $\mu_i$ else 保持当前均值向量不变 end if end for util 当前均值向量都没有更新 输出: 簇划分 $C = \{C_1, C_2, ..., C_k\}$ 扯淡暂时先写到这里,第一次写latex,第一次写博客,写的不好请见谅,以上大段文字有些是建立在自己的理解写的,如果有错误,希望能不吝赐教.写的比较浅,其实也没什么意思啊,就是梳理下自己跟其他同学的思路.]]></content>
      <tags>
        <tag>ML</tag>
        <tag>Clusters</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Web API的设计与开发》读书笔记]]></title>
    <url>%2F2017%2F08%2F08%2F%E3%80%8AWeb%20API%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%BC%80%E5%8F%91%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[第1章 什么是 Web APIWeb API 的重要性一些在线服务能够对那些公开的API进行会话控制、访问控制、服务分析，提供面向用户的仪表盘，以及发布文档等，承接了各种各样的工作。 通过对外公开 Web API，同外部其他服务的集成变得更加便捷，并从中衍生出了新的价值，使在线服务以及业务不断发展，逐步形成了“API经济学”的景象，并在这几年受到相当大的关注。 各种各样的 API 模式 将已发布的 Web 在线服务的数据或功能通过 API 公开 将附加在其他网页上的微件公开 构建现代 Web 应用 开发智能手机应用 公司内部多个系统的集成 应该通过 API 公开什么 最简洁的答案是将你的在线服务所能做的事情全部通过API公开 不存在彻底屏蔽搜集信息的行为，所以无需担心盗用，公开 API并不意味着毫无限制的访问 公开 API 将原来的服务组合成新的应用来为用户提供服务的“间接销售”模式 设计优美 API 的重要性 易于使用 便于更改 健壮性好 不怕公之于众 如何美化 Web API两个重要原则： 设计规范明确的内容必须遵守相关规范 没有设计规范的内容必须遵守相关事实标准 REST 与 Web APIREST 依次一般指下面两种意思： 符合 Fielding 的 REST 架构风格的Web服务系统 符合 RPC风格的 XML (或JSON) + HTTP 接口的系统(不使用SOAP) 小结 如果尚未公开 Web AP，则应立即考虑公开 设计优美的 Web API 不用过分拘泥于 REST 一词]]></content>
      <categories>
        <category>CS</category>
      </categories>
      <tags>
        <tag>Book</tag>
        <tag>Web</tag>
        <tag>API</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[欢迎，刚搭建好的博客]]></title>
    <url>%2F2017%2F08%2F08%2F%E6%AC%A2%E8%BF%8E%EF%BC%8C%E5%88%9A%E6%90%AD%E5%BB%BA%E5%A5%BD%E7%9A%84%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[分享知识与见解博客搭建的过程照理说用hexo搭建博客是一分钟的事情，结果我花了6小时，其中绝大部分时间花在了主题上面，想找个能写数学公式的主题不容易啊，又要符合自己的审美，而且文档要齐全没有bug。最后找到了这个，但是文档还是不够详解，有些功能不知道如何使用，但暂且将就下吧。这个主题自带了mathjax，不过我还没有测试过能不能写公式，我现在就来测试 $ (\frac{1}{2})^2 = \frac{1}{4} $ 顺便测试下代码效果吧 12345678class Person&#123; constructor(name, age)&#123; this.name = name this.age = age &#125;&#125;let person = new Person('yjh', 20) 我把这个博客发了一下，看见真的可以写公式了，很开心，但这6个小时的苦大概是说不清了，而现在已经9点多了，我应该要继续学习无监督学习了。暂且这样吧，回去再折腾折腾，现在赶紧发个url给同学来测试下效果吧！]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Blog</tag>
        <tag>Primer</tag>
      </tags>
  </entry>
</search>
